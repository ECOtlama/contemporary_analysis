---
title: "pipeline"
output: html_document
---

# Pipeline for contemporary samples

##  Overview & variable definition

This is the pipeline for Illumina read merging and bwa mapping of modern libraries generated using the double stranded protocol. Merged and unmerged PE reads are mapped.

${i} core name of input file
In order to keep samples reasonably sorted and facilitate coding each sample has a unique prefix (this should be filename for the file filename.fastq.gz) each step in the pipeline then has a suffix.

```{r, engine=bash, eval=FALSE}
# VARIABLES:

REF=/home/GRUPOS/grupolince/reference_genomes/lynx_pardinus_genome/lp23.fa					
THREADS=20   # No. of computer cores used by bwa and samtools. 20 = OK, >20 = ask people first!
ARRAY=($(ls *.fastq.gz |  cut -d'_' -f1-3 | uniq)) # The array will contain the names of all the samples and we'll loop through it to process all the samples. 

# BARCODE LYNX_06_08:
# declare -A BARCODEID=(["C5RR4ACXX_7_14nf"]="c_ll_no_0075" ["C5T3DACXX_1_14nf"]="c_ll_no_0075" ["C3553ACXX_4_15nf"]="c_ll_no_0076" ["C5TMDACXX_5_15nf"]="c_ll_no_0076" ["C4592ACXX_3_16nf"]="c_ll_no_0077" ["C5TMDACXX_3_16nf"]="c_ll_no_0077" ["C3553ACXX_4_18nf"]="c_ll_no_0078" ["C5TMDACXX_1_18nf"]="c_ll_no_0078" ["C3553ACXX_4_19nf"]="c_ll_no_0079" ["C5TMDACXX_1_19nf"]="c_ll_no_0079" ["C3553ACXX_5_20nf"]="c_ll_no_0080" ["C5TMDACXX_2_20nf"]="c_ll_no_0080" ["C4592ACXX_3_21nf"]="c_ll_no_0081" ["C5TMDACXX_2_21nf"]="c_ll_no_0081" ["C3553ACXX_5_22nf"]="c_ll_no_0082" ["C5TMDACXX_3_22nf"]="c_ll_no_0082" ["C3553ACXX_5_23nf"]="c_ll_ki_0091" ["C5TMDACXX_5_23nf"]="c_ll_ki_0091" ["C3553ACXX_6_25nf"]="c_ll_ki_0092" ["C5TMDACXX_4_25nf"]="c_ll_ki_0092" ["C3553ACXX_6_27nf"]="c_ll_ki_0093" ["C5TMDACXX_4_27nf"]="c_ll_ki_0093" ["C3553ACXX_6_1nf"]="c_ll_ki_0094" ["C5T3DACXX_4_1nf"]="c_ll_ki_0094" ["C5RRAACXX_6_8nf"]="c_ll_ki_0095" ["C5RRAACXX_6_9nf"]="c_ll_ki_0096" ["C5TMDACXX_8_9nf"]="c_ll_ki_0096" ["C5TMDACXX_4_10nf"]="c_ll_ki_0097" ["C5TMUACXX_4_10nf"]="c_ll_ki_0097" ["C5TMUACXX_4_11nf"]="c_ll_ki_0098" ["C5TN1ACXX_7_11nf"]="c_ll_ki_0098" ["C5TMDACXX_8_12nf"]="c_ll_ki_0099" ["C5TMUACXX_5_12nf"]="c_ll_ki_0099" ["C5TMDACXX_8_13nf"]="c_ll_ki_0100" ["C5TMUACXX_5_13nf"]="c_ll_ki_0100" ["C5TMDACXX_7_14nf"]="c_ll_ki_0101" ["C5TMTACXX_8_14nf"]="c_ll_ki_0101" ["C5TMDACXX_7_15nf"]="c_ll_ki_0102" ["C5TMTACXX_8_15nf"]="c_ll_ki_0102" ["C5TMUACXX_2_1nf"]="c_lp_sm_0134" ["C5TN1ACXX_7_1nf"]="c_lp_sm_0134" ["C5TMUACXX_2_2nf"]="c_lp_do_0141" ["C5RRAACXX_5_3nf"]="c_lp_do_0144" ["C5T3DACXX_2_3nf"]="c_lp_do_0144" ["C5RRAACXX_5_4nf"]="c_lp_sm_0155" ["C5TMUACXX_3_5nf"]="c_lp_sm_0156" ["C5TN1ACXX_7_5nf"]="c_lp_sm_0156" ["C5TMUACXX_3_6nf"]="c_lp_sm_0161" ["C5TMTACXX_6_7nf"]="c_lp_do_0162" ["C5TN1ACXX_7_7nf"]="c_lp_do_0162" ["C5RR4ACXX_1_1nf"]="c_lp_do_0163" ["C5TN1ACXX_8_1nf"]="c_lp_do_0163" ["C5RR4ACXX_1_2nf"]="c_lp_sm_0206" ["C5TN1ACXX_8_2nf"]="c_lp_sm_0206" ["C5RR4ACXX_2_3nf"]="c_lp_sm_0208" ["C5T3DACXX_1_3nf"]="c_lp_sm_0208" ["C5RR4ACXX_2_4nf"]="c_lp_sm_0213" ["C5T3DACXX_1_4nf"]="c_lp_sm_0213" ["C5RR4ACXX_3_5nf"]="c_lp_sm_0226" ["C5T3DACXX_1_5nf"]="c_lp_sm_0226" ["C5RR4ACXX_3_6nf"]="c_lp_sm_0276" ["C5T3DACXX_1_6nf"]="c_lp_sm_0276" ["C5RR4ACXX_4_7nf"]="c_lp_do_0300" ["C5T3DACXX_2_7nf"]="c_lp_do_0300" ["C5RR4ACXX_4_8nf"]="c_lp_sm_0320" ["C5T3DACXX_1_8nf"]="c_lp_sm_0320" ["C5RR4ACXX_5_9nf"]="c_lp_sm_0325" ["C5T3DACXX_2_9nf"]="c_lp_sm_0325" ["C5RR4ACXX_5_10nf"]="c_lp_do_0333" ["C5TMDACXX_5_10nf"]="c_lp_do_0333" ["C5RR4ACXX_6_11nf"]="c_lp_do_0335" ["C5T3DACXX_2_11nf"]="c_lp_do_0335" ["C5RR4ACXX_6_12nf"]="c_lp_do_0444" ["C5T3DACXX_2_12nf"]="c_lp_do_0444" ["C5RR4ACXX_7_13nf"]="c_lp_sm_0450" ["C5T3DACXX_2_13nf"]="c_lp_sm_0450")

# BARCODE LYNX_09:
# declare -A BARCODEID=(["C6DUUANXX_2_12nf"]="c_ll_po_0001" ["C6DV6ANXX_1_12nf"]="c_ll_po_0001" ["C6DUUANXX_3_13nf"]="c_ll_po_0002" ["C6DV6ANXX_1_13nf"]="c_ll_po_0002" ["C6DUUANXX_3_14nf"]="c_ll_po_0003" ["C6DV6ANXX_1_14nf"]="c_ll_po_0003" ["C6DUUANXX_3_15nf"]="c_ll_po_0011" ["C6DV6ANXX_1_15nf"]="c_ll_po_0011" ["C6DUUANXX_2_16nf"]="c_ll_po_0014" ["C6DV6ANXX_1_16nf"]="c_ll_po_0014" ["C6DUUANXX_3_18nf"]="c_ll_po_0019" ["C6DV6ANXX_1_18nf"]="c_ll_po_0019" ["C6DUUANXX_4_19nf"]="c_ll_po_0105" ["C6DV6ANXX_1_19nf"]="c_ll_po_0105" ["C6DUUANXX_4_20nf"]="c_ll_po_0106" ["C6DV6ANXX_1_20nf"]="c_ll_po_0106" ["C6DUUANXX_2_21nf"]="c_ll_vl_0107" ["C6DV6ANXX_1_21nf"]="c_ll_vl_0107" ["C6DV6ANXX_7_5nf"]="c_ll_vl_0108" ["C6DUUANXX_2_22nf"]="c_ll_vl_0109" ["C6DUUANXX_1_23nf"]="c_ll_vl_0110" ["C6DV6ANXX_1_23nf"]="c_ll_vl_0110")

# BARCODES LYNX_PROYECT:
# declare -A BARCODEID=(["B09HCABXX_2_0"]="c_lp_do_0153" ["B09HCABXX_1_0"]="c_lp_do_0153" ["B0B5KABXX_1_0"]="c_lp_do_0153" ["B0B5KABXX_2_0"]="c_lp_do_0153" ["B09HCABXX_5_0"]="c_lp_do_0173" ["B09HCABXX_6_0"]="c_lp_do_0173" ["B0B5KABXX_6_0"]="c_lp_do_0173" ["B0B5KABXX_5_0"]="c_lp_do_0173" ["D0D6JABXX_4_0"]="c_lp_do_0443" ["D0D6JABXX_3_0"]="c_lp_do_0443" ["B0999ABXX_3_0"]="c_lp_do_0443" ["B0999ABXX_4_0"]="c_lp_do_0443" ["B09HCABXX_3_0"]="c_lp_sm_0138" ["B09HCABXX_4_0"]="c_lp_sm_0138" ["B0B5KABXX_3_0"]="c_lp_sm_0138" ["B0B5KABXX_4_0"]="c_lp_sm_0138" ["C02CHABXX_1_0"]="c_lp_sm_0140" ["C02CHABXX_2_0"]="c_lp_sm_0140" ["C02CHABXX_4_0"]="c_lp_sm_0140" ["C02CHABXX_3_0"]="c_lp_sm_0140" ["D0D6JABXX_2_0"]="c_lp_sm_0185" ["D0D6JABXX_1_0"]="c_lp_sm_0185" ["B0999ABXX_2_0"]="c_lp_sm_0185" ["B0999ABXX_1_0"]="c_lp_sm_0185" ["D0D6JABXX_5_0"]="c_lp_sm_0186" ["D0D6JABXX_6_0"]="c_lp_sm_0186" ["B0999ABXX_6_0"]="c_lp_sm_0186" ["B0999ABXX_5_0"]="c_lp_sm_0186" ["D0D6JABXX_8_0"]="c_lp_sm_0298" ["D0D6JABXX_7_0"]="c_lp_sm_0298" ["B0999ABXX_7_0"]="c_lp_sm_0298" ["B0999ABXX_8_0"]="c_lp_sm_0298" ["C02CHABXX_6_0"]="c_lp_sm_0359" ["C02CHABXX_7_0"]="c_lp_sm_0359" ["C02CHABXX_5_0"]="c_lp_sm_0359" ["C02CHABXX_8_0"]="c_lp_sm_0359" ["B09HCABXX_8_0"]="h_lp_do_0007" ["B09HCABXX_7_0"]="h_lp_do_0007" ["B0B5KABXX_7_0"]="h_lp_do_0007" ["B0B5KABXX_8_0"]="h_lp_do_0007")

# BARCODE LYNX_13:
# declare -A BARCODEID=(["C9KJ0ANXX_1_22nf"]="c_ll_vl_0113" ["CA5U3ANXX_2_22nf"]="c_ll_vl_0113" ["C9KH1ANXX_7_25nf"]="c_ll_vl_0128" ["C9KH3ANXX_8_25nf"]="c_ll_vl_0128" ["C9KJ0ANXX_2_23nf"]="c_ll_vl_0132" ["CA5U3ANXX_2_23nf"]="c_ll_vl_0132" ["C9KH1ANXX_5_21nf"]="c_ll_ya_0138" ["C9KH3ANXX_7_21nf"]="c_ll_ya_0138" ["C9KH1ANXX_5_10nf"]="c_ll_ya_0139" ["C9KH3ANXX_7_10nf"]="c_ll_ya_0139" ["C9KH1ANXX_5_11nf"]="c_ll_ya_0140" ["C9KH3ANXX_7_11nf"]="c_ll_ya_0140" ["C9KH1ANXX_5_12nf"]="c_ll_ya_0142" ["C9KH3ANXX_7_12nf"]="c_ll_ya_0142" ["C9KH1ANXX_5_15nf"]="c_ll_ya_0143" ["C9KH3ANXX_7_15nf"]="c_ll_ya_0143" ["C9KH1ANXX_7_14nf"]="c_ll_ya_0145" ["C9KH3ANXX_7_14nf"]="c_ll_ya_0145" ["C9KH1ANXX_5_13nf"]="c_ll_ya_0147" ["C9KH3ANXX_7_13nf"]="c_ll_ya_0147" ["C9KJ0ANXX_4_19nf"]="c_ll_cr_0205" ["CA5U3ANXX_2_19nf"]="c_ll_cr_0205" ["CA2W6ANXX_4_16nf"]="c_ll_cr_0206" ["CA5U3ANXX_2_16nf"]="c_ll_cr_0206" ["C9KJ0ANXX_3_20nf"]="c_ll_cr_0207" ["CA5U3ANXX_2_20nf"]="c_ll_cr_0207" ["CA2W6ANXX_4_18nf"]="c_ll_cr_0208" ["CA5U3ANXX_2_18nf"]="c_ll_cr_0208" ["C9KH1ANXX_7_27nf"]="c_ll_cr_0209" ["C9KH3ANXX_8_27nf"]="c_ll_cr_0209")

# BARCODE LYNX_14:
# declare -A BARCODEID=(["C9KH6ANXX_5_LYNX7-702ii5-4"]="h_lp_mt_0884" ["C9KN6ANXX_7_LYNX7-702ii5-4"]="h_lp_mt_0884" ["C9KNWANXX_1_LYNX7-702ii5-4"]="h_lp_mt_0884" ["C9KNWANXX_2_LYNX7-702ii5-4"]="h_lp_mt_0884" ["C9KNWANXX_3_LYNX7-702ii5-4"]="h_lp_mt_0884" ["C9KNWANXX_4_LYNX7-702ii5-4"]="h_lp_mt_0884" ["C9KNWANXX_5_LYNX7-702ii5-4"]="h_lp_mt_0884" ["C9KH6ANXX_5_LYNX7-710ii5-4"]="h_lp_mt_0885" ["C9KN6ANXX_7_LYNX7-710ii5-4"]="h_lp_mt_0885" ["C9KNWANXX_1_LYNX7-710ii5-4"]="h_lp_mt_0885" ["C9KNWANXX_2_LYNX7-710ii5-4"]="h_lp_mt_0885" ["C9KNWANXX_3_LYNX7-710ii5-4"]="h_lp_mt_0885" ["C9KNWANXX_4_LYNX7-710ii5-4"]="h_lp_mt_0885" ["C9KNWANXX_5_LYNX7-710ii5-4"]="h_lp_mt_0885" ["C9KH6ANXX_5_LYNX7-625ii5-2"]="h_lp_mt_0976" ["C9KN6ANXX_7_LYNX7-625ii5-2"]="h_lp_mt_0976" ["C9KNWANXX_1_LYNX7-625ii5-2"]="h_lp_mt_0976" ["C9KNWANXX_2_LYNX7-625ii5-2"]="h_lp_mt_0976" ["C9KNWANXX_3_LYNX7-625ii5-2"]="h_lp_mt_0976" ["C9KNWANXX_4_LYNX7-625ii5-2"]="h_lp_mt_0976" ["C9KNWANXX_5_LYNX7-625ii5-2"]="h_lp_mt_0976" ["C9KH6ANXX_5_LYNX7-709ii5-4"]="h_lp_mt_0978" ["C9KN6ANXX_7_LYNX7-709ii5-4"]="h_lp_mt_0978" ["C9KNWANXX_1_LYNX7-709ii5-4"]="h_lp_mt_0978" ["C9KNWANXX_2_LYNX7-709ii5-4"]="h_lp_mt_0978" ["C9KNWANXX_3_LYNX7-709ii5-4"]="h_lp_mt_0978" ["C9KNWANXX_4_LYNX7-709ii5-4"]="h_lp_mt_0978" ["C9KNWANXX_5_LYNX7-709ii5-4"]="h_lp_mt_0978" ["C9KH6ANXX_5_LYNX7-604ii5-2"]="h_lp_mt_0979" ["C9KN6ANXX_7_LYNX7-604ii5-2"]="h_lp_mt_0979" ["C9KNWANXX_1_LYNX7-604ii5-2"]="h_lp_mt_0979" ["C9KNWANXX_2_LYNX7-604ii5-2"]="h_lp_mt_0979" ["C9KNWANXX_3_LYNX7-604ii5-2"]="h_lp_mt_0979" ["C9KNWANXX_4_LYNX7-604ii5-2"]="h_lp_mt_0979" ["C9KNWANXX_5_LYNX7-604ii5-2"]="h_lp_mt_0979" ["C9KH6ANXX_5_LYNX7-706ii5-4"]="h_lp_mt_1025" ["C9KN6ANXX_7_LYNX7-706ii5-4"]="h_lp_mt_1025" ["C9KNWANXX_1_LYNX7-706ii5-4"]="h_lp_mt_1025" ["C9KNWANXX_2_LYNX7-706ii5-4"]="h_lp_mt_1025" ["C9KNWANXX_3_LYNX7-706ii5-4"]="h_lp_mt_1025" ["C9KNWANXX_4_LYNX7-706ii5-4"]="h_lp_mt_1025" ["C9KNWANXX_5_LYNX7-706ii5-4"]="h_lp_mt_1025" ["CA3D2ANXX_3_LYNX7-706ii5-4"]="h_lp_mt_1025" ["C9KH6ANXX_5_LYNX7-687ii5-4"]="h_lp_mt_1087" ["C9KN6ANXX_7_LYNX7-687ii5-4"]="h_lp_mt_1087" ["C9KNWANXX_1_LYNX7-687ii5-4"]="h_lp_mt_1087" ["C9KNWANXX_2_LYNX7-687ii5-4"]="h_lp_mt_1087" ["C9KNWANXX_3_LYNX7-687ii5-4"]="h_lp_mt_1087" ["C9KNWANXX_4_LYNX7-687ii5-4"]="h_lp_mt_1087" ["C9KNWANXX_5_LYNX7-687ii5-4"]="h_lp_mt_1087" ["CA3D2ANXX_3_LYNX7-687ii5-4"]="h_lp_mt_1087" ["C9KH6ANXX_5_LYNX7-700ii5-4"]="h_lp_mt_1117" ["C9KN6ANXX_7_LYNX7-700ii5-4"]="h_lp_mt_1117" ["C9KNWANXX_1_LYNX7-700ii5-4"]="h_lp_mt_1117" ["C9KNWANXX_2_LYNX7-700ii5-4"]="h_lp_mt_1117" ["C9KNWANXX_3_LYNX7-700ii5-4"]="h_lp_mt_1117" ["C9KNWANXX_4_LYNX7-700ii5-4"]="h_lp_mt_1117" ["C9KNWANXX_5_LYNX7-700ii5-4"]="h_lp_mt_1117" ["CA3D2ANXX_3_LYNX7-700ii5-4"]="h_lp_mt_1117" ["C9KH6ANXX_5_LYNX7-693ii5-4"]="h_lp_mt_1141" ["C9KN6ANXX_7_LYNX7-693ii5-4"]="h_lp_mt_1141" ["C9KNWANXX_1_LYNX7-693ii5-4"]="h_lp_mt_1141" ["C9KNWANXX_2_LYNX7-693ii5-4"]="h_lp_mt_1141" ["C9KNWANXX_3_LYNX7-693ii5-4"]="h_lp_mt_1141" ["C9KNWANXX_4_LYNX7-693ii5-4"]="h_lp_mt_1141" ["C9KNWANXX_5_LYNX7-693ii5-4"]="h_lp_mt_1141" ["CA3D2ANXX_3_LYNX7-693ii5-4"]="h_lp_mt_1141" ["C9KH6ANXX_5_LYNX7-686ii5-4"]="h_lp_mt_1272" ["C9KN6ANXX_7_LYNX7-686ii5-4"]="h_lp_mt_1272" ["C9KNWANXX_1_LYNX7-686ii5-4"]="h_lp_mt_1272" ["C9KNWANXX_2_LYNX7-686ii5-4"]="h_lp_mt_1272" ["C9KNWANXX_3_LYNX7-686ii5-4"]="h_lp_mt_1272" ["C9KNWANXX_4_LYNX7-686ii5-4"]="h_lp_mt_1272" ["C9KNWANXX_5_LYNX7-686ii5-4"]="h_lp_mt_1272" ["C9KH6ANXX_5_LYNX7-695ii5-4"]="h_lp_mt_1295" ["C9KN6ANXX_7_LYNX7-695ii5-4"]="h_lp_mt_1295" ["C9KNWANXX_1_LYNX7-695ii5-4"]="h_lp_mt_1295" ["C9KNWANXX_2_LYNX7-695ii5-4"]="h_lp_mt_1295" ["C9KNWANXX_3_LYNX7-695ii5-4"]="h_lp_mt_1295" ["C9KNWANXX_4_LYNX7-695ii5-4"]="h_lp_mt_1295" ["C9KNWANXX_5_LYNX7-695ii5-4"]="h_lp_mt_1295" ["C9KH6ANXX_5_LYNX7-708ii5-4"]="h_lp_mt_1305" ["C9KN6ANXX_7_LYNX7-708ii5-4"]="h_lp_mt_1305" ["C9KNWANXX_1_LYNX7-708ii5-4"]="h_lp_mt_1305" ["C9KNWANXX_2_LYNX7-708ii5-4"]="h_lp_mt_1305" ["C9KNWANXX_3_LYNX7-708ii5-4"]="h_lp_mt_1305" ["C9KNWANXX_4_LYNX7-708ii5-4"]="h_lp_mt_1305" ["C9KNWANXX_5_LYNX7-708ii5-4"]="h_lp_mt_1305" ["CA3D2ANXX_3_LYNX7-708ii5-4"]="h_lp_mt_1305")

# BARCODE CANDILES (6 samples that I added lately + (12/04/2017) I also added finally all the CANDILES here, because I need to have them in a separate array for the stats checkings): 
# declare -A BARCODEID=(["6220RAAXX_lane3_sequence_0"]="c_lp_sm_0221" ["6220RAAXX_lane4_sequence_0"]="c_lp_sm_0221" ["6220RAAXX_lane6_sequence_0"]="c_lp_sm_0221" ["6220RAAXX_lane7_sequence_0"]="c_lp_sm_0221" ["6220RAAXX_lane8_sequence_0"]="c_lp_sm_0221" ["62AHEAAXX_lane1_sequence_0"]="c_lp_sm_0221" ["621CYAAXX_lane1_sequence_0"]="c_lp_sm_0221" ["6220RAAXX_lane1_sequence_0"]="c_lp_sm_0221" ["6220RAAXX_lane5_sequence_0"]="c_lp_sm_0221" ["6220RAAXX_lane2_sequence_0"]="c_lp_sm_0221")

# BARCODE LYNX_15:
# declare -A BARCODEID=(["C9KH1ANXX_7_1nf"]="c_ll_to_0191" ["C9KH1ANXX_7_7nf"]="c_ll_la_0053" ["C9KH1ANXX_7_10nf"]="c_ll_la_0054" ["C9KH1ANXX_8_11nf"]="c_ll_la_0044" ["C9KH1ANXX_8_12nf"]="c_ll_tu_0157" ["C9KH1ANXX_8_13nf"]="c_ll_tu_0158" ["C9KH1ANXX_8_14nf"]="c_ll_tu_0159" ["C9KH1ANXX_8_15nf"]="c_ll_tu_0165" ["C9KH1ANXX_8_18nf"]="c_ll_tu_0166" ["C9KH1ANXX_8_19nf"]="c_ll_tu_0153" ["C9KH3ANXX_7_1nf"]="c_ll_to_0191" ["C9KH3ANXX_8_7nf"]="c_ll_la_0053" ["C9KH3ANXX_8_10nf"]="c_ll_la_0054" ["C9KH3ANXX_8_11nf"]="c_ll_la_0044" ["C9KH3ANXX_8_12nf"]="c_ll_tu_0157" ["C9KH3ANXX_8_13nf"]="c_ll_tu_0158" ["CA2W6ANXX_1_1nf"]="c_ll_og_0181" ["CA2W6ANXX_1_2nf"]="c_ll_og_0187" ["CA2W6ANXX_1_3nf"]="c_ll_ka_0184" ["CA2W6ANXX_1_4nf"]="c_ll_ka_0186" ["CA2W6ANXX_2_5nf"]="c_ll_ka_0189" ["CA2W6ANXX_2_7nf"]="c_ll_to_0190" ["CA2W6ANXX_2_8nf"]="c_ll_la_0047" ["CA2W6ANXX_4_6nf"]="c_ll_ka_0188" ["CA3D2ANXX_5_1nf"]="c_ll_og_0181" ["CA3D2ANXX_5_2nf"]="c_ll_og_0187" ["CA3D2ANXX_5_3nf"]="c_ll_ka_0184" ["CA3D2ANXX_5_4nf"]="c_ll_ka_0186" ["CA3D2ANXX_5_5nf"]="c_ll_ka_0189" ["CA3D2ANXX_5_6nf"]="c_ll_ka_0188" ["CA3D2ANXX_6_7nf"]="c_ll_to_0190" ["CA3D2ANXX_6_8nf"]="c_ll_la_0047" ["CA3D2ANXX_6_9nf"]="c_ll_la_0048" ["CA3D2ANXX_6_21nf"]="c_ll_la_0052" ["CAABGANXX_5_14nf"]="c_ll_tu_0159" ["CAABGANXX_5_15nf"]="c_ll_tu_0165" ["CAABGANXX_5_18nf"]="c_ll_tu_0166" ["CAABGANXX_5_19nf"]="c_ll_tu_0153" ["CAABGANXX_5_9nf"]="c_ll_la_0048" ["CAABGANXX_5_21nf"]="c_ll_la_0052")

# BARCODE LYNX_16:
# declare -A BARCODEID=(["CA2W6ANXX_4_23nf"]="h_ll_ba_0214" ["CA2W6ANXX_4_25nf"]="c_ll_ba_0216" ["CA2W6ANXX_4_27nf"]="h_ll_ba_0215" ["CA3D2ANXX_6_23nf"]="h_ll_ba_0214" ["CA3D2ANXX_6_25nf"]="c_ll_ba_0216" ["CA3D2ANXX_6_27nf"]="h_ll_ba_0215") 

# BARCODE MACROGEN:
# declare -A BARCODEID=(["LC1"]="c_lc_zz_0001" ["LL112"]="c_ll_vl_0112" ["LL146"]="c_ll_ya_0146" ["LL212"]="c_ll_cr_0212" ["LL90"]="c_ll_ki_0090" ["LR1"]="c_lr_zz_0001")

```

24/05/2017
We realize that the mapping went wrong with some samples. So, we copied the raw samples to a new folder and remmaped them. 

```{bash}
# We realize that the mapping went wrong for some samples (total reads vs fastq reads is below 60%). So I would like to repeat the mapping. 

# I copy those file to a new folder:
mkdir /home/mlucena/new_mapping/

[mlucena@genomics_b FASTQ]$ ls CA2W6ANXX_1_1nf*
CA2W6ANXX_1_1nf_1.fastq.gz  CA2W6ANXX_1_1nf_1.fastq.gz.md5  CA2W6ANXX_1_1nf_2.fastq.gz  CA2W6ANXX_1_1nf_2.fastq.gz.md5
[mlucena@genomics_b FASTQ]$ scp  CA2W6ANXX_1_1nf*gz /home/mlucena/new_mapping/
[mlucena@genomics_b FASTQ]$ ls  CA2W6ANXX_2_5nf*
CA2W6ANXX_2_5nf_1.fastq.gz  CA2W6ANXX_2_5nf_1.fastq.gz.md5  CA2W6ANXX_2_5nf_2.fastq.gz  CA2W6ANXX_2_5nf_2.fastq.gz.md5
[mlucena@genomics_b FASTQ]$ scp  CA2W6ANXX_2_5nf*gz /home/mlucena/new_mapping/
[mlucena@genomics_b FASTQ]$ ls CA3D2ANXX_5_1nf*
CA3D2ANXX_5_1nf_1.fastq.gz  CA3D2ANXX_5_1nf_1.fastq.gz.md5  CA3D2ANXX_5_1nf_2.fastq.gz  CA3D2ANXX_5_1nf_2.fastq.gz.md5
[mlucena@genomics_b FASTQ]$ scp CA3D2ANXX_5_1nf*gz /home/mlucena/new_mapping/
[mlucena@genomics_b FASTQ]$ ls CA3D2ANXX_5_5nf*
CA3D2ANXX_5_5nf_1.fastq.gz  CA3D2ANXX_5_5nf_1.fastq.gz.md5  CA3D2ANXX_5_5nf_2.fastq.gz  CA3D2ANXX_5_5nf_2.fastq.gz.md5
[mlucena@genomics_b FASTQ]$ scp CA3D2ANXX_5_5nf*gz /home/mlucena/new_mapping/

# BARCODE LYNX_15_TO_REPEAT:
# declare -A BARCODEID=(["CA2W6ANXX_1_1nf"]="c_ll_og_0181" ["CA2W6ANXX_2_5nf"]="c_ll_ka_0189" ["CA3D2ANXX_5_1nf"]="c_ll_og_0181" ["CA3D2ANXX_5_5nf"]="c_ll_ka_0189" )

ARRAY=($(ls *.fastq.gz |  cut -d'_' -f1-3 | uniq)) # The array will contain the names of all the samples and we'll loop through it to process all the samples. 

```


1/07/2017
I completed the new mapping of two samples that seems to have failed. The mapping didn't went well again, as there was an issue with the RG, anyway I was able to get the stats. They proved that those two samples actually failed while mapping the first time, so I would like to keep trying with the other samples that seems to have a lower amount of reads that enter the mapping compared to the reads in the fastq. Besides I will start with these ones again from the very beginning. 

I start with:

c_ll_ka_0188 --> 63,44521703 LYNX_15
c_ll_la_0048 --> 65,850268   LYNX_15
h_ll_ba_0214 --> 68,80822511 
c_ll_ba_0216 --> 74,73082202
c_ll_tu_0159 --> 79,80165467 LYNX_15


Their reads are:
c_ll_ka_0188 ----> LYNX_15
CA2W6ANXX_4_6nf c_ll_ka_0188 
CA3D2ANXX_5_6nf c_ll_ka_0188 

c_ll_la_0048 ----> LYNX_15
CAABGANXX_5_9nf c_ll_la_0048
CA3D2ANXX_6_9nf c_ll_la_0048 

c_ll_tu_0159 ----> LYNX_15
CAABGANXX_5_14nf c_ll_tu_0159 
C9KH1ANXX_8_14nf c_ll_tu_0159 

h_ll_ba_0214 ----> LYNX_16
CA2W6ANXX_4_23nf h_ll_ba_0214 
CA3D2ANXX_6_23nf h_ll_ba_0214 

c_ll_ba_0216 ----> LYNX_16
CA2W6ANXX_4_25nf c_ll_ba_0216 
CA3D2ANXX_6_25nf c_ll_ba_0216 


```{bash}

scp	/backup/grupolince/raw_data/LYNX_15/20170206/FASTQ/CA2W6ANXX_1_1nf*gz 	/home/mlucena/new_mapping/
scp	/backup/grupolince/raw_data/LYNX_15/20170206/FASTQ/CA2W6ANXX_2_5nf*gz 	/home/mlucena/new_mapping/
scp	/backup/grupolince/raw_data/LYNX_15/20170206/FASTQ/CA3D2ANXX_5_1nf*gz	  /home/mlucena/new_mapping/
scp	/backup/grupolince/raw_data/LYNX_15/20170206/FASTQ/CA3D2ANXX_5_5nf*gz	  /home/mlucena/new_mapping/
scp /backup/grupolince/raw_data/LYNX_15/20170206/FASTQ/CA2W6ANXX_4_6nf*gz	  /home/mlucena/new_mapping/
scp /backup/grupolince/raw_data/LYNX_15/20170206/FASTQ/CA3D2ANXX_5_6nf*gz	  /home/mlucena/new_mapping/
scp /backup/grupolince/raw_data/LYNX_15/20170206/FASTQ/CAABGANXX_5_9nf*gz	  /home/mlucena/new_mapping/
scp /backup/grupolince/raw_data/LYNX_15/20170206/FASTQ/CA3D2ANXX_6_9nf*gz	  /home/mlucena/new_mapping/
scp /backup/grupolince/raw_data/LYNX_15/20170206/FASTQ/CAABGANXX_5_14nf*gz	/home/mlucena/new_mapping/
scp /backup/grupolince/raw_data/LYNX_15/20170206/FASTQ/C9KH1ANXX_8_14nf*gz	 /home/mlucena/new_mapping/
scp /backup/grupolince/raw_data/LYNX_16/20170206/FASTQ/CA2W6ANXX_4_23nf*gz	 /home/mlucena/new_mapping/
scp /backup/grupolince/raw_data/LYNX_16/20170206/FASTQ/CA3D2ANXX_6_23nf*gz	 /home/mlucena/new_mapping/
scp /backup/grupolince/raw_data/LYNX_16/20170206/FASTQ/CA2W6ANXX_4_25nf*gz	 /home/mlucena/new_mapping/
scp /backup/grupolince/raw_data/LYNX_16/20170206/FASTQ/CA3D2ANXX_6_25nf*gz	 /home/mlucena/new_mapping/

# Array for trimming
ARRAY=(CA2W6ANXX_4_6nf CA3D2ANXX_5_6nf CAABGANXX_5_9nf CA3D2ANXX_6_9nf CAABGANXX_5_14nf C9KH1ANXX_8_14nf CA2W6ANXX_4_23nf CA3D2ANXX_6_23nf CA2W6ANXX_4_25nf CA3D2ANXX_6_25nf)

# Array for mapping
ARRAY=($(ls | grep trimmed | cut -d"_" -f 1-3 | sort | uniq ))

# BARCODE LYNX_15 y LYNX_16:
# declare -A BARCODEID=(["C9KH1ANXX_7_1nf"]="c_ll_to_0191" ["C9KH1ANXX_7_7nf"]="c_ll_la_0053" ["C9KH1ANXX_7_10nf"]="c_ll_la_0054" ["C9KH1ANXX_8_11nf"]="c_ll_la_0044" ["C9KH1ANXX_8_12nf"]="c_ll_tu_0157" ["C9KH1ANXX_8_13nf"]="c_ll_tu_0158" ["C9KH1ANXX_8_14nf"]="c_ll_tu_0159" ["C9KH1ANXX_8_15nf"]="c_ll_tu_0165" ["C9KH1ANXX_8_18nf"]="c_ll_tu_0166" ["C9KH1ANXX_8_19nf"]="c_ll_tu_0153" ["C9KH3ANXX_7_1nf"]="c_ll_to_0191" ["C9KH3ANXX_8_7nf"]="c_ll_la_0053" ["C9KH3ANXX_8_10nf"]="c_ll_la_0054" ["C9KH3ANXX_8_11nf"]="c_ll_la_0044" ["C9KH3ANXX_8_12nf"]="c_ll_tu_0157" ["C9KH3ANXX_8_13nf"]="c_ll_tu_0158" ["CA2W6ANXX_1_1nf"]="c_ll_og_0181" ["CA2W6ANXX_1_2nf"]="c_ll_og_0187" ["CA2W6ANXX_1_3nf"]="c_ll_ka_0184" ["CA2W6ANXX_1_4nf"]="c_ll_ka_0186" ["CA2W6ANXX_2_5nf"]="c_ll_ka_0189" ["CA2W6ANXX_2_7nf"]="c_ll_to_0190" ["CA2W6ANXX_2_8nf"]="c_ll_la_0047" ["CA2W6ANXX_4_6nf"]="c_ll_ka_0188" ["CA3D2ANXX_5_1nf"]="c_ll_og_0181" ["CA3D2ANXX_5_2nf"]="c_ll_og_0187" ["CA3D2ANXX_5_3nf"]="c_ll_ka_0184" ["CA3D2ANXX_5_4nf"]="c_ll_ka_0186" ["CA3D2ANXX_5_5nf"]="c_ll_ka_0189" ["CA3D2ANXX_5_6nf"]="c_ll_ka_0188" ["CA3D2ANXX_6_7nf"]="c_ll_to_0190" ["CA3D2ANXX_6_8nf"]="c_ll_la_0047" ["CA3D2ANXX_6_9nf"]="c_ll_la_0048" ["CA3D2ANXX_6_21nf"]="c_ll_la_0052" ["CAABGANXX_5_14nf"]="c_ll_tu_0159" ["CAABGANXX_5_15nf"]="c_ll_tu_0165" ["CAABGANXX_5_18nf"]="c_ll_tu_0166" ["CAABGANXX_5_19nf"]="c_ll_tu_0153" ["CAABGANXX_5_9nf"]="c_ll_la_0048" ["CAABGANXX_5_21nf"]="c_ll_la_0052" ["CA2W6ANXX_4_23nf"]="h_ll_ba_0214" ["CA2W6ANXX_4_25nf"]="c_ll_ba_0216" ["CA2W6ANXX_4_27nf"]="h_ll_ba_0215" ["CA3D2ANXX_6_23nf"]="h_ll_ba_0214" ["CA3D2ANXX_6_25nf"]="c_ll_ba_0216" ["CA3D2ANXX_6_27nf"]="h_ll_ba_0215") 
```

07/06/2017:

The result after rmdup and sort is:

sample_name	total_seq	total_reads	percentage_reads_mapped_vs_fastq	new_mapping_total	percentage_new_mapping
c_ll_ka_0189	144401872	20258074	14,02895525	119152042	82,51419483 <--- BAD :::::::::::: ka
c_ll_og_0181	153034834	85008954	55,54876088	136432850	89,15149998 <--- BAD :::::::::::: og
c_ll_ka_0188	140053708	88857379	63,44521703	121032131	86,41836959 <--- BAD :::::::::::: ka
c_ll_la_0048	160404108	105626535	65,850268	105626533	65,85026675   ok
h_ll_ba_0214	155496480	106994368	68,80822511	106994365	68,80822318 ok
c_ll_ba_0216	148431160	110923826	74,73082202	110923829	74,73082404 ok
c_ll_tu_0159	152606768	121782726	79,80165467	153894758	100,8439927 <----- PUTADA! :::::::::::: tu

the first 3 improve the mapping, the next 3 are the same and the last one also improve it, so I will need to try with all the rest.

So the next ones untill 100% mapped are:

sample_name	total_seq	total_reads	percentage_reads_mapped_vs_fastq
c_ll_la_0052	172109704	140835339	81,82881948 : below 100%
c_ll_la_0047	166995702	139257501	83,38987132 : below 100%
h_ll_ba_0215	152814174	131203988	85,8585199 : below 100%
c_ll_og_0187	155208878	133278626	85,87049125 : below 100%
c_ll_to_0190	166328398	147316259	88,56951715 : below 100%
c_ll_ka_0186	152875132	137803212	90,14102568 : below 100%
c_ll_ka_0184	150635072	139684512	92,73040478 : below 100%


The original reads are:

c_ll_la_0052 
CA3D2ANXX_6_21nf c_ll_la_0052
CAABGANXX_5_21nf c_ll_la_0052

c_ll_la_0047
CA2W6ANXX_2_8nf c_ll_la_0047
CA3D2ANXX_6_8nf c_ll_la_0047

h_ll_ba_0215
CA2W6ANXX_4_27nf h_ll_ba_0215
CA3D2ANXX_6_27nf h_ll_ba_0215

c_ll_og_0187
CA3D2ANXX_5_2nf c_ll_og_0187
CA2W6ANXX_1_2nf c_ll_og_0187

c_ll_to_0190
CA2W6ANXX_2_7nf c_ll_to_0190
CA3D2ANXX_6_7nf c_ll_to_0190

c_ll_ka_0186
CA3D2ANXX_5_4nf c_ll_ka_0186
CA2W6ANXX_1_4nf c_ll_ka_0186

c_ll_ka_0184
CA2W6ANXX_1_3nf c_ll_ka_0184
CA3D2ANXX_5_3nf c_ll_ka_0184


```{bash}


scp /backup/grupolince/raw_data/LYNX_15/20170206/FASTQ/CA3D2ANXX_6_21nf*gz /home/mlucena/new_mapping/new_mapping2
scp /backup/grupolince/raw_data/LYNX_15/20170206/FASTQ/CAABGANXX_5_21nf*gz /home/mlucena/new_mapping/new_mapping2
scp /backup/grupolince/raw_data/LYNX_15/20170206/FASTQ/CA2W6ANXX_2_8nf*gz /home/mlucena/new_mapping/new_mapping2
scp /backup/grupolince/raw_data/LYNX_15/20170206/FASTQ/CA3D2ANXX_6_8nf*gz /home/mlucena/new_mapping/new_mapping2
scp /backup/grupolince/raw_data/LYNX_16/20170206/FASTQ/CA2W6ANXX_4_27nf*gz /home/mlucena/new_mapping/new_mapping2
scp /backup/grupolince/raw_data/LYNX_16/20170206/FASTQ/CA3D2ANXX_6_27nf*gz /home/mlucena/new_mapping/new_mapping2
scp /backup/grupolince/raw_data/LYNX_15/20170206/FASTQ/CA3D2ANXX_5_2nf*gz /home/mlucena/new_mapping/new_mapping2
scp /backup/grupolince/raw_data/LYNX_15/20170206/FASTQ/CA2W6ANXX_1_2nf*gz /home/mlucena/new_mapping/new_mapping2
scp /backup/grupolince/raw_data/LYNX_15/20170206/FASTQ/CA2W6ANXX_2_7nf*gz /home/mlucena/new_mapping/new_mapping2
scp /backup/grupolince/raw_data/LYNX_15/20170206/FASTQ/CA3D2ANXX_6_7nf*gz /home/mlucena/new_mapping/new_mapping2
scp /backup/grupolince/raw_data/LYNX_15/20170206/FASTQ/CA3D2ANXX_5_4nf*gz /home/mlucena/new_mapping/new_mapping2
scp /backup/grupolince/raw_data/LYNX_15/20170206/FASTQ/CA2W6ANXX_1_4nf*gz /home/mlucena/new_mapping/new_mapping2
scp /backup/grupolince/raw_data/LYNX_15/20170206/FASTQ/CA2W6ANXX_1_3nf*gz /home/mlucena/new_mapping/new_mapping2
scp /backup/grupolince/raw_data/LYNX_15/20170206/FASTQ/CA3D2ANXX_5_3nf*gz /home/mlucena/new_mapping/new_mapping2

REF=/home/GRUPOS/grupolince/reference_genomes/lynx_pardinus_genome/lp23.fa					
THREADS=20  
ARRAY=($(ls *.fastq.gz | cut -d"_" -f 1-3 | sort | uniq))



```



## 	FastQC: quality summary of raw data 

```{r, engine=bash, eval=FALSE}

for i in ${ARRAY[@]}
do
echo "${i}"
fastqc ${i}_1.fastq.gz  # -a adapters.txt
done

for i in ${ARRAY[@]}
do
echo "${i}"
fastqc ${i}_2.fastq.gz  # -a adapters.txt
done

```

## 	Trimming

Trim adapter seqs and merge overlapping R1 and R2. 
To decide whether to trim or not, check fastQC files. 

Nosotro sólo trimamos para LYNX_14

LYNX_06 y LYNX_08_09 venían trimados. No los hemos trimado. 
LYNX_13, LYNX_15 y LYNX_16 también venían trimados. 
Los del proyecto genómca concluimos que no merece la pena trimarlos, porque aunque tienen adaptadores la proporción es muy poca, y el tiempo de computación es mucho. 

```{r, engine=bash, eval=FALSE}

for i in ${ARRAY[@]}
do

echo "${i}"
SeqPrep -f "${i}"_1.fastq.gz -r "${i}"_2.fastq.gz -1 "${i}"_R1_trimmed.fastq.gz -2 "${i}"_R2_trimmed.fastq.gz -A AGATCGGAAGAGCACACGTC -B AGATCGGAAGAGCGTCGTGT
done   

```

##	FastQC: quality summary of trimmed data 

Only if you have trimmed. 

```{r, engine=bash, eval=FALSE}

for i in ${ARRAY[@]}
do
fastqc "${i}"_R1_trimmed.fastq.gz   # -a adapters.txt;
done

for i in ${ARRAY[@]}
do
fastqc "${i}"_R2_trimmed.fastq.gz   # -a adapters.txt;	
done

```

**CHECK CA2W6ANXX_2_7nf

## 	Aligment BWA-MEM	

Checked whether you are using trimmed or untrimmed data.

```{r, engine=bash, eval=FALSE}

for i in ${ARRAY[@]}
do
echo $i

# bwa mem $REF ${i}_1.fastq.gz ${i}_2.fastq.gz -t $THREADS  >  ${i}.sam
# samtools view -hbS  ${i}.sam -@ $THREADS | samtools sort -@ $THREADS -o ${i}_sorted.bam
# rm ${i}.sam
# samtools flagstat  ${i}_sorted.bam >  ${i}_raw.stats

# Para los de LYNX_14: Lanzado!
bwa mem $REF ${i}_R1_trimmed.fastq.gz ${i}_R2_trimmed.fastq.gz -t $THREADS  >  ${i}.sam
samtools view -hbS  ${i}.sam -@ $THREADS | /opt/samtools/samtools sort -@ $THREADS -o ${i}_sorted.bam && rm ${i}.sam

done
```


```{bash}

# Me he cargado esta muestra así que la vuelvo a lanzar
bwa mem $REF CA2W6ANXX_1_1nf_R1_trimmed.fastq.gz CA2W6ANXX_1_1nf_R2_trimmed.fastq.gz -t 5  >  CA2W6ANXX_1_1nf.sam

```


## 	Adding read groups

```{r, engine=bash, eval=FALSE}
for i in ${ARRAY[@]}
do

run=($(echo $i | cut -d"_" -f 1))  #Sacar el run de i

java -jar /opt/picard-tools/picard.jar AddOrReplaceReadGroups I=${i}_sorted.bam O=${BARCODEID["${i}"]}_${i}_sorted_rg.bam RGID=${i} RGLB=${BARCODEID["${i}"]}_lib RGPL=Illumina RGPU=${run} RGSM=${BARCODEID["${i}"]} VALIDATION_STRINGENCY=SILENT && rm ${i}_sorted.bam
done

```




## 	Merge BAM	

```{r, engine=bash, eval=FALSE}

SAMPLESLIST=($(echo ${BARCODEID[@]} | tr ' ' '\n' | sort | uniq | tr ' ' '\n'))

for sample in "${SAMPLESLIST[@]}"
do
echo "${sample}"
ls ./"${sample}"_*_sorted_rg.bam  > "${sample}".bam.list
echo;echo
echo `wc -l "${sample}".bam.list`;
lines=`wc -l "${sample}".bam.list | cut -f 1 -d " " `
if [ "$lines" -eq "1" ]; 
then echo "ONE";
cp `cat "${sample}".bam.list`  "${sample}".bam;
fi
if [ "$lines" -gt "1" ]; 
then echo "more than ONE";
/opt/samtools/samtools merge  -@ 25 -r "${sample}".bam `cat "${sample}".bam.list`;
fi
samtools flagstat "${sample}".bam >  "${sample}".stats;
/opt/samtools/samtools sort  -@ 25 "${sample}".bam -o "${sample}"_sorted.bam && rm "${sample}".bam;
done

```


## 	Remove duplicates	

```{r, engine=bash, eval=FALSE}

ARRAY_MERGED_BAM_SAMPLE_NAME=($(ls *_sorted.bam |  cut -d'_' -f1,2,3,4 | uniq))

for i in ${ARRAY_MERGED_BAM_SAMPLE_NAME[@]}
do
echo "${i}"
java -jar /opt/picard-tools/picard.jar MarkDuplicates METRICS_FILE=${i}_rmdup.txt I=${i}_sorted.bam O=${i}_sorted_rmdup.bam MAX_FILE_HANDLES_FOR_READ_ENDS_MAP=2000 
/opt/samtools/samtools sort ${i}_sorted_rmdup.bam -@ 30 -o ${i}_sorted_rmdup_sorted.bam
samtools index ${i}_sorted_rmdup_sorted.bam
samtools flagstat ${i}_sorted_rmdup_sorted.bam > ${i}_sorted_rmdup_sorted.stats

done

```

## 	Realignment with GATK	

Requirement:
Coordinate-sorted and indexed BAM alignment data
This two-step indel realignment process:
A) first identifies such regions where alignments may potentially be improved 
B) then realigns the reads in these regions using a consensus model that takes all reads in the alignment context together.


```{r, engine=bash, eval=FALSE}

for i in ${ARRAY_MERGED_BAM_SAMPLE_NAME[@]}
do
echo "${i}"
# RealignerTargetCreator
# rm ${i}_sorted.bam
# rm ${i}_sorted_rmdup.bam
java -jar /home/tmp/Software/GATK_3.4/GenomeAnalysisTK.jar -T RealignerTargetCreator -nt 24 -R $REF -I ${i}_sorted_rmdup_sorted.bam -o ${i}_realignertargetcreator.intervals
# IndelRealigner
java -jar /home/tmp/Software/GATK_3.4/GenomeAnalysisTK.jar -T IndelRealigner -R $REF -targetIntervals ${i}_realignertargetcreator.intervals -I ${i}_sorted_rmdup_sorted.bam -o ${i}_sorted_rmdup_sorted_indelrealigner.bam
done
```

##  Move all the samples to a common folder	  

```{r, engine=bash, eval=FALSE}

mv /home/GRUPOS/grupolince/lynx_genomes_5x/raw_data/LYNX_09/*bam /home/GRUPOS/grupolince/lynx_genomes_5x/BAM_files
mv /home/GRUPOS/grupolince/lynx_genomes_5x/raw_data/LYNX_09/*stas /home/GRUPOS/grupolince/lynx_genomes_5x/BAM_files
mv /home/GRUPOS/grupolince/lynx_genomes_5x/raw_data/LYNX_06_08/*bam /home/GRUPOS/grupolince/lynx_genomes_5x/BAM_files
mv /home/GRUPOS/grupolince/lynx_genomes_5x/raw_data/LYNX_06_08/*stats /home/GRUPOS/grupolince/lynx_genomes_5x/BAM_files

# Remove also intermediated files. 
```



## 	Recalibration with GATK	

The base recalibration process involves two key steps: 
A) first the program builds a model of covariation based on the data and a set of known variants (which you can bootstrap if there is none available for your organism) 
B) then it adjusts the base quality scores in the data based on the model. 

### SNP calling

```{r, engine=bash, eval=FALSE}
# Define new variables

POP=("og")
ROUND=1
PREVIOUS_ROUND=$(expr $ROUND - 1)

for pop in ${POP[@]}
do
echo "${pop}"
if [ "$ROUND" -eq "1" ];
then
ls *_${pop}_*.bam | grep -v "recal" | cut -d' ' -f9 >  "${pop}"_round-"$ROUND".bam.list;
fi
if [ "$ROUND" -gt "1" ];
then
ls *_${pop}*recal_round-"$PREVIOUS_ROUND".bam | cut -d' ' -f9 > "${pop}"_round-"$ROUND".bam.list;
fi
number_id=$(printf "%03d" `wc -l  "${pop}"_round-"$ROUND".bam.list | tr -s ' ' | cut -d ' ' -f1`); # He cambiado el -f2 por -f1 porque si no los de macrogen no corrian, comprobar que siguen biuen
echo "${number_id}";
mv  "${pop}"_round-"$ROUND".bam.list  "${pop}"_round-"$ROUND"_n"$number_id".bam.list;
INPUT_BAMS_FOR_CALLING="${pop}"_round-"$ROUND"_n"$number_id".bam.list;
java  -XX:MaxMetaspaceSize=1g -XX:+UseG1GC  -XX:+UseStringDeduplication -Xms64g  -Xmx64g -jar /home/tmp/Software/GATK_3.4/GenomeAnalysisTK.jar -T HaplotypeCaller -nct 8 -R $REF -I $INPUT_BAMS_FOR_CALLING --genotyping_mode DISCOVERY -stand_emit_conf 10 -stand_call_conf 30 -o "${pop}"_n"$number_id"_raw_round-"$ROUND".vcf
/opt/vcflib/bin/vcffilter -f "QD > 2 & MQ > 40 & FS < 100 " "${pop}"_n"$number_id"_raw_round-"$ROUND".vcf > "${pop}"_n"$number_id"_raw_round-"$ROUND"_filtered.vcf
done

```

### Recalibration 

```{r, engine=bash, eval=FALSE}

REF=/home/GRUPOS/grupolince/reference_genomes/lynx_pardinus_genome/lp23.fa
POP=("ka")
ROUND=1
PREVIOUS_ROUND=$(expr $ROUND - 1)

for pop in ${POP[@]}
do
echo "${pop}"
# esto de number_id si corro todo junto está repetido:
number_id=$(printf "%03d" `wc -l  "${pop}"_round-"$ROUND"*.bam.list | tr -s ' ' | cut -d ' ' -f1`); # He cambiado el -f2 por -f1 porque si no los de macrogen no corrian, comprobar que siguen biuen
INPUT_BAMS_FOR_RECALIBRATING=($(cat "${pop}"_round-1_n*.bam.list))  # He cambiado el nombre del archivo porque corriendo los de macrogen me he dado cuenta de que estaban mal
echo $number_id
for id in ${INPUT_BAMS_FOR_RECALIBRATING[@]}
do
echo $id
java -XX:MaxMetaspaceSize=1g -XX:+UseG1GC  -XX:+UseStringDeduplication -Xms10g  -Xmx50g -jar /home/tmp/Software/GATK_3.4/GenomeAnalysisTK.jar   -T BaseRecalibrator -R $REF -I ${id} -knownSites "${pop}"_n"$number_id"_raw_round-"$ROUND"_filtered.vcf -o ${id/_sorted_rmdup_sorted_indelrealigner.bam/_recal_data_round-"$ROUND".table}
java -jar /home/tmp/Software/GATK_3.4/GenomeAnalysisTK.jar -T PrintReads -R $REF -I ${id} -BQSR ${id/_sorted_rmdup_sorted_indelrealigner.bam/_recal_data_round-"$ROUND".table} -o ${id/_sorted_rmdup_sorted_indelrealigner.bam/_recal_round-"$ROUND".bam}

# Be sure that you have install and running in R the following packages:
# R: library("ggplot2"), library("gplots"), library("reshape"), library("grid"), library("tools"), library("gsalib")

if [ "$ROUND" -eq "1" ]; 
then
echo "AnalyzeCovariates  round ""$ROUND" " sample ""${id}"
java -jar /home/tmp/Software/GATK_3.4/GenomeAnalysisTK.jar -T AnalyzeCovariates -R $REF -BQSR ${id/_sorted_rmdup_sorted_indelrealigner.bam/_recal_data_round-1.table} -plots ${id/_sorted_rmdup_sorted_indelrealigner.bam/_BQSR_round-1.pdf}
fi
if [ "$ROUND" -eq "2" ]; 
then
java -jar GenomeAnalysisTK.jar -T AnalyzeCovariates -R $REF -before ${id/_sorted_rmdup_sorted_indelrealigner.bam/_recal_data_round-1.table} -after ${id/_sorted_rmdup_sorted_indelrealigner.bam/_recal_data_round-2.table} -csv ${id/_sorted_rmdup_sorted_indelrealigner.bam/BQSR_round-2.csv} -plots ${id/_sorted_rmdup_sorted_indelrealigner.bam/BQSR_round-2.pdf}
fi   
if [ "$ROUND" -eq "3" ]; 
then
java -jar GenomeAnalysisTK.jar -T AnalyzeCovariates -R $REF -ignoreLMT -BQSR ${id/_sorted_rmdup_sorted_indelrealigner.bam/_recal_data_round-1.table} -before ${id/_sorted_rmdup_sorted_indelrealigner.bam/_recal_data_round-2.table} -after ${id/_sorted_rmdup_sorted_indelrealigner.bam/_recal_data_round-3.table} -plots BQSR_round-3.pdf
fi
done
done

```





```{bash}


## Where I have to keep the files:
mkdir /home/mlucena/new_mapping/new_calling/
mv /backup/grupolince/BAM_files_Lynx/BAM_indelrealigner_before_recal/LYNX_15/* /home/mlucena/new_mapping/new_calling/

#For the new mappings I have these samples:

-rw-rw-r--. 1 mlucena mlucena  9124140668 jun  7 13:28 c_ll_ka_0188_sorted_rmdup_sorted_indelrealigner.bam
-rw-rw-r--. 1 mlucena mlucena  8948313915 jun  7 14:34 c_ll_ka_0189_sorted_rmdup_sorted_indelrealigner.bam
-rw-rw-r--. 1 mlucena mlucena 10011795287 jun  7 16:32 c_ll_og_0181_sorted_rmdup_sorted_indelrealigner.bam
-rw-rw-r--. 1 mlucena mlucena 11168547837 jun  7 17:42 c_ll_tu_0159_sorted_rmdup_sorted_indelrealigner.bam

# I will move them with the other indelrealigner bam files from project LYNX_15 to recalibrate them. 

# First I will rm the old ones:
rm  c_ll_ka_0188_sorted_rmdup_sorted_indelrealigner.*
rm  c_ll_ka_0189_sorted_rmdup_sorted_indelrealigner.*
rm  c_ll_og_0181_sorted_rmdup_sorted_indelrealigner.*
rm  c_ll_tu_0159_sorted_rmdup_sorted_indelrealigner.*




```





## Fastq & BAM stats

```{r, engine=bash, eval=FALSE}


 # FASTQ
 ######################################################################### 


# Get in with Godoy's account.

mkdir /home/jagodoy/backup/raw_data/stats # En este directorio voy a almacenar todos los archivos que genere de los distintos proyectos.

mkdir /home/jagodoy/grupolince/lynx_genomes_5x/BAM_files_final/stats_information # Los transfiero a la carpeta BAM para poder fusionarlos con los de los mapeados. 


# Voy a llevar la cuenta de los que estoy haciendo en una hoja excel para hacerlo sobre todos los proyectos que tenemos. 

cd /home/jagodoy/backup/raw_data/LYNX_06_08/LYNX_06_08_not_trimmed # LYNX_06_08
cd /home/jagodoy/backup/raw_data/LYNX_09/LYNX_09_not_trimmed # LYNX_09
cd /home/jagodoy/backup/raw_data/LYNX_13/20170127/LYNX_13_not_trimmed # LYNX_13
cd /home/jagodoy/backup/raw_data/LYNX_15/20170206/FASTQ # LYNX_15
cd /home/jagodoy/backup/raw_data/LYNX_16/20170206/FASTQ # LYNX_16
cd /home/jagodoy/backup/raw_data/MACROGEN/MACROGEN_not_trimmed # MACROGEN
cd /home/jagodoy/backup/raw_data/genome_project_raw_data_used_for_5x_project # genome proyect

# Corre el BARCODEID que corresponda
ARRAY=($(ls *.fastq.gz |  cut -d'_' -f1-3 | uniq))                     

# This part calculates how many reads do I have in the fastq file and name with the sample name so that latter we can added if they are coming from the same library.
# In case they are different libraries, you should use it, cause you have to calculate your statistics per library not per sample. 

for i in ${ARRAY[@]}
do
echo $i 
echo ${BARCODEID["${i}"]}_${i} >> kk
zcat ${i}_1.fastq.gz | wc -l | awk '{print $1/4}' > /home/jagodoy/backup/raw_data/stats/${BARCODEID["${i}"]}_${i}.borrar1.rawseq
zcat ${i}_2.fastq.gz | wc -l | awk '{print $1/4}' > /home/jagodoy/backup/raw_data/stats/${BARCODEID["${i}"]}_${i}.borrar2.rawseq
done



# # For MACROGEN SAMPLES:
# ARRAY=($(ls *.fastq.gz |  cut -d'_' -f1 | uniq))
# for i in ${ARRAY[@]}
# do
# echo $i
# echo ${BARCODEID["${i}"]}_${i}
# zcat ${i}_R1.fastq.gz | wc -l | awk '{print $1/4}' > /home/jagodoy/backup/raw_data/stats/${BARCODEID["${i}"]}_${i}.borrar1.rawseq
# zcat ${i}_R2.fastq.gz | wc -l | awk '{print $1/4}' > /home/jagodoy/backup/raw_data/stats/${BARCODEID["${i}"]}_${i}.borrar2.rawseq
# done
#
# For the genome proyect I run them lately after copying the former files, so I keep them in the same folder and them copy and move.
# 
# # For genome project
# ARRAY=($(ls *.fastq.gz |  cut -d'_' -f1-3 | uniq | grep -v "^6")) # grep -v "^6" es para quitar todos los run de candiles que se corren aparte
# for i in ${ARRAY[@]}
# do
# echo $i
# echo ${BARCODEID["${i}"]}_${i}
# zcat ${i}_1.fastq.gz | wc -l | awk '{print $1/4}' > ${BARCODEID["${i}"]}_${i}.borrar1.rawseq
# zcat ${i}_2.fastq.gz | wc -l | awk '{print $1/4}' > ${BARCODEID["${i}"]}_${i}.borrar2.rawseq
# done
# 
# # For CANDILES:
# ARRAY=($(ls *.fastq.gz |  cut -d'_' -f1-4 | uniq | grep "^6"))                     
# for i in ${ARRAY[@]}
# do
# echo $i
# echo ${BARCODEID["${i}"]}_${i}
# zcat ${i}_1.fastq.gz | wc -l | awk '{print $1/4}' > ${BARCODEID["${i}"]}_${i}.borrar1.rawseq
# zcat ${i}_2.fastq.gz | wc -l | awk '{print $1/4}' > ${BARCODEID["${i}"]}_${i}.borrar2.rawseq
# done

# --> to do with genome proyect:

# for sample in "${SAMPLESLIST[@]}"
# SAMPLESLIST=($(echo ${BARCODEID[@]} | tr ' ' '\n' | sort | uniq | tr ' ' '\n'))
# for sample in "${SAMPLESLIST[@]}"
# do
# cat "${sample}"*.borrar1.rawseq | awk '{sum+=$1}END{print sum}' > "${sample}"_R1.rawseq
# cat "${sample}"*.borrar2.rawseq | awk '{sum+=$1}END{print sum}' > "${sample}"_R2.rawseq
# done
# rm *borrar*
# --> scp /home/jagodoy/backup/raw_data/genome_project_raw_data_used_for_5x_project/*rawseq /home/jagodoy/grupolince/lynx_genomes_5x/BAM_files_final/stats_information
# --> mv /home/jagodoy/backup/raw_data/genome_project_raw_data_used_for_5x_project/*rawseq /home/jagodoy/backup/raw_data/stats/


cd /home/jagodoy/backup/raw_data/stats

SAMPLESLIST=($(echo ${BARCODEID[@]} | tr ' ' '\n' | sort | uniq | tr ' ' '\n'))


for sample in "${SAMPLESLIST[@]}"
do
cat "${sample}"*.borrar1.rawseq | awk '{sum+=$1}END{print sum}' > "${sample}"_R1.rawseq
cat "${sample}"*.borrar2.rawseq | awk '{sum+=$1}END{print sum}' > "${sample}"_R2.rawseq
done

rm *borrar*
scp *rawseq /home/jagodoy/grupolince/lynx_genomes_5x/BAM_files_final/stats_information

cd /home/jagodoy/grupolince/lynx_genomes_5x/BAM_files_final/

  
 # BAM  
 ######################################################################### 
  
# Generalizo el samplelist: (lo podría haber hecho así también antes)
  
SAMPLELIST=($(ls *_recal_round-1.bam | uniq ))
for sample in "${SAMPLELIST[@]}"
do
echo $sample
samtools flagstat $sample  >  /home/GRUPOS/grupolince/lynx_genomes_5x/BAM_files_final/stats_information/${sample}_bam.stats
done


  
SAMPLELIST=($(ls *_recal_round-1.bam | uniq ))
for sample in "${SAMPLELIST[@]}"
do
echo $sample
samtools flagstat $sample  > ${sample}_bam.stats
done

# Checks:

# * He tenido que modificar h_lp_do_007 a c_lp_do_007, y c_lr_xx y c_lc_xx a c_lr_zz y c_lc_zz respectivamente para que sean comparables. 
cd /home/mlucena/grupolince/lynx_genomes_5x/BAM_files_final
ls *_recal_round-1.bam | cut -d "_" -f1-4 | sort | uniq > stats_information/recal_list
cd /home/GRUPOS/grupolince/lynx_genomes_5x/BAM_files_final/stats_information/
ls *rawseq | cut -d "_" -f1-4 | sort | uniq > rawseq_list
ls *stats | cut -d "_" -f1-4 | sort | uniq > stats_list

diff rawseq_list stats_list # ok! none! 
diff recal_list stats_list  # ok! none!

## 



```


## Insert size distribution

Hay dos formas de calcularlo

```{r, engine=bash, eval=FALSE}

# I run picard tool with the option H, that output an histogram. 
cd /home/GRUPOS/grupolince/lynx_genomes_5x/BAM_files_final/
  
for sample in *recal_round-1.bam
do
echo $sample
java  -Xms10g  -Xmx40g -jar /home/tmp/Software/Picard/picard-tools-1.66/CollectInsertSizeMetrics.jar I=$sample H=${sample/.bam/.histogram_length} O=${sample/.bam/.picard_length}
done

mv /home/GRUPOS/grupolince/lynx_genomes_5x/BAM_files_final/*.picard_length /home/GRUPOS/grupolince/lynx_genomes_5x/BAM_files_final/stats_information/

mv /home/GRUPOS/grupolince/lynx_genomes_5x/BAM_files_final/*.histogram_length /home/GRUPOS/grupolince/lynx_genomes_5x/BAM_files_final/stats_information/

for sample in *.histogram_length
do
echo "$sample""--->""${sample/.histogram_length/_histogram_length.pdf}"
mv $sample ${sample/.histogram_length/_histogram_length.pdf}
done

```


## Coverage

Hay dos formas de calcularlo

```{r, engine=bash, eval=FALSE}

# GATK: Slower.

# REF=/home/GRUPOS/grupolince/reference_genomes/lynx_pardinus_genome/lp23.fa					
# SAMPLELIST=($(ls *_recal_round-1.bam | uniq ))
# for sample in "${SAMPLELIST[@]}"
# do
# java -XX:MaxMetaspaceSize=1g -XX:+UseG1GC  -XX:+UseStringDeduplication -Xms10g  -Xmx64g -jar /home/tmp/Software/GATK_3.4/GenomeAnalysisTK.jar -I ${sample} -o ${sample/.bam/.coverage} -T DepthOfCoverage -R $REF
# done

# Samtools: The one I am doing 
# If I run it again add the last sed after the loop so that is comparable with the previous stats!!! 

SAMPLELIST=($(ls *_recal_round-1.bam | uniq ))
rm global_coverage.tsv
echo -e "sample_name\tcoverage_based_samtools\tstdev_based_samtools" > global_coverage.tsv
for sample in "${SAMPLELIST[@]}"
do
echo $sample
DEPTH=$(samtools depth $sample | awk '{sum+=$3; sumsq+=$3*$3} END { print sum/2413209059; print sqrt(sumsq/2413209059 - (sum/2413209059)**2)}')
paste \
<(echo $sample ) \
<(echo $DEPTH ) |\
sed 's/ /\t/g'| sed 's/\t\+/\t/g' >>  global_coverage.tsv
done

# I modify the name so that it has a common field with the previous report:
cat global_coverage.tsv | sed 's/_recal_round-1.bam//g' | sed 's/\t/,/g' > coverage_samtools
rm global_coverage.tsv





```

<!-- ## Count primary alignment: -->

<!-- samtools view -c -f 0x40 -F 0x904 --> This will exclude unmapped (0x4), secondary (0x100), and supplementary (0x800) alignments. -->


<!-- ```{bash} -->
<!-- screen -S primary_aligments -->

<!-- cd /home/GRUPOS/grupolince/lynx_genomes_5x/BAM_files_final -->
<!-- SAMPLELIST=($(ls *_recal_round-1.bam | sort | uniq ))   -->
<!-- rm primary_alignments.csv -->
<!-- echo -e "sample_name\tprimary_alignments" > primary_alignments.csv -->
<!-- for sample in "${SAMPLELIST[@]}" -->
<!-- do -->
<!-- echo $sample -->
<!-- PRIMARY_ALIGNMENT=$(samtools view -c -f 0x40 -F 0x904 $sample | cut -f1 | sort | uniq | wc -l) -->
<!-- paste \ -->
<!-- <(echo $sample ) \ -->
<!-- <(echo $PRIMARY_ALIGNMENT ) |\ -->
<!-- sed 's/ /,/g' | sed 's/_recal_round-1.bam//g' >>  primary_alignments.csv -->
<!-- done -->

<!-- ``` -->

## Summary report


```{r, engine=bash, eval=FALSE}

cd /home/GRUPOS/grupolince/lynx_genomes_5x/BAM_files_final
SAMPLELIST=($(ls *_recal_round-1.bam | cut -d "_" -f1-4 | sort | uniq )) 
cd /home/GRUPOS/grupolince/lynx_genomes_5x/BAM_files_final/stats_information/
  
echo "sample_name,total_seq,total_reads,duplicates,mapped,properly_pair,with_mat_to_another_chr,uniq_mapped,uniq_mapped_vs_total_reads,duplicates_vs_total_reads,mapped_vs_total_seq,properly_pair_vs_total_seq,with_mat_to_another_chr_vs_total_seq,coverage_based_distribution" > raw.stats 
for sample in "${SAMPLELIST[@]}"
do
echo "${sample}"
NAME="${sample}"
TOTAL_SEQ="$(cat /home/GRUPOS/grupolince/lynx_genomes_5x/BAM_files_final/stats_information/"${sample}"*.rawseq | awk '{sum+=$1}END{print sum}')"
TOTAL_READS="$(grep "in total" "${sample}"*.stats | cut -d "+" -f 1)"

DUPLICATES="$(grep "duplicates" "${sample}"*.stats | cut -d "+" -f 1)"

MAPPED="$(grep "0 mapped" "${sample}"*.stats | cut -d "+" -f 1)"

PROPERLY_PAIR="$(grep "properly paired" "${sample}"*.stats | cut -d "+" -f 1)"

WITH_MATE_TO_ANOTHER_CHR="$(grep "with mate mapped to a different chr$" "${sample}"*.stats | cut -d "+" -f 1)"

UNIQ_MAPPED="$(echo "$MAPPED - $DUPLICATES" | bc )"
UNIQ_MAPPED_vs_TOTAL_READS="$(echo "scale=5; $UNIQ_MAPPED *100 / $TOTAL_READS"  | bc -l)"
DUPLICATES_vs_TOTAL_READS="$(echo "scale=5; $DUPLICATES / $TOTAL_READS" | bc -l )"
MAPPED_vs_TOTAL_SEQ="$(echo "scale=5; $MAPPED *100  / $TOTAL_SEQ" | bc )"
PROPERLY_PAIR_vs_TOTAL_SEQ="$(echo "scale=5; $PROPERLY_PAIR *100 / $TOTAL_SEQ" | bc )"
WITH_MATE_TO_ANOTHER_CHR_vs_TOTAL_SEQ="$(echo "scale=5; $WITH_MATE_TO_ANOTHER_CHR *100 / $TOTAL_SEQ" | bc )"
COVERAGE_BASED_DISTRIBUTION=$(tail -n+12 "${sample}"*.picard_length  | awk '($1 > 30) {print $1*$2}' | paste -sd+ | bc | awk '{print $1/2413209059}')

echo "$NAME,$TOTAL_SEQ,$TOTAL_READS,$DUPLICATES,$MAPPED,$PROPERLY_PAIR,$WITH_MATE_TO_ANOTHER_CHR,$UNIQ_MAPPED,$UNIQ_MAPPED_vs_TOTAL_READS,$DUPLICATES_vs_TOTAL_READS,$MAPPED_vs_TOTAL_SEQ,$PROPERLY_PAIR_vs_TOTAL_SEQ,$WITH_MATE_TO_ANOTHER_CHR_vs_TOTAL_SEQ,$COVERAGE_BASED_DISTRIBUTION" | sed 's/ //g' >> raw.stats 
done

LANG=en_EN  join -1 1 -2 1 -t","  <(head -n 2 raw.stats && tail -n +3 raw.stats | LANG=en_EN sort -k 1,1) <(head -n 2 coverage_samtools && tail -n +3 coverage_samtools | LANG=en_EN sort -k 1,1 ) | sed 's/,/;/g' | sed 's/\./,/g' > global_stats_all_samples.csv # de esta manera tiene en cuenta el header. 
 
# 138732275 + 0 in total (QC-passed reads + QC-failed reads)
# 301702 + 0 duplicates
# 135997388 + 0 mapped (98.03%:-nan%)
# 138732275 + 0 paired in sequencing
# 69315360 + 0 read1
# 69416915 + 0 read2
# 132686988 + 0 properly paired (95.64%:-nan%)
# 135464480 + 0 with itself and mate mapped
# 532908 + 0 singletons (0.38%:-nan%)
# 2667465 + 0 with mate mapped to a different chr
# 1511516 + 0 with mate mapped to a different chr (mapQ>=5)

```

## Download report. 

```{bash}
scp mlucena@genomics-b.ebd.csic.es:/home/GRUPOS/grupolince/lynx_genomes_5x/BAM_files_final/stats_information/global_stats_all_samples.csv ./Desktop


```

