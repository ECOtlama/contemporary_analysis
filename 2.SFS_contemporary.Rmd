---
title: "pipeline_contemporary_sfs"
output: html_document
---

# Generating SAF,SFS & thetas per site.

## Introduction and populations:

I am going to run SAF in my target populations.

Populations:

c_ll_no 
c_ll_po 
c_ll_ki 

c_lp_do 
c_lp_sm 

I checked that all bam files were indexed --> OK

## Global variables:


```{r, engine=bash, eval=FALSE}

RUTA=/home/mlucena/ANGSD_analysis 

cd $RUTA/whole_genome_analysis/sfs

#To launch one by one

# POP="c_lp_do_n012"  # <--CHANGE POP HERE
POP="c_ll_ki_n008"
screen -S "$POP"_sfs_wg
# screen -S "$POP"_thetas
#POP="all pops"
#POP="c_lp_do_n012"  # <--CHANGE POP HERE

#script "$POP"_sfs_wg.log
# script "$POP"_thetas.log

#POP="c_lp_sm_n012"  # <--CHANGE POP HERE
POP="c_ll_ki_n008"  # <--CHANGE POP HERE

# POPS=(c_lp_sm_n012 c_ll_ki_n008)   # <--CHANGE POP HERE

THREADS=15                     # no. of computer cores used by bwa and samtools. 20 = OK, >20 = ask people first!

RUTA=/home/mlucena/ANGSD_analysis/depth_calculus
ANGSD="/opt/angsd/angsd"
NGSTOOLS="/opt/angsd/angsd/misc"
REF="/home/GRUPOS/grupolince/reference_genomes/lynx_pardinus_genome/lp23_without_repetitive_transposable_low_complexity.fa"

###  Ojo para correr esto en el futuro: hay que tener en cuenta que este no tiene filtrado los sitios repetitivos y low complexity y que si nos interesa quitarlos habría que recortarlo luego, o usar una referencia libre de estos sitios. 
ANC="/home/GRUPOS/grupolince/reference_genomes/lynx_rufus_genome/c_lr_zz_0001_recal1.fa"
###  

FILTER1=" -uniqueOnly 1 -remove_bads 1 -only_proper_pairs 1 -baq 1 -C 50 "
FILTER2=" -minMapQ 30 -minQ 20 -doCounts 1 "
RUTA=/home/mlucena/ANGSD_analysis


# 6/3/2018 --> Lo he lanzado en un loop que acaba en SAF posprob. 

#for POP in  ${POPS[@]}
#do

read POP mean sd mean_truncated sd_truncated maxDepth minDepth maxDepth_truncated minDepth_truncated < $RUTA/depth_calculus/"${POP}"_mean_sd_depthGlobal_lynx_per_pop_mean_folds_0.95.csv

N_IND=$(echo ${POP: -3} )
MIN_IND=$(expr $N_IND / 2)

# Sanity checks:

echo $POP
echo $N_IND
echo $MIN_IND
echo $maxDepth_truncated
echo $minDepth_truncated

```

## Unfolded SAF 

```{r, engine=bash, eval=FALSE}


##########################
#  SAF (likelihood):     
##########################

echo "-------$POP----------SAF (likelihood)-----------------------------------------"

$ANGSD/angsd -P $THREADS -b $RUTA/whole_genome_analysis/"$POP".bamlist -ref $REF -anc $ANC \
-out "$POP".unfolded-lr \
$FILTER1 \
$FILTER2 \
-GL 1 -doSaf 1  \
-minInd  $MIN_IND -setMaxDepth $maxDepth_truncated -setMinDepth $minDepth_truncated


```

## SFS 

```{r, engine=bash, eval=FALSE}

##########################
#  SFS                   #(I dont require the -rf file as the saf already only contains the -rf sites
##########################
echo "-------$POP----------SFS------------------------------------------------------"

$NGSTOOLS/realSFS "$POP".unfolded-lr.saf.idx  -P $THREADS > "$POP".unfolded-lr.sfs


```

##  SAF (postprob) 

```{r, engine=bash, eval=FALSE}


echo "-------$POP----------SAF (postprob)-----------------------------------------"

$ANGSD/angsd -P $THREADS -b $RUTA/whole_genome_analysis/"$POP".bamlist -ref $REF -anc $ANC \
-out "$POP".unfolded-lr.postprob \
$FILTER1 \
$FILTER2 \
-GL 1 -doSaf 1  \
-minInd  $MIN_IND -setMaxDepth $maxDepth_truncated -setMinDepth $minDepth_truncated \
-doThetas 1 -pest "$POP".unfolded-lr.sfs 

done

```




##  Make bed theta        

La población de sierra morena se volvió a lanzar desde aquí en abril/mayo de 2018 porque me di cuenta de que los archivos que había creado se habína corrompido por el camino. Antes de esto (archivos saf, sfs, etc) sí están bien. 

```{r, engine=bash, eval=FALSE}

echo "-------$POP----------Make bed theta -----------------------------------------"

$NGSTOOLS/thetaStat print $POP.unfolded-lr.postprob.thetas.idx > $POP.printed.stats

# Transform the coordinates to use 0Based bedtools

awk 'NR>1 {print  $1" "$2-1" "$2}' $POP.printed.stats > $POP.0basedcoordinates.borrar # --> 1, 2, 3
# log transformation of watterson and pairwise:
awk 'NR>1 {print  "e("$3")"}' $POP.printed.stats | bc -l > "$POP".watterson.borrar # --> 4
awk 'NR>1 {print  "e("$4")"}' $POP.printed.stats | bc -l > "$POP".pairwise.borrar # --> 5
                                                                                  # --> PAIRWISE - WATTERSON
awk 'NR>1 {print  "e("$5")"}' $POP.printed.stats | bc -l > "$POP".thetaFL.borrar # --> 6
awk 'NR>1 {print  "e("$6")"}' $POP.printed.stats | bc -l > "$POP".thetaH.borrar # --> 7
awk 'NR>1 {print  "e("$7")"}' $POP.printed.stats | bc -l > "$POP".thetaL.borrar # --> 8

# Create a header for the "$POP".transformedThetas file

echo "scaffold position1 position2 watterson pairwise pairwise-watterson thetaFL thetaH thetaL" > "$POP".transformedThetas

# ¿Tengo las mismas posiciones?

wc -l $POP*borrar



paste $POP.0basedcoordinates.borrar "$POP".watterson.borrar "$POP".pairwise.borrar "$POP".thetaFL.borrar "$POP".thetaH.borrar "$POP".thetaL.borrar | \
awk -v OFS='\t'  '{print $1, $2, $3, $4, $5, $5-4, $6, $7, $8}' >> "$POP".transformedThetas

rm "$POP".*.borrar
mv $POP.printed.stats /backup/mlucena/intermediate_files_ANGSD/whole_genome_analysis/
scp *.transformedThetas /backup/mlucena/intermediate_files_ANGSD/whole_genome_analysis/

# done

```



# ----------------------------------

## SFS graphical representation


First we copy the SFS files to our local folder.

```{bash}
scp mlucena@genomics-b.ebd.csic.es://home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/*sfs /Users/marialucenaperez/Owncloud/publico/WG_diversity/ANGSD/sfs

# I need to change name as R does not like "-lr.sfs"

cd /Users/marialucenaperez/Owncloud/publico/WG_diversity/ANGSD/sfs/
for i in *sfs
do 
#echo $i
echo $i "--->" ${i/-lr.sfs/_lr.sfs}
mv $i ${i/-lr.sfs/_lr.sfs}
done


```

Now we run the script. 

```{r}
library(dplyr)
library(magrittr)

wd<-"/Users/marialucenaperez/Owncloud/publico/WG_diversity/ANGSD/sfs/"

my_files_unfolded = list.files(path = wd, pattern="*sfs$")    
for (i in 1:length(my_files_unfolded)) 
{ assign(my_files_unfolded[i], (scan(paste(wd,my_files_unfolded[i], sep = ""), sep = " ", dec = ".")) %>% .[!is.na(.)])}

norm <- function(x) x/sum(x)

for (i in 1:length (my_files_unfolded))
{
  temp1=(eval(parse(text=my_files_unfolded[i])))
  temp2=norm(temp1[-1])
  temp3=norm(temp2[-c(length(temp2))])
  barplot(temp3,xlab="Chromosomes",
          names=1:length(temp3),ylab="Proportions",main=my_files_unfolded[i],col='grey')
  dev.print(device=postscript, paste("/Users/marialucenaperez/Owncloud/publico/WG_diversity/ANGSD/sfs/",my_files_unfolded[i],".eps", sep =""), onefile=FALSE, horizontal=FALSE)
  dev.off()
  rm(temp1)
  rm(temp2)
  rm(temp3)
}


```

# ------------------------------------------------
# 18/05/2018 PROBLEMA NO Y DO

Quería hacer un filtrado y me di cuenta de un problema que solucioné por el camino. En el script real esta parte se puede obviar e irías directamente al siguiente apartado. 

## Global diversity per feaure

Este archivo generado hasta ahora contiene todos los sitios, incluyendo los repetitivos. Hasta ahora he corrido todo usando este archivo pero el 17/05/2018 me di cuenta de que incluía las repeticiones, así que voy a repetir el proceso hasta aquí dejando fuera estas low complexity and repetitive regions. 

Para ello primero filtro el archivo

### Filtering repetitive regions 

Mi archivo de interés es el archivo Theta. 

```{bash}

mv c_ll_no_n008_separated_by_scaffold/*transformedThetas .
mv c_ll_ki_n013_separated_by_scaffold/*transformedThetas .
mv c_lp_do_n012_separated_by_scaffold/*transformedThetas .
mv c_ll_po_n008_separated_by_scaffold/*transformedThetas .
mv c_lp_sm_n019_separated_by_scaffold/*transformedThetas .

head -n1 c_ll_ki_n013.transformedThetas > header_thetas_file
for theta_file in *transformedThetas
do
echo $theta_file
bedtools subtract -a <(tail -n +2 $theta_file) -b /home/mlucena/grupolince/reference_genomes/lynx_pardinus_genome/repeats_and_lowcomp_no_redundant.bed | cat header_thetas_file - > ${theta_file/transformedThetas/transformedThetas_filtered}
done


```

Esto me devuelve un mensaje de error para dos poblaciones Noruega y Doñana.

## Sanity check -- > Problema detectado

Nos hemos dado cuenta al hacer el filtrado que bedtools cantaba porque decía que había archivos que tenían 8 columnas en vez de 9. Hemos ido a /backup/mlucena/intermediate_files_ANGSD/whole_genome_analysis y nos hemos dado cuenta que la columna que falta es watterson. Esto es grave porque es la primera columna, así que pasaría desapercibido.

¿Cómo lo he sabido?

```{bash}
cd /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs

awk 'NF == 8' < c_ll_no_n008.transformedThetas | head

# Me sale a partir de lp23.s37389. Así que busco ese scaffold en printed.stats para ver si está bien.  

cd

grep lp23.s37390  c_ll_no_n008.printed.stats | head > kk

# Efectivamente tiene todas las columnas, así que vamos a comprobar cual es la que luego falta comparando con el c_ll_no_n008.transformedThetas para ese scaffold. 

# Lo hago así:

awk '{print  "e("$3")"}' kk  | bc -l  # ¡AUSENTE!
awk '{print  "e("$4")"}' kk  | bc -l  # --> Presente
awk '{print  "e("$5")"}' kk  | bc -l  # --> Presente
awk '{print  "e("$6")"}' kk  | bc -l  # --> Presente
awk '{print  "e("$7")"}' kk  | bc -l  # --> Presente

```

Dada la gravedad del descubrimiento voy a comprobar si hay más archivos que les falten columnas: 

```{bash}

awk 'NF !=9' < c_ll_po_n008.transformedThetas | head # --> Perfecto!
awk 'NF !=9' < c_ll_no_n008.transformedThetas | head # PROBLEMA
awk 'NF !=9'< c_lp_do_n012.transformedThetas | head # PROBLEMA
awk 'NF !=9' < c_lp_sm_n019.transformedThetas | head # --> Perfecto!
awk 'NF !=9' c_ll_ki_n013.transformedThetas | head # --> Perfecto!

```

¿Qué poblaciones tienen este problema?

Efectivamente las que dieron el mensaje de aviso:

Noruega
Doñana

¿A partir de qué scaffold?
```{bash}
awk 'NF == 8' < c_ll_no_n008.transformedThetas | head

# lp23.s37389	1704	1705	.00000210987258195238	.00001016749185751371	8.05762e-06	.00000022186028500950	.00000116586646570892

# A partir del scaffold lp23.s37389 para noruega.


 awk 'NF == 8' < c_lp_do_n012.transformedThetas | head

# lp23.s37350	1364	1365	.00000006491013903094	.00000059407990881920	5.2917e-07	.00000000363581697066	.00000003427293024659	

# A partir del scaffold lp23.s37350 para doñana. 

# Para ambos:
# Cuando hago el tail me doy cuenta de que es hasta el final 


# Empiezo con doñana.

# Qué linea es esa?

cd /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs
grep -nr lp23.s37350 c_lp_do_n012.transformedThetas | head

# 1369623421:lp23.s37350	84	85	.00000160262764124604	.00000100177412987377	-6.00854e-07.00000266362654746090	.00000011972073675458	.00000056074756833874

cd /backup/mlucena/intermediate_files_ANGSD/whole_genome_analysis
grep -nr lp23.s37350 c_lp_do_n012.printed.stats | head

# 1369623421:lp23.s37350	85	-13.257177	-13.813738	-12.835822	-15.938104	-14.393995

# Para Doñana, ¡coinciden! El problema es a partir de la linea 1369623421


# ¿Y en Noruega?

cd /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs
grep -nr lp23.s37389 c_ll_no_n008.transformedThetas | head

# 1368906858:lp23.s37389	57	58	.00000280538155921492	.00000887284561212872	6.06746e-06	.00002134658039437027	.00000149690337555105	.00000518486890558351

cd /backup/mlucena/intermediate_files_ANGSD/whole_genome_analysis
grep -nr lp23.s37389 c_ll_no_n008.printed.stats | head

# 1368906858:lp23.s37389	58	-11.239233	-11.632515	-10.754619-13.412112	-12.169766

# Para Noruega, ¡coincide! El problema es a partir de la linea 1368906858

```

Corro desde el scaffold que va mal para cada una de las poblaciones. 

### Norway: Thetas file

```{bash}

# Población: c_ll_no_n008

#Corro todo para lo que sea mayor que 1368906857, por lo que debe empezar en el 1368906858 

# Me voy a backup que es donde tengo mis archivos:

cd /backup/mlucena/intermediate_files_ANGSD/whole_genome_analysis

# Transform the coordinates to use 0Based bedtools

awk 'NR>1368906857 {print  $1" "$2-1" "$2}' c_ll_no_n008.printed.stats > c_ll_no_n008.0basedcoordinates.borrar # --> 1, 2, 3
# log transformation of watterson and pairwise:
awk 'NR>1368906857 {print  "e("$3")"}' c_ll_no_n008.printed.stats | bc -l > c_ll_no_n008.watterson.borrar # --> 4
awk 'NR>1368906857 {print  "e("$4")"}' c_ll_no_n008.printed.stats | bc -l > c_ll_no_n008.pairwise.borrar # --> 5
                                                                                  # --> PAIRWISE - WATTERSON
awk 'NR>1368906857 {print  "e("$5")"}' c_ll_no_n008.printed.stats | bc -l > c_ll_no_n008.thetaFL.borrar # --> 6
awk 'NR>1368906857 {print  "e("$6")"}' c_ll_no_n008.printed.stats | bc -l > c_ll_no_n008.thetaH.borrar # --> 7
awk 'NR>1368906857 {print  "e("$7")"}' c_ll_no_n008.printed.stats | bc -l > c_ll_no_n008.thetaL.borrar # --> 8

# Create a header for the "$POP".transformedThetas file

# Sanity check:

wc -l *borrar
# 283248 c_ll_no_n008.0basedcoordinates.borrar
# 283248 c_ll_no_n008.pairwise.borrar
# 283248 c_ll_no_n008.thetaFL.borrar
# 283248 c_ll_no_n008.thetaH.borrar
# 283248 c_ll_no_n008.thetaL.borrar
# 283248 c_ll_no_n008.watterson.borrar

# ¡Bien!

# Ahora hago elimino a partir de esa linea en mi archivo de Thetas:

# Sanity:

cd /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs
wc -l c_ll_no_n008.transformedThetas
# 1369190105
awk 'NR>1368906857 {print $0}' c_ll_no_n008.transformedThetas | wc -l 
# 283248
awk 'NR<=1368906857 { print }' c_ll_no_n008.transformedThetas | wc -l 
# 1368906857

# 1368906857 + 283248 =  1369190105 --> ¡Bien!

# Ahora podría pegar los resultantes de *.borrar, eliminar las filas NR<=1368906857 en mi archivo Theta y pegarles estas. 

# Creo mi archivo con theta nuevo:

cd /backup/mlucena/intermediate_files_ANGSD/whole_genome_analysis

awk 'NR<=1368906857 { print }' c_ll_no_n008.transformedThetas > c_ll_no_n008.transformedThetas_subset

wc -l c_ll_no_n008.transformedThetas_subset
# 1368906857 c_ll_no_n008.transformedThetas_subset

# Ahora junto estas columnas y las añado a nuestro archivo subset. 

paste c_ll_no_n008.0basedcoordinates.borrar c_ll_no_n008.watterson.borrar c_ll_no_n008.pairwise.borrar c_ll_no_n008.thetaFL.borrar c_ll_no_n008.thetaH.borrar c_ll_no_n008.thetaL.borrar | awk -v OFS='\t'  '{print $1, $2, $3, $4, $5, $5-4, $6, $7, $8}' >> c_ll_no_n008.transformedThetas_subset

# ¿Y ahora cuanto mide?

wc -l c_ll_no_n008.transformedThetas_subset
# 1369190105 c_ll_no_n008.transformedThetas_subset
#Perfecto.

awk 'NF != 9' < c_ll_no_n008.transformedThetas_subset
# Nada! Perfecto!

mv c_ll_no_n008.transformedThetas_subset c_ll_no_n008.transformedThetas

```


### Doñana: Thetas file

```{bash}

# Población: c_lp_do_n012

#Corro todo para lo que sea mayor que 1369623420, por lo que debe empezar en el 1369623421 

# Me voy a backup que es donde tengo mis archivos:

cd /backup/mlucena/intermediate_files_ANGSD/whole_genome_analysis

# Transform the coordinates to use 0Based bedtools

awk 'NR>1369623420 {print  $1" "$2-1" "$2}' c_lp_do_n012.printed.stats > c_lp_do_n012.0basedcoordinates.borrar # --> 1, 2, 3
# log transformation of watterson and pairwise:
awk 'NR>1369623420 {print  "e("$3")"}' c_lp_do_n012.printed.stats | bc -l > c_lp_do_n012.watterson.borrar # --> 4
awk 'NR>1369623420 {print  "e("$4")"}' c_lp_do_n012.printed.stats | bc -l > c_lp_do_n012.pairwise.borrar # --> 5
                                                                                  # --> PAIRWISE - WATTERSON
awk 'NR>1369623420 {print  "e("$5")"}' c_lp_do_n012.printed.stats | bc -l > c_lp_do_n012.thetaFL.borrar # --> 6
awk 'NR>1369623420 {print  "e("$6")"}' c_lp_do_n012.printed.stats | bc -l > c_lp_do_n012.thetaH.borrar # --> 7
awk 'NR>1369623420 {print  "e("$7")"}' c_lp_do_n012.printed.stats | bc -l > c_lp_do_n012.thetaL.borrar # --> 8

# Create a header for the "$POP".transformedThetas file

# Sanity check:

wc -l c_lp*borrar
#  282955 c_lp_do_n012.0basedcoordinates.borrar
#  282955 c_lp_do_n012.pairwise.borrar
#  282955 c_lp_do_n012.thetaFL.borrar
#  282955 c_lp_do_n012.thetaH.borrar
#  282955 c_lp_do_n012.thetaL.borrar
#  282955 c_lp_do_n012.watterson.borrar

# ¡Bien!

# Ahora hago elimino a partir de esa linea en mi archivo de Thetas:

# Sanity:

cd /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs
wc -l c_lp_do_n012.transformedThetas
# 1369906375

awk 'NR>1369623420 {print $0}' c_lp_do_n012.transformedThetas | wc -l 
# 282955

awk 'NR<=1369623420 { print }' c_lp_do_n012.transformedThetas | wc -l 
# 1369623420

# 1369623420 + 282955 = 1369906375 --> ¡Bien!

# Ahora podría pegar los resultantes de *.borrar, eliminar las filas NR<=1369623420 en mi archivo Theta y pegarles estas. 

# Creo mi archivo con theta nuevo:

cd /backup/mlucena/intermediate_files_ANGSD/whole_genome_analysis

awk 'NR<=1369623420 { print }' c_lp_do_n012.transformedThetas > c_lp_do_n012.transformedThetas_subset

# Ahora junto estas columnas y las añado a nuestro archivo subset. 

paste c_lp_do_n012.0basedcoordinates.borrar c_lp_do_n012.watterson.borrar c_lp_do_n012.pairwise.borrar c_lp_do_n012.thetaFL.borrar c_lp_do_n012.thetaH.borrar c_lp_do_n012.thetaL.borrar | awk -v OFS='\t'  '{print $1, $2, $3, $4, $5, $5-4, $6, $7, $8}' >> c_lp_do_n012.transformedThetas_subset

wc -l c_lp_do_n012.transformedThetas_subset
# 1369906375 c_lp_do_n012.transformedThetas_subset
# Perfecto!

awk 'NF != 9' < c_lp_do_n012.transformedThetas_subset
# Nada, perfecto!

mv c_lp_do_n012.transformedThetas_subset c_lp_do_n012.transformedThetas

```

Como ha ido bien en principio, elimino los archivos *borrar.

```{bash}
rm *borrar
```

Elimino todos los filtrados, porque durante este tiempo que hemos solucionado este problema hemos decidido filtrar también los que tienen baja mappability (script mappability).

```{bash}
cd /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs
rm *transformedThetas_filtered
```

Copiar a mi ruta ANGSD/wholegenome/sfs

```{bash}
cd /backup/mlucena/intermediate_files_ANGSD/whole_genome_analysis/
scp c_lp_do_n012.transformedThetas /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs
scp c_ll_no_n008.transformedThetas /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs
```

Hago sanity checks en mi carpeta donde los he movido, por segunda vez.

```{bash}

cd /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs

awk 'NF != 9' < c_lp_do_n012.transformedThetas | head # Ya están comprobados cuando los generé.
awk 'NF != 9' < c_ll_no_n008.transformedThetas | head # Ya están comprobados cuando los generé.

awk 'NF != 9' < c_lp_sm_n019.transformedThetas | head # --> Bien!
awk 'NF != 9' < c_ll_ki_n013.transformedThetas | head # --> Bien!
awk 'NF != 9' < c_ll_po_n008.transformedThetas | head # --> Bien!


tail c_lp_do_n012.transformedThetas
tail c_ll_no_n008.transformedThetas

tail c_lp_sm_n019.transformedThetas # --> Buena pinta
tail c_ll_ki_n013.transformedThetas # --> Buena pinta
tail c_ll_po_n008.transformedThetas # --> Buena pinta

 
```


# ---------------------------------------------

# Filtering & separate by scaffold.

Normalmente ahora pasaríamos a separar las carpetas de theta por scaffold. Como vamos a usar unos archivos filtrados (sin regiones de baja mappability ni de low complexity o repetitivo), voy a crearlos y despues hacemos las carpetas para esos archivos filtrados. 

### Filtering repetitive regions

Mi archivo de interés es el archivo Theta. 

```{bash}

cd /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs
head -n1 c_ll_ki_n013.transformedThetas > header_thetas_file
for theta_file in *transformedThetas
do
echo $theta_file
bedtools subtract -a <(tail -n +2 $theta_file) -b /home/mlucena/grupolince/reference_genomes/lynx_pardinus_genome/repeats_and_lowcomp_no_redundant_mappability.k75_lessthan90.bed | cat header_thetas_file - > ${theta_file/transformedThetas/transformedThetas_filtered}
done

# ¿Cuanto ocupaban sin filtrar y filtrados?

# Not filtered

# 1369113172 c_ll_ki_n013.transformedThetas
# 1369400768 c_ll_po_n008.transformedThetas
# 1383676766 c_lp_sm_n019.transformedThetas
# 1369906375 c_lp_do_n012.transformedThetas
# 1369190105 c_ll_no_n008.transformedThetas

# Filtered

# 1284340665 c_ll_ki_n013.transformedThetas_filtered --> 1369113172 - 1284340665 = 84772507 que he filtrado. 
# 1277662393 c_ll_po_n008.transformedThetas_filtered --> 1369400768 - 1277662393 = 91738375 que he filtrado.
# 1289013196 c_lp_sm_n019.transformedThetas_filtered --> 1383676766 - 1289013196 = 94663570 que he filtrado. 
# 1282704524 c_lp_do_n012.transformedThetas_filtered --> 1369906375 - 1282704524 = 87201851 que he filtrado.
# 1284367841 c_ll_no_n008.transformedThetas_filtered --> 1369190105 - 1284367841 = 84822264 que he filtrado. 

# Ahora vamos a eliminar el archivo thetas de aquí y lo dejamos sólo en backup




#¡Lanzar ahora esto, pero cuidad con la población!
#rm *transformedThetas

```

## Separate by scaffold

Ahora creo las carpetas de theta per scaffold para todas las poblaciones de nuevo partiendo de los archivos filtrados. 

```{bash}

cd /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/

#NEW_FOLDER=("c_ll_no_n008_separated_by_scaffold" "c_lp_do_n012_separated_by_scaffold" "c_lp_sm_n019_separated_by_scaffold" "c_ll_ki_n013_separated_by_scaffold" "c_ll_po_n008_separated_by_scaffold")

NEW_FOLDER=("c_lp_sm_n012_separated_by_scaffold")


for FOLDER in "${NEW_FOLDER[@]}"
do
echo "---------------------------------------------------$FOLDER---------------------------------------------------"
mkdir $FOLDER;
done
 
 
POPS=("c_lp_sm_n012" )

# POPS=("c_ll_no_n008" "c_lp_do_n012")

for POP in "${POPS[@]}"
do
echo $POP
cd /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/
mv "$POP".transformedThetas_filtered "$POP"_separated_by_scaffold/
cd  /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/"$POP"_separated_by_scaffold
# Create multiple files base on one column: transformedThetas:
awk '{print >> $1; close($1)}' "$POP".transformedThetas_filtered
rm scaffold # esto borra la primera linea que forma un archivo llamado scaffold.
# Rename the files: transformedThetas

for file in lp23*
do
echo $file
mv $file ${file/lp23/"$POP".transformedThetas_lp23}
done
done

# Copiamos el archivo filtered a backup

POPS=("c_lp_sm_n019" "c_ll_ki_n013" "c_ll_po_n008" "c_ll_no_n008" "c_lp_do_n012")

cd /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs

for POP in "${POPS[@]}"
do
echo $POP
cd "$POP"_separated_by_scaffold/
scp *.transformedThetas_filtered /backup/mlucena/intermediate_files_ANGSD/whole_genome_analysis
cd /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs
done


```

# ---------------------------------------------

# Diversity calculus

He modificado este script para que lo haga en bloques de 10, por lo que tadaría 10 veces menos. La estrategia consiste en establecer 10 bloques con empezando por un scaffold hasta otro y despues iterar sobre este bloque. Como los 10 bloques se lanzan en un bucle al poner & al final se lanzan los 10 a la vez. Además no necesitamos definir los scaffold a priori como hacíamos antes porque vamos a iterar por todos. 

c_ll_ki_n013 # --> Lanzado
c_ll_po_n008 # --> Lanzado
c_lp_sm_n019 # --> Lanzado
c_ll_no_n008 # --> Lanzado
c_lp_do_n012 # --> Lanzado
c_ll_ki_n008 # --> Lanzado
c_lp_sm_n012 # --> Lanzado


```{bash}

# Run pop by pop:

POP=c_lp_sm_n012 # <--- Change pop here!
screen -S "$POP"_scaffold_per_unit
POP=c_lp_sm_n012 # <--- Change pop here!
script log_screen_"$POP"_scaffold_per_unit.log
POP=c_lp_sm_n012 # <--- Change pop here!

cd /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs

## Variables
SCAFFOLDS_FOLDER=/GRUPOS/grupolince/Lyp_annotation_Apr14_final/LYPA23C.CDS.GENE_promoters.GENE_introns.UTRs.ncRNA.lncRNA.lncRNA_introns.lncRNA_promoters.UCNE.intergenic.nr.gff3.PerScaffold/
DIVERSITY_PER_UNIT_FOLDER=/home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/diversity_per_unit/
SFS_FOLDER=/home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/


echo "---------------------------------------------------$POP---------------------------------------------------"

# Create headers for the outfile
echo -e "scaffold\tstart_cero_based\tend\tlength\tNAs\tinformative_sites\tfeature\tstrandness\tframe\tid_gene\tid\twatterson_ave\twatterson_sd\tpairwise_ave\tpairwise_sd\ttajimaD\tpop\tspecie\tepoch" > $DIVERSITY_PER_UNIT_FOLDER"$POP".per.unit.averages.tsv

for iteration in {1..10}
do

rm $DIVERSITY_PER_UNIT_FOLDER"$POP".per.unit.averages_iteration"$iteration".tsv

SCAFFOLD_START=$( echo '1 + 4170 * ('$iteration' - 1)' | bc)
SCAFFOLD_END=$( echo '4170 * '$iteration | bc )

for SCAFFOLD_NUMBER in $(seq -s " " -f %05g $SCAFFOLD_START $SCAFFOLD_END) # esta manera de hacer seq te va a sacar siempre 5 cifras.
do

SCAFFOLD=$(echo "lp23.s"$SCAFFOLD_NUMBER)

#echo "---------------------$SCAFFOLD---------------------"

#For each unit

cd "$SFS_FOLDER""$POP"_separated_by_scaffold

while read LOCATION METHOD FEATURE START_ONEBASED END POINT STRANDNESS FRAME IDRAW;

do

#echo "--------$SCAFFOLD:$FEATURE--------"

if [ "$METHOD" = "PipeR" ] 
then
ID_GENE=$(echo $IDRAW | awk -F "_" '{ split ($0, a, "ID="); split (a[2],b,"_"); print b[1]"_"b[2] }')
else
ID_GENE=$(echo $IDRAW | awk '{ split($0,a,"ID="); split (a[2],b,";"); split(b[1],c,"T"); print c[1] }') 
fi

if [ "$FEATURE" = "CDS" ]
then
ID=$(echo $IDRAW | awk -F "_" '{split ($0,a,"Target="); split(a[2],b,";"); print b[1]}' |  awk '{print $1"_"$2"_"$3}' )
else
ID=$(echo $IDRAW | awk '{ split($0,a,"ID="); split (a[2],b,";"); print b[1] }')
fi

START_CEROBASED=($(echo $START_ONEBASED-1 | bc))
LENGTH=($(echo $END-$START_CEROBASED | bc)) 
SPECIE=$(echo $POP | cut -d"_" -f 2)
EPOCH=$(echo $POP | cut -d"_" -f 1)


cat "$POP".transformedThetas_"$SCAFFOLD" | bedtools intersect -a stdin -b <(echo -e "$LOCATION\t$START_CEROBASED\t$END") > "$POP".iter.transformedThetas.iteration"$iteration".borrar
# He comprobado que en el archivo de Thetas hay por lo menos 4 posiciones que empieza en 0. por tanto asumo que este archivo es 0-based.


WATTERSON_AVERAGE_PER_UNIT=$(cut -f 4 "$POP".iter.transformedThetas.iteration"$iteration".borrar | awk -v OFS='\t' '{for(i=1;i<=NF;i++) {sum[i] += $i }} END {for (i=1;i<=NF;i++) {printf ("%.10e\n", sum[i]/NR )}}' | sed 's/[eE]+\{0,1\}/*10^/g')
WATTERSON_SD_PER_UNIT=$(cut -f 4 "$POP".iter.transformedThetas.iteration"$iteration".borrar | awk -v OFS='\t' '{for(i=1;i<=NF;i++) {sum[i] += $i; sumsq[i] += ($i)^2}} END {for (i=1;i<=NF;i++) {printf ("%.10e\n", sqrt((sumsq[i]-sum[i]^2/NR)/NR))}}' | sed 's/[eE]+\{0,1\}/*10^/g')

PAIRWISE_AVERAGE_PER_UNIT=$(cut -f 5 "$POP".iter.transformedThetas.iteration"$iteration".borrar | awk -v OFS='\t' '{for(i=1;i<=NF;i++) {sum[i] += $i }} END {for (i=1;i<=NF;i++) {printf ("%.10e\n", sum[i]/NR )}}' | sed 's/[eE]+\{0,1\}/*10^/g') 
PAIRWISE_SD_PER_UNIT=$(cut -f 5  "$POP".iter.transformedThetas.iteration"$iteration".borrar | awk -v OFS='\t' '{for(i=1;i<=NF;i++) {sum[i] += $i; sumsq[i] += ($i)^2}} END {for (i=1;i<=NF;i++) {printf ("%.10e\n", sqrt((sumsq[i]-sum[i]^2/NR)/NR))}}' | sed 's/[eE]+\{0,1\}/*10^/g') 

DIFFERENCE_PAIRWISE_WATTERSON_SD_PER_UNIT=$(cut -f 6  "$POP".iter.transformedThetas.iteration"$iteration".borrar   |  awk -v OFS='\t' '{for(i=1;i<=NF;i++) {sum[i] += $i; sumsq[i] += ($i)^2}} END {for (i=1;i<=NF;i++) {printf ("%.10e\n", sqrt((sumsq[i]-sum[i]^2/NR)/NR ))}}' | sed 's/[eE]+\{0,1\}/*10^/g') 

TAJIMAS_D_PER_UNIT=$(echo "(($PAIRWISE_AVERAGE_PER_UNIT) - ($WATTERSON_AVERAGE_PER_UNIT))/($DIFFERENCE_PAIRWISE_WATTERSON_SD_PER_UNIT)" | bc -l | awk '{printf ("%.10e\n",$1)}' |  sed 's/[eE]+\{0,1\}/*10^/g' )

INFORMATIVESITES=$(wc -l "$POP".iter.transformedThetas.iteration"$iteration".borrar | cut -d" " -f1)

NAs=($(echo $LENGTH - $INFORMATIVESITES | bc)) 

#Before printing check all variables are full, if empty use NA
if [ -z ${LOCATION} ]; then  LOCATION=NA;  fi
if [ -z ${START_CEROBASED} ]; then  START_CEROBASED=NA;  fi
if [ -z ${END} ]; then  END=NA;  fi
if [ -z ${LENGTH} ]; then  LENGTH=NA;  fi
if [ -z ${NAs} ]; then  NAs=NA;  fi
if [ -z ${INFORMATIVESITES} ]; then  INFORMATIVESITES=NA;  fi
if [ -z ${FEATURE} ]; then  FEATURE=NA;  fi
if [ -z ${STRANDNESS} ]; then  STRANDNESS=NA;  fi
if [ -z ${FRAME} ]; then  FRAME=NA;  fi
if [ -z ${ID_GENE} ]; then  ID_GENE=NA;  fi
if [ -z ${ID} ]; then  ID=NA;  fi
if [ -z ${WATTERSON_AVERAGE_PER_UNIT} ]; then  WATTERSON_AVERAGE_PER_UNIT=NA;  fi
if [ -z ${WATTERSON_SD_PER_UNIT} ]; then  WATTERSON_SD_PER_UNIT=NA;  fi
if [ -z ${PAIRWISE_AVERAGE_PER_UNIT} ]; then  PAIRWISE_AVERAGE_PER_UNIT=NA;  fi
if [ -z ${PAIRWISE_SD_PER_UNIT} ]; then  PAIRWISE_SD_PER_UNIT=NA;  fi
if [ -z ${TAJIMAS_D_PER_UNIT} ]; then  TAJIMAS_D_PER_UNIT=NA;  fi
if [ -z ${POP} ]; then  POP=NA;  fi
if [ -z ${SPECIE} ]; then  SPECIE=NA;  fi
if [ -z ${EPOCH} ]; then  EPOCH=NA;  fi


#Paste averages, and standatd deviations

paste \
<(echo $LOCATION ) \
<(echo $START_CEROBASED ) \
<(echo $END ) \
<(echo $LENGTH ) \
<(echo $NAs) \
<(echo $INFORMATIVESITES) \
<(echo $FEATURE ) \
<(echo $STRANDNESS ) \
<(echo $FRAME ) \
<(echo $ID_GENE ) \
<(echo $ID) \
<(echo $WATTERSON_AVERAGE_PER_UNIT) \
<(echo $WATTERSON_SD_PER_UNIT) \
<(echo $PAIRWISE_AVERAGE_PER_UNIT) \
<(echo $PAIRWISE_SD_PER_UNIT) \
<(echo $TAJIMAS_D_PER_UNIT) \
<(echo $POP) \
<(echo $SPECIE) \
<(echo $EPOCH) |\
sed 's/ /\t/g'| sed 's/\t\+/\t/g'  >>  $DIVERSITY_PER_UNIT_FOLDER"$POP".per.unit.averages_iteration"$iteration".tsv

#Reset all (but POP,SPECIE or EPOCH (those should be the same during the whole iteration and if you remove POP the loop won't work)) variables before next iteration

unset LOCATION
unset START_CEROBASED
unset END
unset LENGTH
unset NAs
unset INFORMATIVESITES
unset FEATURE
unset STRANDNESS
unset FRAME
unset ID_GENE
unset ID
unset WATTERSON_AVERAGE_PER_UNIT
unset WATTERSON_SD_PER_UNIT
unset PAIRWISE_AVERAGE_PER_UNIT
unset PAIRWISE_SD_PER_UNIT
unset TAJIMAS_D_PER_UNIT

done < <(cat "$SCAFFOLDS_FOLDER"LYPA23C.CDS.GENE_promoters.GENE_introns.UTRs.ncRNA.lncRNA.lncRNA_introns.lncRNA_promoters.UCNE.intergenic.nr.gff3_"$SCAFFOLD" ) 

done &

done

# rm "$POP".iter.transformedThetas*borrar

```


Concateno con la tabla que tiene el header

c_ll_ki_n013
c_ll_no_n008
c_ll_po_n008
c_lp_do_n012
c_lp_sm_n019


```{bash}

mkdir iterations_to_rm_when_everything_is_correct
mv *_iteration*.tsv  iterations_to_rm_when_everything_is_correct 
cd iterations_to_rm_when_everything_is_correct

POPS=($(ls *tsv | cut -d"." -f 1 | sort | uniq))
rm *raw.tsv
for POP in "${POPS[@]}"
do
echo $POP
cat "$POP".per.unit.averages_iteration{1..10}.tsv >> "$POP".per.unit.averages_raw.tsv
done

```

## Sanity check

```{bash}
POPS=($(ls *tsv | cut -d"." -f 1 | sort | uniq))
for POP in "${POPS[@]}"
do
echo $POP
wc -l $POP*_iteration*tsv >> "${POP}".wcperiteration.list
wc -l $POP.per.unit.averages_raw.tsv >>  "${POP}".wctotal.list
done

```

Esto me está creando una tabla que tiene una entrada por cada una de las unidades de la notación aunque eso no significa que tengan ningún resultado. No tiene sentido arrastrar toda esta información todo el rato, así que la filtro.

```{bash}
cd iterations_to_rm_when_everything_is_correct

for FILE_DIVERSITY_EACH_ITERATION in *per.unit.averages_raw.tsv
do
echo $FILE_DIVERSITY_EACH_ITERATION
awk '($12!="NA") || ($13!="NA") || ($14!="NA") || ($15!="NA") || ($16!="NA")' $FILE_DIVERSITY_EACH_ITERATION | sort -k1,1 > ${FILE_DIVERSITY_EACH_ITERATION/_raw.tsv/.tsv}
done

# Sanity
wc -l *_filtered.tsv >>  wcfiltered.list

# Perfecto! Mas o menos todos son iguales.

mv *.per.unit.averages.tsv ..
rm *raw.tsv
rm *list



```


# ---------------------------------------------

# Anotación de las unidades: cromosoma y regiones; tel y centr. 

Ahora vamos a anotar esas unidades según pertenezcan o no a distintos cromosomas, y según sean teloméricas, centromérica o no. 


#### Cromosomas 

Para ello hemos usado el archivo que tienen en común las coordenadas de gato y de lince

```{bash}
head /GRUPOS/grupolince/copia_fabascal/MAPPINGS/lynx2cat_wTiger.sorted.bed
```
  
Creo una carpeta donde voy a guardar los archivos:
```{bash}
mkdir /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/chromosome_annotation
```

Los archivos de los que parto estén en:
```{bash}
/home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/diversity_per_unit/
```

Empiezo a unirlos. 

El primer Bed te devolvería un archivo con una línea por cada posición de sintenia tal que así:

lp23.s00001	0	15011	15011	11950	3061	intergenic	-	.	intergenic_region_1	intergenic_region_1	2.4096547746*10^-06	1.3458559263*10^-05	1.1416658651*10^-06	6.2801913098*10^-06	-1.7606196687*10^-01	c_ll_no_n008	ll	c	lp23.s00001	4071	4072	TCL=TTT:chrA3:15104518-15104519:NO	1

Como no queremos pasar por ahí ni guardar tanto, vamos a intentar usar awk directamente para quedarnos con lo que nos interese. 

```{bash}

screen -S chromosome_annotation


cd /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/chromosome_annotation
echo -e "scaffold\tstart_cero_based\tend\tlength\tNAs\tinformative_sites\tfeature\tstrandness\tframe\tid_gene\tid\twatterson_ave\twatterson_sd\tpairwise_ave\tpairwise_sd\ttajimaD\tpop\tspecie\tepoch\tchr" > header.chr.annotation

cd /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/diversity_per_unit/
for FILE in *.per.unit.averages.tsv
do
echo $FILE
cd /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/chromosome_annotation
intersectBed -sorted -wo -a <(tail -n +2 /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/diversity_per_unit/$FILE) -b /GRUPOS/grupolince/copia_fabascal/MAPPINGS/lynx2cat_wTiger.sorted.bed | awk -v OFS='\t' '{split ($23, a, ":"); split(a[3],b, "-"); print $1,$2,$3,$4,$5,$6,$7,$8,$9,$10,$11,$12,$13,$14,$15,$16,$17,$18,$19,a[2] }' | awk '!a[$0]++' | cat header.chr.annotation - > ${FILE/.per.unit.averages.tsv/.per.unit.averages.chr.tsv}
done

```


##### OLD Sanity checks

```{bash}

cd /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/chromosome_annotation

# Calculo qué unidades aparecen más de una vez:

awk '{print $7"_"$11}' c_lp_do_n012.per.unit.averages.chr.tsv | sort | uniq -c | grep -v "1 " | wc -l
# Hay 1477 

# ¿Qué hay en esta lista?

 awk '{print $7"_"$11}' c_lp_do_n012.per.unit.averages.chr.tsv | sort | uniq -c | grep -v "1 " | sed -e 's/^[ \t]*//' | cut -d' ' -f2  | cut -d"_" -f 1 | sort  | uniq -c

      9 3UTR
     14 5UTR
     12 CDS
   1024 intergenic
    406 intron
      2 lncRNA
      1 ncRNA
      9 promoter

# Esta lista incluye varias features distintas.

# Dentro de estas features repetidas tengo otras casuisticas:

# Ejemplo: Un intron repetido tres veces:

# grep LYPA23C000420T1 c_lp_do_n012.per.unit.averages.chr.tsv | grep intron_62

# lp23.s05342	577057	603192	26135	15052	11083	intron	+	0	LYPA23C000420	LYPA23C000420T1_intron_62	1.9406089032*10^-04	7.1920669656*10^-03	2.7265147013*10^-04	1.0183730201*10^-02	2.5254946264*10^-02	c_lp_do_n012	lp	cchrB2
# lp23.s05342	577057	603192	26135	15052	11083	intron	+	0	LYPA23C000420	LYPA23C000420T1_intron_62	1.9406089032*10^-04	7.1920669656*10^-03	2.7265147013*10^-04	1.0183730201*10^-02	2.5254946264*10^-02	c_lp_do_n012	lp	cchrX
# lp23.s05342	577057	603192	26135	15052	11083	intron	+	0	LYPA23C000420	LYPA23C000420T1_intron_62	1.9406089032*10^-04	7.1920669656*10^-03	2.7265147013*10^-04	1.0183730201*10^-02	2.5254946264*10^-02	c_lp_do_n012	lp	cchrB1

# Este intrón aparece asignado a tres cromosomas distintos. 

# Compruebo que efectivamente se asigna a tres cromosomas en el archivo de notación

grep lp23.s05342 /GRUPOS/grupolince/copia_fabascal/MAPPINGS/lynx2cat_wTiger.sorted.bed  | grep -v chrB2

# Efectivamente hay más de un cromosoma en ese scaffold. 

# Vamos a ver que número de features presentan este problema. 

# Creo una lista PARA DOÑANA de aquellas features que son identicas también en las posiciones (lo que excluiría a los casos de las UCR que hemos hablado antes). Para ello imprimo también la posición de comienzo y la de fin.

awk -v OFS='\t' '{print $1,$2,$3,$7,$11}' c_lp_do_n012.per.unit.averages.chr.tsv | sort | uniq -c | grep -v "1 " | sort | sed -e 's/^[ \t]*//'  > list_of_repeated_features_do

# Compruebo a qué unidades pertenecen. 

 cut -d' ' -f2 list_of_repeated_features_do | awk '{print $4}' | sort  | uniq -c

      9 3UTR
     14 5UTR
     12 CDS
   1024 intergenic
    382 intron
     24 intron_lncRNA
      2 lncRNA
      1 ncRNA
      1 promoter_gene_1000
      3 promoter_lncRNA_1000
      2 promoter_lncRNA_250
      3 promoter_lncRNA_500
      
      
# Vemos que todos los numeros coinciden con las unidades repetidas que habían salido antes (promotor e intrón son la suma). Por tanto, parece que todas estas regiones repetidas están asignadas a mas de un cromosoma. Ahora vamos a ver cuantas bases son eso.

# Compruebo que en las otras poblaciones son las mismas.

cd /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/chromosome_annotation

awk -v OFS='\t' '{print $1,$2,$3,$7,$11}' c_ll_ki_n013.per.unit.averages.chr.tsv | sort | uniq -c | grep -v "1 " | sort | sed -e 's/^[ \t]*//' > list_of_repeated_features_ki

awk -v OFS='\t' '{print $1,$2,$3,$7,$11}' c_ll_no_n008.per.unit.averages.chr.tsv | sort | uniq -c | grep -v "1 " | sort | sed -e 's/^[ \t]*//' > list_of_repeated_features_no

awk -v OFS='\t' '{print $1,$2,$3,$7,$11}' c_ll_po_n008.per.unit.averages.chr.tsv | sort | uniq -c | grep -v "1 " | sort | sed -e 's/^[ \t]*//' > list_of_repeated_features_po

awk -v OFS='\t' '{print $1,$2,$3,$7,$11}' c_lp_sm_n019.per.unit.averages.chr.tsv | sort | uniq -c | grep -v "1 " | sort | sed -e 's/^[ \t]*//' > list_of_repeated_features_sm

awk -v OFS='\t' '{print $1,$2,$3,$7,$11}' c_lp_sm_n012.per.unit.averages.chr.tsv | sort | uniq -c | grep -v "1 " | sort | sed -e 's/^[ \t]*//' > list_of_repeated_features_sm_n012

awk -v OFS='\t' '{print $1,$2,$3,$7,$11}' c_ll_ki_n008.per.unit.averages.chr.tsv | sort | uniq -c | grep -v "1 " | sort | sed -e 's/^[ \t]*//' > list_of_repeated_features_ki_n008




cut -d' ' -f2 list_of_repeated_features_po | awk '{print $4}' | sort  | uniq -c

     9 3UTR
     14 5UTR
     12 CDS
   1026 intergenic
    383 intron
     30 intron_lncRNA
      2 lncRNA
      1 ncRNA
      1 promoter_gene_1000
      3 promoter_lncRNA_1000
      2 promoter_lncRNA_250
      3 promoter_lncRNA_500

cut -d' ' -f2 list_of_repeated_features_ki | awk '{print $4}' | sort  | uniq -c

      9 3UTR
     14 5UTR
     12 CDS
   1023 intergenic
    382 intron
     30 intron_lncRNA
      2 lncRNA
      1 ncRNA
      1 promoter_gene_1000
      3 promoter_lncRNA_1000
      2 promoter_lncRNA_250
      3 promoter_lncRNA_500
      
cut -d' ' -f2 list_of_repeated_features_no | awk '{print $4}' | sort  | uniq -c

    9 3UTR
     14 5UTR
     12 CDS
   1025 intergenic
    384 intron
     30 intron_lncRNA
      2 lncRNA
      1 ncRNA
      1 promoter_gene_1000
      3 promoter_lncRNA_1000
      2 promoter_lncRNA_250
      3 promoter_lncRNA_500
      
      
cut -d' ' -f2 list_of_repeated_features_sm | awk '{print $4}' | sort  | uniq -c

      9 3UTR
     14 5UTR
     12 CDS
   1025 intergenic
    382 intron
     24 intron_lncRNA
      2 lncRNA
      1 ncRNA
      1 promoter_gene_1000
      3 promoter_lncRNA_1000
      2 promoter_lncRNA_250
      3 promoter_lncRNA_500
      

# Los números no coindicen, ¿en qúe se diferencian?


diff list_of_repeated_features_do list_of_repeated_features_po
diff list_of_repeated_features_do list_of_repeated_features_ki
diff list_of_repeated_features_do list_of_repeated_features_no
diff list_of_repeated_features_do list_of_repeated_features_sm


# No son iguales, aunque las diferencias son muy pequeñas.
# He comprobado que al menos en un caso, la feature que se reporta para Norway y no para Doñana no está disponible proque esas posiciones de diversidad no están.

# Ejemplo, intergenic_region_29405 no aparece en Doñana pero sí en Sierra Morena. 

# Lo busco.


grep  intergenic_region_29405  list_of_repeated_features_sm
# 2 lp23.s16211   3567    16982   intergenic      intergenic_region_29405

      
grep lp23.s16211 c_lp_do_n012.transformedThetas_filtered  
# Nada!



# La opción sería quitar todas las unidades que aparecen repetidas al menos en una población, visto las unidades que perdemos, el número de bases no sería muy alto. Anteriormente con aproximadamente las mismas unidades (algunos intergénicos más porque ahora hemos filtrado mejor) teníamos que:


# ¿Cuantas bases perdemos?

mkdir features_repeted
cd features_repeted/

# Primero calculamos cuantas bases totales tenemos:

# Este archivo que genero tiene todas las unidades, no solo las repetidas.

awk -v OFS='\t' '{print $1,$2,$3,$7,$11}' ../c_lp_sm_n019.per.unit.averages.chr.tsv |  awk '{print >> $4; close($4)}'
rm feature 

# ¿Cuantas bases por cada unidad?

for file in * 
do
echo $file "$(awk '{sum+=$3-$2}END{print sum}' $file )"
done >> bases_total.txt

mv bases_total.txt ../

# Ahora elimino estos archivos y hago la misma cuenta para aquellas que están repetidas.

# Ten cuidado de estar en la carpeta que debes!!!
cd features_repeted/
# rm *

# ¿Cuantas repetidas?

awk -v OFS='\t' '{print $1,$2,$3,$7,$11}' ../c_lp_sm_n019.per.unit.averages.chr.tsv | sort | uniq -c | grep -v "1 " | sort | sed -e 's/^[ \t]*//' | cut -d' ' -f2-6 | awk '{print >> $4; close($4)}'

rm feature 

# ¿Cuantas bases por cada unidad?

for file in * 
do
echo $file "$(awk '{sum+=$3-$2}END{print sum}' $file )"
done >> bases_repeted.txt

mv bases_repeted.txt ../

# rm *

# ¿Qué porcentaje supone del total?

cd ..

join -1 1 -2 1 bases_total.txt bases_repeted.txt | awk -v OFS='\t' '{print $1,$2,$3,$3/$2*100}' | cat <(echo -e "feature\ttotal_bases\tbases_assinged_to_2_or_more_chr\tpercentage") - > percentage_of_bases_assigned_to_2_chr_or_more_sm.txt

rm bases_repeted.txt
rm bases_total.txt 
rm -r features_repeted/
rm list_of_repeated_features_po

cat percentage_of_bases_assigned_to_2_chr_or_more_sm.txt

<!-- feature total_bases     bases_assinged_to_2_or_more_chr percentage -->
<!-- 3UTR    10888821        31596   0.290169 -->
<!-- 5UTR    39173937        747134  1.90722 -->
<!-- CDS     29521748        12656   0.0428701 -->
<!-- intergenic      1575985801      108644867       6.89377 -->
<!-- intron  781408454       12974162        1.66036 -->
<!-- intron_lncRNA   36490600        775481  2.12515 -->
<!-- lncRNA  3932428 1503    0.0382207 -->
<!-- ncRNA   319359  37      0.0115857 -->
<!-- promoter_gene_1000      6885450 1001    0.0145379 -->
<!-- promoter_lncRNA_1000    2905061 3003    0.103371 -->
<!-- promoter_lncRNA_250     660912  502     0.0759556 -->
<!-- promoter_lncRNA_500     1391917 1503    0.107981 -->


# Esto es un ejemplo,pero todas las poblaciones son más o menos igual, así que no se perderían tantas bases. 

```

Creo la lista de unidades que tengo que filtrar.

Sanity checks

```{bash}
cat list_of_repeated_features* | sort -k5,5 | uniq | wc -l  
1488
wc -l list_of_repeated_features_ki 
1482 list_of_repeated_features_ki
wc -l list_of_repeated_features_sm
1478 list_of_repeated_features_sm
wc -l list_of_repeated_features_do
1477 list_of_repeated_features_do
wc -l list_of_repeated_features_po
1486 list_of_repeated_features_po
wc -l list_of_repeated_features_no
1486 list_of_repeated_features_no
wc -l list_of_repeated_features_sm_n012
1464 list_of_repeated_features_sm_n012
wc -l  list_of_repeated_features_sm_n012
1464 list_of_repeated_features_sm_n012

```

Creo la lista

```{bash}
cat list_of_repeated_features* | sort -k5,5 | uniq > list_of_repeated_features_all_pops
```

Sanity checks

```{bash}

awk -v OFS='\t' '{print $2"_"$3"_"$4"_"$5"_"$6}' list_of_repeated_features_all_pops > list_of_repeated_features_all_pops_merged

wc -l *per.unit.averages.chr.tsv 
#   430830 c_ll_ki_n013.per.unit.averages.chr.tsv
#   430997 c_ll_no_n008.per.unit.averages.chr.tsv
#   430662 c_ll_po_n008.per.unit.averages.chr.tsv
#   429972 c_lp_do_n012.per.unit.averages.chr.tsv
#   430625 c_lp_sm_n019.per.unit.averages.chr.tsv --> 428377

awk -v OFS='\t' '{print $1"_"$2"_"$3"_"$7"_"$11, $0}' c_lp_sm_n019.per.unit.averages.chr.tsv | grep -f list_of_repeated_features_all_pops_merged - | awk '{print $1}' | sort | uniq -c | sort | sed -e 's/^[ \t]*//' | awk '{print $1"_"$2}' > lista_de_unidades_repetidas_que_encuentra

head lista_de_unidades_repetidas_que_encuentra

wc -l lista_de_unidades_repetidas_que_encuentra
1478 lista_de_unidades_repetidas_que_encuentra

awk -v OFS='\t' '{print $1"_"$2"_"$3"_"$4"_"$5"_"$6}' list_of_repeated_features_sm > lista_de_unidades_repetidas_que_hay

head lista_de_unidades_repetidas_que_hay

 wc -l lista_de_unidades_repetidas_que_hay
1478 lista_de_unidades_repetidas_que_hay

grep -f -v lista_de_unidades_repetidas_que_encuentra lista_de_unidades_repetidas_que_hay
# Nada! genial!

awk -v OFS='\t' '{print $1"_"$2"_"$3"_"$7"_"$11, $0}' c_lp_sm_n019.per.unit.averages.chr.tsv | 
grep -f list_of_repeated_features_all_pops_merged - | wc -l 
# 3102

awk '{sum+=$1}END{print sum}' list_of_repeated_features_sm
# 3102
# Perfecto!

```

Modifico mis archivos para eliminar la información de cromosoma cuando la unidad esté en dos o más cromosomas. 


```{bash}

cd /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/chromosome_annotation

# Como lo hice antes. YA NO porque esto elimina las filas repetidas, no elimina exclusivamente la info de cromosoma que es lo que nos interesa. 
<!-- for FILE in *per.unit.averages.chr.tsv  -->
<!-- do -->
<!-- echo $FILE -->
<!-- awk -v OFS='\t' '{print $1"_"$2"_"$3"_"$7"_"$11, $0}' $FILE |  -->
<!-- grep -v -f list_of_repeated_features_all_pops_merged - | cut -f2- > ${FILE/.per.unit.averages.chr.tsv/.per.unit.averages.chr_filtered.tsv} -->
<!-- done -->


###

for FILE in *per.unit.averages.chr.tsv 

do

echo $FILE

awk -v OFS='\t' '{print $1"_"$2"_"$3"_"$7"_"$11, $0}' $FILE | 
grep -f list_of_repeated_features_all_pops_merged - | awk -v OFS='\t' '{$21="na"; print}' > ${FILE/.per.unit.averages.chr.tsv/.per.unit.averages.chr_repeated_feature.tsv.borrar}

awk -v OFS='\t' '{print $1"_"$2"_"$3"_"$7"_"$11, $0}' $FILE | 
grep -v -f list_of_repeated_features_all_pops_merged - > ${FILE/.per.unit.averages.chr.tsv/.per.unit.averages.chr_NOT_repeated_feature.tsv.borrar}

cat <(tail -n +2 ${FILE/.per.unit.averages.chr.tsv/.per.unit.averages.chr_NOT_repeated_feature.tsv.borrar}) <( cat ${FILE/.per.unit.averages.chr.tsv/.per.unit.averages.chr_repeated_feature.tsv.borrar}) | cut -f2-  | sort -k1,1 -k2,2n -k3,3n | cat header.chr.annotation - | awk '!a[$0]++' > ${FILE/.per.unit.averages.chr.tsv/.per.unit.averages.chr_filtered.tsv}


done


# Sanity check!

# grep 3UTR c_lp_sm_n019.per.unit.averages.chr.tsv | grep LYPA23C004261 

# lp23.s10426	1414779	1415453	674	122	552	3UTR	-	.	LYPA23C004261	LYPA23C004261	5.2285065677*10^-07	3.1562104437*10^-06	1.2810182872*10^-07	7.8744833016*10^-07	-1.6655608018*10^-01	c_lp_sm_n019	lp	c	chrA2
# lp23.s10426	1414779	1415453	674	122	552	3UTR	-	.	LYPA23C004261	LYPA23C004261	5.2285065677*10^-07	3.1562104437*10^-06	1.2810182872*10^-07	7.8744833016*10^-07	-1.6655608018*10^-01	c_lp_sm_n019	lp	c	chrX

# grep 3UTR c_lp_sm_n019.per.unit.averages.chr_filtered.tsv | grep LYPA23C004261 

# lp23.s10426	1414779	1415453	674	122	552	3UTR	-	.	LYPA23C004261	LYPA23C004261	5.2285065677*10^-07	3.1562104437*10^-06	1.2810182872*10^-07	7.8744833016*10^-07	-1.6655608018*10^-01	c_lp_sm_n019	lp	c	na
# lp23.s10426	1414779	1415453	674	122	552	3UTR	-	.	LYPA23C004261	LYPA23C004261	5.2285065677*10^-07	3.1562104437*10^-06	1.2810182872*10^-07	7.8744833016*10^-07	-1.6655608018*10^-01	c_lp_sm_n019	lp	c	na


# Perfecto!

```


#### Telomeros2m

El archivo de interés, es: 
```{bash}
/GRUPOS/grupolince/telomers_centromers_definition/tel2m_regions_based_on_synteny_1000bp.bed
```

Creo una carpeta donde voy a guardar los archivos:
```{bash}
mkdir /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/region_annotation
```

Los archivos de los que parto estén en:
```{bash}
/home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/diversity_per_unit/
```

Empiezo a unirlos. 

```{bash}
cd /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/diversity_per_unit/

files_per_pop=($(ls *.per.unit.averages.tsv))

cd /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/region_annotation

echo -e "scaffold\tstart_cero_based\tend\tlength\tNAs\tinformative_sites\tfeature\tstrandness\tframe\tid_gene\tid\twatterson_ave\twatterson_sd\tpairwise_ave\tpairwise_sd\ttajimaD\tpop\tspecie\tepoch\ttel2m_bases\ttel2m_percentage" > header.region.tel2m.tsv

for file_pop in ${files_per_pop[@]}
do
echo $file_pop
intersectBed -sorted -wo -a <(tail -n +2 /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/diversity_per_unit/$file_pop) -b <(awk -v OFS='\t' '{print $1,$2, $3}' /GRUPOS/grupolince/telomers_centromers_definition/tel2m_regions_based_on_synteny_1000bp.bed) | awk -v OFS='\t' '{print $1,$2,$3,$4,$5,$6,$7,$8,$9,$10,$11,$12,$13,$14,$15,$16,$17,$18,$19,$23,$24 }' | awk '!a[$0]++' | bedtools groupby -i stdin -g 1-19 -c 20 -o sum | awk -v OFS='\t'  '{percentage=$20/$4; printf "%s\t%f\n",$0,percentage}' | cat header.region.tel2m.tsv - > ${file_pop/.per.unit.averages.tsv/.per.unit.averages.region.tel2m.tsv}
done

```

Esto me devuelve algo como así:
lp23.s00006	14749	29606	14857	8272	6585	intergenic	-	.	intergenic_region_445	intergenic_region_445	6.8371515163*10^-04	1.2551611729*10^-02	1.1155938889*10^-03	2.1427002344*10^-02	4.4628203002*10^-02	c_lp_sm_n019	lp	c	ChrB2 485

donde el último número representa las bases que solapan. 

#### Telomeros10m

El archivo de interés, es: 
```{bash}
/GRUPOS/grupolince/telomers_centromers_definition/tel0-10m_regions_based_on_synteny_1000bp.bed
```

Voy a guardar los archivos en la carpeta anteriormente creada:
```{bash}
/home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/region_annotation
```

Los archivos de los que parto estén en:
```{bash}
/home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/diversity_per_unit/
```

Empiezo a unirlos. 

```{bash}
cd /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/diversity_per_unit/

files_per_pop=($(ls *.per.unit.averages.tsv))

cd /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/region_annotation

echo -e "scaffold\tstart_cero_based\tend\tlength\tNAs\tinformative_sites\tfeature\tstrandness\tframe\tid_gene\tid\twatterson_ave\twatterson_sd\tpairwise_ave\tpairwise_sd\ttajimaD\tpop\tspecie\tepoch\ttel10m_bases\ttel10m_percentage" > header.region.tel10m.tsv

for file_pop in ${files_per_pop[@]}
do
echo $file_pop
intersectBed -sorted -wo -a <(tail -n +2 /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/diversity_per_unit/$file_pop) -b <(awk -v OFS='\t' '{print $1,$2, $3}' /GRUPOS/grupolince/telomers_centromers_definition/tel0-10m_regions_based_on_synteny_1000bp.bed) | awk -v OFS='\t' '{print $1,$2,$3,$4,$5,$6,$7,$8,$9,$10,$11,$12,$13,$14,$15,$16,$17,$18,$19,$23,$24 }' | awk '!a[$0]++' |  bedtools groupby -i stdin -g 1-19 -c 20 -o sum | awk -v OFS='\t'  '{percentage=$20/$4; printf "%s\t%f\n",$0,percentage}' | cat header.region.tel10m.tsv - > ${file_pop/.per.unit.averages.tsv/.per.unit.averages.region.tel10m.tsv}
done

```


Y por último los centrómeros:

#### Centrómeros

El archivo de interés, es: 
```{bash}
/GRUPOS/grupolince/telomers_centromers_definition/centr_regions_based_on_synteny_1000bp.bed
```

Voy a guardar los archivos en la carpeta anteriormente creada:
```{bash}
/home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/region_annotation
```

Los archivos de los que parto estén en:
```{bash}
/home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/diversity_per_unit/
```

Empiezo a unirlos. 

```{bash}
cd /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/diversity_per_unit/

files_per_pop=($(ls *.per.unit.averages.tsv))

cd /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/region_annotation

echo -e "scaffold\tstart_cero_based\tend\tlength\tNAs\tinformative_sites\tfeature\tstrandness\tframe\tid_gene\tid\twatterson_ave\twatterson_sd\tpairwise_ave\tpairwise_sd\ttajimaD\tpop\tspecie\tepoch\tcentr_bases\tcentr_percentage" > header.region.centr.tsv

for file_pop in ${files_per_pop[@]}
do
echo $file_pop
# En el awk que le aplico a la region clasificada como cent, lo que estoy haciendo es eliminar el cromsoma. Así aunque esté asignado a distintos cromosomas, yo sí m voy a creer que es región centromérica. 
intersectBed -sorted -wo -a <(tail -n +2 /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/diversity_per_unit/$file_pop) -b <(awk -v OFS='\t' '{print $1,$2, $3}' /GRUPOS/grupolince/telomers_centromers_definition/centr_regions_based_on_synteny_1000bp.bed) | awk -v OFS='\t' '{print $1,$2,$3,$4,$5,$6,$7,$8,$9,$10,$11,$12,$13,$14,$15,$16,$17,$18,$19,$23,$24 }' | awk '!a[$0]++' |  bedtools groupby -i stdin -g 1-19 -c 20 -o sum | awk -v OFS='\t'  '{percentage=$20/$4; printf "%s\t%f\n",$0,percentage}' | cat header.region.centr.tsv - > ${file_pop/.per.unit.averages.tsv/.per.unit.averages.region.centr.tsv}
done


# Con el bedtools groupby agrupo a regiones que estaban partidas. 
```

Ahora borro lo que no necesito

```{bash}
# rm header.region.*

```

Sanity checks

```{bash}
awk -v OFS='\t' '{print $1,$2,$3,$7,$11}' c_lp_sm_n019.per.unit.averages.region.centr_filtered.tsv | sort | uniq -c | grep -v "1 " | head

# Nada perfecto! Y además había comprobado que antes salían dos regiones independientes, pero ahora con el groupby se ha arreglado y se ha sumado bien las columnas. 


awk -v OFS='\t' '{print $1,$2,$3,$7,$11}' c_ll_ki_n013.per.unit.averages.region.centr_filtered.tsv | sort | uniq -c | grep -v "1 " | head

# Nada perfecto!

```


# ------------------------------------------

# Descargar tablas 

Primero me bajo las tablas.


```{bash}

scp mlucena@genomics-b.ebd.csic.es:/home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/diversity_per_unit/*tsv /Users/marialucenaperez/Documents/WG_lynx_diversity_per_unit/raw_tables/
scp mlucena@genomics-b.ebd.csic.es:/home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/region_annotation/*.tsv /Users/marialucenaperez/Documents/WG_lynx_diversity_per_unit/raw_tables/
scp mlucena@genomics-b.ebd.csic.es:/home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/chromosome_annotation/*.per.unit.averages.chr_filtered.tsv /Users/marialucenaperez/Documents/WG_lynx_diversity_per_unit/raw_tables/

```

# ------------------------------------------

# R analysis

Ahora tengo que descargarme todos los archivos de cada población y despues hacer un super-join para cada población para acabar haciendo una unica tabla con rbind con todas las poblaciones. 

## Creación tablas

```{r}

# Ahora hago un join de la tabla principal de diversidad con las de telómeros, centrómeros y cromosoma. 

library(dplyr)
library(ggplot2)
library(tidyr)

wd_in <- "/Users/marialucenaperez/Documents/WG_lynx_diversity_per_unit/raw_tables/"
wd_out <- "/Users/marialucenaperez/Documents/WG_lynx_diversity_per_unit/"


# Ojo! antes de correr esto, asegurarme que no lo he corrido antes porque si no estoy haciendo append a las tablas viejas y no creando nuevas. 

poplist <- c("c_ll_ki_n013", "c_ll_ki_n008", "c_ll_po_n008","c_ll_no_n008","c_lp_sm_n019", "c_lp_sm_n012", "c_lp_do_n012")

  
for (pop in poplist)
{
  if (exists("dataset"))
  {rm (dataset)
  }
  
  files_for_given_pop <- list.files(path = wd_in, pattern = pop)
  
  for (file_for_given_pop in files_for_given_pop)
  {
    # if the merged dataset doesn't exist, create it
    if (!exists("dataset"))
    {
      dataset <- read.table(paste(wd_in,file_for_given_pop, sep=""), header=TRUE, sep="\t") 
    }
    
    # if the merged dataset does exist, append to it
    if (exists("dataset"))
    {
      temp_dataset <-read.table(paste(wd_in,file_for_given_pop, sep=""), header=TRUE, sep="\t", na.strings = c("NA", "na"))
      dataset<-full_join(dataset, temp_dataset, by = c("scaffold", "start_cero_based", "end", "length", "NAs", "informative_sites", "feature", "strandness", "frame", "id_gene", "id", "watterson_ave", "watterson_sd", "pairwise_ave", "pairwise_sd", "tajimaD", "pop", "specie", "epoch"))   
      rm(temp_dataset)
      assign(pop, dataset) # Con esto quiero ponerle el nombre de la población a la dataframe. 
    }
  }    
}


rm(dataset)



data_diversity <-rbind(c_ll_ki_n013,c_ll_ki_n008,c_ll_po_n008,c_ll_no_n008,c_lp_sm_n019,c_lp_sm_n012,c_lp_do_n012) %>% 
    mutate( watterson_ave = as.numeric(gsub("\\*10\\^","e",watterson_ave)),
            watterson_sd = as.numeric(gsub("\\*10\\^","e",watterson_sd)),
            pairwise_ave  = as.numeric(gsub("\\*10\\^","e",pairwise_ave)),
            pairwise_sd  = as.numeric(gsub("\\*10\\^","e",pairwise_sd)),
            tajimaD = as.numeric(gsub("\\*10\\^","e",tajimaD)))


write.table (data_diversity, paste(wd_out, "global.per.unit.averages.chr.all.regions.tsv", sep=""), row.names = F, quote = F, sep = '\t')
write.table (c_ll_ki_n013, paste(wd_out, "c_ll_ki_n013.per.unit.averages.chr.all.regions.tsv", sep=""), row.names = F, quote = F, sep = '\t')
write.table (c_ll_ki_n008, paste(wd_out, "c_ll_ki_n008.per.unit.averages.chr.all.regions.tsv", sep=""), row.names = F, quote = F, sep = '\t')
write.table (c_ll_no_n008, paste(wd_out, "c_ll_no_n008.per.unit.averages.chr.all.regions.tsv", sep=""), row.names = F, quote = F, sep = '\t')
write.table (c_ll_po_n008, paste(wd_out, "c_ll_po_n008.per.unit.averages.chr.all.regions.tsv", sep=""), row.names = F, quote = F, sep = '\t')
write.table (c_lp_do_n012, paste(wd_out, "c_ll_do_n012.per.unit.averages.chr.all.regions.tsv", sep=""), row.names = F, quote = F, sep = '\t')
write.table (c_lp_sm_n019, paste(wd_out, "c_ll_sm_n019.per.unit.averages.chr.all.regions.tsv", sep=""), row.names = F, quote = F, sep = '\t')
write.table (c_lp_sm_n012, paste(wd_out, "c_ll_sm_n012.per.unit.averages.chr.all.regions.tsv", sep=""), row.names = F, quote = F, sep = '\t')


```


## Cargar tablas originales 

```{r}

library(dplyr)
library(ggplot2)
library(tidyr)


wd <- "/Users/marialucenaperez/Documents/WG_lynx_diversity_per_unit/"
wd_output <- "/Users/marialucenaperez/Owncloud/publico/WG_diversity/ANGSD/sfs/"
  

data_diversity <- read.table(paste(wd, "global.per.unit.averages.chr.all.regions.tsv", sep=""), header=T, na.strings = c("NA", "na")) %>%  
  replace(., is.na(.), "0") %>%  # Para que pueda calcular cuando es mayor o menor que 75%; si es NA no lo reconoce. 
  mutate (., Populations =  ifelse (pop == "c_ll_po_n008", "Bialowieza",
                            ifelse (pop == "c_ll_ki_n013", "Kirov", 
                            ifelse (pop == "c_ll_ki_n008", "Kirov_subsampled", 
                            ifelse (pop == "c_ll_no_n008", "Norway", 
                            ifelse (pop == "c_lp_sm_n019", "Andujar", 
                            ifelse (pop == "c_lp_sm_n012", "Andujar_subsampled", 
                            ifelse (pop == "c_lp_do_n012", "Donana", NA)))))))) %>% 
  mutate (., Species =ifelse (specie=="ll", "L.lynx", 
                   ifelse (specie=="lp", "L.pardinus", NA))) %>% 
  # Anoto qué es cada región 
  mutate (., region = ifelse (tel2m_percentage >= 0.75, "Telomere_2m", 
                       ifelse (centr_percentage >= 0.75, "Centromere", "Arm" )))  %>% 
  # Creo un identificador único para luego poder filtrar por este cuando tenga que sacar los comunes a todas las poblaciones. 
  mutate (., id_feature = paste(feature,"_",id, sep ="")) %>% 
  mutate (., chr = ifelse (chr.x == chr.y, as.character(chr.x), "PROBLEMA")) %>% select (-c(chr.x, chr.y)) %>%   mutate (unique_id = paste (id,feature, sep="_"))


unique(data_diversity$pop)

unique(data_diversity$Populations)

``` 


##### Sanity check

Chequeo por qué las submuestreadas tienen menos unidades que las otras.

```{r}

c_ll_ki_n0013 <- data_diversity %>% filter (pop=="c_ll_ki_n013")
c_ll_ki_n0008 <- data_diversity %>% filter (pop=="c_ll_ki_n008")

lost_c_ll_ki <- dplyr::anti_join(c_ll_ki_n0013, c_ll_ki_n0008, by = c("unique_id"))

# Con esta lista estoy sacando las que solo están en la no submuestreada. Me fijo en una de ellas al azar:

# LYPA23C013736 y la busco en el servidor en la tabla de diversidad:

```

```{bash}
grep LYPA23C013736 c_ll_ki_n008.per.unit.averages_iteration1.tsv

# lp23.s00001	42532	42623	91	9	82	CDS	+	0	LYPA23C013736	LYPA23C013736P1_1_31	8.4643631707*10^-06	4.2156730876*10^-05	3.9287378174*10^-06	1.9473962736*10^-05	-2.2590594071*10^-01	c_ll_ki_n008	ll	c

# lp23.s00001	42623	43535	912	43	869	intron	+	0	LYPA23C013736	LYPA23C013736T1_intron_1	3.5047507546*10^-04	1.0217133672*10^-02	2.8990944884*10^-04	8.4917807232*10^-03	-7.1323235666*10^-03	c_ll_ki_n008	ll	c

# lp23.s00001	43535	43669	134	0	134	CDS	+	2	LYPA23C013736	LYPA23C013736P1_31_75	4.2079181642*10^-06	1.5220192831*10^-06	1.9598973779*10^-06	7.7132835007*10^-07	NA	c_ll_ki_n008	ll	c

grep LYPA23C013736 ../c_ll_ki_n008.per.unit.averages.tsv

# lp23.s00001	42532	42623	91	9	82	CDS	+	0	LYPA23C013736	LYPA23C013736P1_1_31	8.4643631707*10^-06	4.2156730876*10^-05	3.9287378174*10^-06	1.9473962736*10^-05	-2.2590594071*10^-01	c_ll_ki_n008	ll	c

# lp23.s00001	42623	43535	912	43	869	intron	+	0	LYPA23C013736	LYPA23C013736T1_intron_1	3.5047507546*10^-04	1.0217133672*10^-02	2.8990944884*10^-04	8.4917807232*10^-03	-7.1323235666*10^-03	c_ll_ki_n008	ll	c



# OJO!!! Parece que sí esta en el archivo de iteraciones pero no en el global, por tanto algo ha debido de ir mal al pegarlos. Reviso esa parte de código. 
```






```{r}




# 2248581

# Compruebo que efectivamente no hay ningún cromosoma que no encaje. 
# unique(data_diversity$chr)
# Está bien!


# Sanity checks de que no hay duplicados:
# filter (data_diversity, pop=="c_ll_ki_n013") %>% nrow
# 450064
# filter (data_diversity, pop=="c_ll_ki_n013") %>% select(id_feature) %>% unique() %>% nrow
# 450064


data_diversity_pre <- data_diversity %>%  
  filter (., informative_sites>=50) %>% 
  filter (., informative_sites/length>=0.20) %>% 
  filter (., !(feature=="promoter_gene_250")) %>% 
  filter (., !(feature=="promoter_gene_500")) %>% 
  filter (., !(feature=="promoter_gene_1000" & length < 1000)) %>% 
  filter (., !(feature=="promoter_lncRNA_250")) %>% 
  filter (., !(feature=="promoter_lncRNA_500")) %>% 
  filter (., !(feature=="promoter_lncRNA_1000" & length < 1000)) 

# 2003073; 2248581-2003073=245508 

# Saco la lista de elementos comunes a todas las unidades para cada especie. 

# Primero par lynx lynx:

data_diversity_filtered_ki <- data_diversity_pre %>% filter (pop=="c_ll_ki_n013")
data_diversity_filtered_ki_filtered <- data_diversity_pre %>% filter (pop=="c_ll_ki_n008")
data_diversity_filtered_no <- data_diversity_pre %>% filter (pop=="c_ll_no_n008")
data_diversity_filtered_po <- data_diversity_pre %>% filter (pop=="c_ll_po_n008")

lista_lynxlynx <- inner_join(data_diversity_filtered_ki_filtered, data_diversity_filtered_no, by = c("scaffold", "start_cero_based", "end", "length", "feature", "strandness", "frame", "id_gene", "id", "region", "id_feature", "chr", "unique_id")) %>% inner_join(., data_diversity_filtered_po, by = c("scaffold", "start_cero_based", "end", "length", "feature", "strandness", "frame", "id_gene", "id", "region", "id_feature", "chr", "unique_id" ))  %>% inner_join(., data_diversity_filtered_ki, by = c("scaffold", "start_cero_based", "end", "length", "feature", "strandness", "frame", "id_gene", "id", "region", "id_feature", "chr", "unique_id" )) %>% select("unique_id")
 
lista_lynxlynx <- inner_join(data_diversity_filtered_ki, data_diversity_filtered_no, by = c("scaffold", "start_cero_based", "end", "length", "feature", "strandness", "frame", "id_gene", "id", "region", "id_feature", "chr", "unique_id")) %>% inner_join(., data_diversity_filtered_po, by = c("scaffold", "start_cero_based", "end", "length", "feature", "strandness", "frame", "id_gene", "id", "region", "id_feature", "chr", "unique_id" ))  %>% select("unique_id")



data_diversity_filtered_lynxlynx <- data_diversity_pre %>% filter (specie=="ll") %>% filter(unique_id %in% lista_lynxlynx$unique_id)

# Ahora para pardinus:

data_diversity_filtered_sm <- data_diversity_pre %>% filter (pop=="c_lp_sm_n019")
data_diversity_filtered_sm_filtered <- data_diversity_pre %>% filter (pop=="c_lp_sm_n012")
data_diversity_filtered_do <- data_diversity_pre %>% filter (pop=="c_lp_do_n012")

lista_lynxpardinus <- inner_join(data_diversity_filtered_sm_filtered, data_diversity_filtered_do, by = c("scaffold", "start_cero_based", "end", "length", "feature", "strandness", "frame", "id_gene", "id", "region", "id_feature", "chr", "unique_id")) %>% inner_join(., data_diversity_filtered_sm, by = c("scaffold", "start_cero_based", "end", "length", "feature", "strandness", "frame", "id_gene", "id", "region", "id_feature", "chr", "unique_id" )) %>% select("unique_id")

data_diversity_filtered_lynxpardinus <- data_diversity_pre %>% filter (specie=="lp") %>% filter(unique_id %in% lista_lynxpardinus$unique_id)

data_diversity_filtered <- rbind(data_diversity_filtered_lynxlynx, data_diversity_filtered_lynxpardinus)

# Con los filtrados hemos perdido 251.736 unidades. Aqui entran los promotores que se han caido. 

# Ordenar los factores. 

data_diversity_filtered$Populations <- factor (data_diversity_filtered$Populations, levels=c("Kirov","Kirov_subsampled","Bialowieza","Norway", "Andujar","Donana"))
data_diversity_filtered$chr <- factor(data_diversity_filtered$chr, levels = c("chrA1","chrC1","chrB1","chrA2","chrC2","chrB2","chrB3","chrB4","chrA3","chrD1","chrD3","chrD4","chrD2","chrF2","chrF1","chrE2","chrE1","chrX"))


# Borro las que no necesito luego
rm (data_diversity_filtered_sm)
rm (data_diversity_filtered_sm_filtered)
rm (data_diversity_filtered_do)
rm (data_diversity_filtered_ki)
rm (data_diversity_filtered_ki_filtered)
rm (data_diversity_filtered_po)
rm (data_diversity_filtered_no)
rm (data_diversity)
rm (data_diversity_filtered_lynxlynx)
rm (data_diversity_filtered_lynxpardinus)
rm (data_diversity_pre)


# 5/6/2018
# Hemos decidido filtrar el 20% de sitios informativos/length.
# También que tengan un mínimo de 50 sitios informativos. 
# Por último tamibén quitamos los promotores que tengan menos del tamaño esperado. 
## Aquí abajo están todas las pruebecillas que he ido haciendo para al final quedarnos con los filtro que explicamos arriba. 

# Voy a nombrar como telómero 10m, 2m o centr lo que tenga más de 75% de bases como tal. 
# Ojo que : Todo lo que tiene bases como tel2m está contenido en 10m, pero por supuesto no todo lo que está en 2m está en 10m . 
# 21/06/2018 --> Despues de pensarlo, y hacer gráficas que comparaban tel 0-10 con tel 0-2 me voy a quedar solo con tel2.  
#
# uu <- filter (data_diversity, data_diversity$tel10m_percentage!=data_diversity$tel2m_percentage)

# # Porcentaje de telomericas.
# ggplot (data = data_diversity, aes(tel10m_percentage)) +
#   geom_histogram() +
#    scale_y_continuous(trans = 'log10') 
# 
# 
# ggplot (data = data_diversity, aes(data_diversity$tel2m_percentage)) +
#   geom_histogram() +
#    scale_y_continuous(trans = 'log10') 
# 
# ggplot (data = data_diversity, aes(data_diversity$centr_percentage)) +
#   geom_histogram() +
#    scale_y_continuous(trans = 'log10') 
# 
# Para cada una de las unidades voy comprobando
# ggplot (data = filter (data_diversity,data_diversity$feature=="CDS" & data_diversity$length<2000), aes(informative_sites)) +
#   geom_histogram(bins=500) 
# ggplot (data = filter(data_diversity, data_diversity$feature=="CDS"), aes(x=informative_sites, y=length)) +
#   geom_point()
# ggplot (data = filter (data_diversity,data_diversity$feature=="intergenic"), aes(length)) +
#   geom_histogram(bins=1000)
# ggplot (data = filter (data_diversity,data_diversity$feature=="intron" & data_diversity$length<150000), aes(length)) +
#   geom_histogram(bins=1000) 
# ggplot (data = filter (data_diversity,data_diversity$feature=="5UTR"), aes(length)) +
#   geom_histogram(bins=1000) 
# ggplot (data = filter (data_diversity,data_diversity$feature=="3UTR"), aes(length)) +
#   geom_histogram(bins=1000) 
# ggplot (data = filter (data_diversity,data_diversity$feature=="lncRNA"), aes(length)) +
#   geom_histogram(bins=1000) 
# ggplot (data = filter (data_diversity,data_diversity$feature=="ncRNA "), aes(length)) +
#   geom_histogram(bins=1000) 
# ggplot (data = filter (data_diversity,data_diversity$feature=="UCNE"), aes(length)) +
#   geom_histogram(bins=1000) 



# Tamaño de las unidades

ggplot (data = data_diversity_filtered, aes(length)) +
  geom_histogram() +
  facet_wrap (~feature, scales = "free")
ggsave(paste (wd_output, "length_per_unit_all_pops.pdf", sep=""))


for (pop in unique(data_diversity_filtered$Populations))
  {
  
  print (pop)
  ggplot (data=dplyr::filter(data_diversity_filtered,data_diversity_filtered$Populations==!!pop), aes(length)) +
  geom_histogram() +
  facet_wrap (~feature, scales = "free") 
  ggsave(paste (wd_output, "length_per_unit_",pop,".pdf", sep="" ), device="pdf")

  }
 
```

##### Otros sanity checks

¿Cómo solapan lncRNA con ncRNA?

```{r}

lncRNA <- data_diversity %>% filter (feature=="lncRNA")
# 55885 observaciones

ggplot(lncRNA, aes(x=length))+
  geom_histogram(bins = 2000) +
  coord_cartesian(xlim=c(0, 3000))

sncRNA <- data_diversity %>% filter (feature=="ncRNA")
# 21043 observaciones

ggplot(sncRNA, aes(x=length))+
  geom_histogram(bins = 2000) +
  coord_cartesian(xlim=c(0, 3000))

ncRNA <- inner_join(lncRNA, sncRNA, by = c("scaffold", "start_cero_based", "end", "length")) %>%  select ("scaffold", "start_cero_based", "end","feature.x","id_gene.x", "id.x", "feature.y","id_gene.y", "id.y" ) %>% unique
# Son 14 los que coinciden. 

# He sacado graficas para tel2m y telomero de 0 a 10m y hemos decidido quedarnos con tel2m.
# Antes la clasificación incluía tel2m_tel10m de esta manera lo separamos en dos y replicamos las columnas de manera que una región que pertenece al 2m también aparece repetida para el 10m. 
data_diversity_filtered <-  data_diversity_filtered %>%  mutate (., region=replace(region, region =="no_centr_no_tel", "nocentrnotel")) %>% separate_rows (data_diversity_filtered, region, sep="_")


```

Algunas notas sobre los lncRNA y ncRNA
Infernal ("INFERence of RNA ALignment") is for searching DNA sequence databases for RNA structure and sequence similarities. It is an implementation of a special case of profile stochastic context-free grammars called covariance models (CMs). A CM is like a sequence profile, but it scores a combination of sequence consensus and RNA secondary structure consensus, so in many cases, it is more capable of identifying RNA homologs that conserve their secondary structure more than their primary sequence.



#### General stats


#### Empirical values per pop

```{r}

# Basic stats per unit per pop. 
library("Hmisc")

diversity_stats_pop <- function(diversity_df)
{
  diversity_stats_df <- 
             diversity_df %>% 
             dplyr::group_by(pop) %>%
             dplyr::summarise(
                       total_count=n(), 

                       mean_watterson_ave=mean(watterson_ave),
                       mean_pairwise_ave=mean(pairwise_ave),
                       mean_tajimaD=mean(tajimaD),
                       
                       median_watterson_ave=median(watterson_ave),
                       median_pairwise_ave=median(pairwise_ave),
                       median_tajimaD=median(tajimaD),
                       
                       wmean_watterson_ave=weighted.mean(watterson_ave,w=informative_sites),
                       wmean_pairwise_ave=mean(pairwise_ave,w=informative_sites),
                       wmean_tajimaD=mean(tajimaD,w =informative_sites),
  
                       # wmean_var_watterson_ave = wtd.var(watterson_ave,informative_sites),
                       # wmean_var_pairwise_ave = wtd.var(pairwise_ave,informative_sites),
                       # wmean_var_tajimaD = wtd.var(tajimaD,informative_sites),

                       # wmean_sd_watterson_ave =  sqrt(wmean_var_watterson_ave),
                       # wmean_sd_pairwise_ave =  sqrt(wmean_var_pairwise_ave),
                       # wmean_sd_tajimaD =  sqrt(wmean_var_tajimaD),

                       q25_watterson_ave=quantile(watterson_ave,0.25),
                       q25_pairwise_ave=quantile(pairwise_ave,0.25),
                       q25_tajimaD=quantile(tajimaD,0.25),
                       
                       q75_watterson_ave=quantile(watterson_ave,0.75),
                       q75_pairwise_ave=quantile(pairwise_ave,0.75),
                       q75_tajimaD=quantile(tajimaD,0.75),
                       
                       q05_watterson_ave=quantile(watterson_ave,0.05),
                       q05_pairwise_ave=quantile(pairwise_ave,0.05),
                       # q05_tajimaD=quantile(tajimaD,0.05),
                       
                       q95_watterson_ave=quantile(watterson_ave,0.95),
                       q95_pairwise_ave=quantile(pairwise_ave,0.95),
                       q95_tajimaD=quantile(tajimaD,0.95),
                       
                       IQR_watterson_ave = q75_watterson_ave - q25_watterson_ave,
                       IQR_pairwise_ave  = q75_pairwise_ave  - q25_pairwise_ave,
                       IQR_tajimaD       = q75_tajimaD       - q25_tajimaD ,
                       
                       lowerw_watterson_ave = q25_watterson_ave - 1.5 * IQR_watterson_ave,
                       lowerw_pairwise_ave  = q25_pairwise_ave  - 1.5 * IQR_pairwise_ave,
                       lowerw_tajimaD       = q25_tajimaD       - 1.5 * IQR_tajimaD,
                       
                       upperw_watterson_ave = q75_watterson_ave + 1.5 * IQR_watterson_ave,
                       upperw_pairwise_ave  = q75_pairwise_ave  + 1.5 * IQR_pairwise_ave) %>% 
  mutate (., Populations =  ifelse (pop == "c_ll_po_n008", "Bialowieza",
                            ifelse (pop == "c_ll_ki_n013", "Kirov", 
                            ifelse (pop == "c_ll_ki_n008", "Kirov_subsampled", 
                            ifelse (pop == "c_ll_no_n008", "Norway", 
                            ifelse (pop == "c_lp_sm_n019", "Andujar", 
                            ifelse (pop == "c_lp_sm_n012", "Andujar_subsampled", 
                            ifelse (pop == "c_lp_do_n012", "Donana", NA)))))))) %>% 
  mutate (., Species =ifelse (pop == "c_ll_po_n008", "L.lynx",
                            ifelse (pop == "c_ll_ki_n013", "L.lynx", 
                            ifelse (pop == "c_ll_ki_n008", "L.lynx",
                            ifelse (pop == "c_ll_no_n008", "L.lynx", 
                            ifelse (pop == "c_lp_sm_n019", "L.pardinus", 
                            ifelse (pop == "c_lp_sm_n019", "L.pardinus", 
                            ifelse (pop == "c_lp_do_n012", "L.pardinus", NA))))))))



diversity_stats_df$Populations <- factor (diversity_stats_df$Populations, levels=c("Kirov","Kirov_subsampled","Bialowieza","Norway", "Andujar", "Andujar_subsampled","Donana"))
  
return(diversity_stats_df)
}

stats_df_pop <- diversity_stats_pop(data_diversity_filtered)
write.table(stats_df_pop, paste(wd_output,"stats_diversity_per_pop.tsv",sep=""),quote = F,  row.names = F, sep =";", dec="," )

```

#### Empirical valuer per pop & different groups (feature, chr, region) 

```{r}

GROUPS=c("feature", "region", "chr")

diversity_stats_grouped <- function(DATAFRAME ,GROUP)
{ diversity_stats_df_grouped <- 
             DATAFRAME %>% 
             dplyr::group_by_("pop",GROUP) %>%
             dplyr::summarise(
                       total_count=n(), 

                       mean_watterson_ave=mean(watterson_ave),
                       mean_pairwise_ave=mean(pairwise_ave),
                       mean_tajimaD=mean(tajimaD),
                       
                       median_watterson_ave=median(watterson_ave),
                       median_pairwise_ave=median(pairwise_ave),
                       median_tajimaD=median(tajimaD),
                       
                       wmean_watterson_ave=weighted.mean(watterson_ave,w=informative_sites),
                       wmean_pairwise_ave=weighted.mean(pairwise_ave,w=informative_sites),
                       wmean_tajimaD=mean(tajimaD,w =informative_sites),
  
                       wmean_var_watterson_ave = wtd.var(watterson_ave,informative_sites),
                       wmean_var_pairwise_ave = wtd.var(pairwise_ave,informative_sites),
                       wmean_var_tajimaD = wtd.var(tajimaD,informative_sites),
  
                       
                       wmean_sd_watterson_ave =  sqrt(wmean_var_watterson_ave),
                       wmean_sd_pairwise_ave =  sqrt(wmean_var_pairwise_ave),
                       wmean_sd_tajimaD =  sqrt(wmean_var_tajimaD),

                       q25_watterson_ave=quantile(watterson_ave,0.25),
                       q25_pairwise_ave=quantile(pairwise_ave,0.25),
                       q25_tajimaD=quantile(tajimaD,0.25),
                       
                       q75_watterson_ave=quantile(watterson_ave,0.75),
                       q75_pairwise_ave=quantile(pairwise_ave,0.75),
                       q75_tajimaD=quantile(tajimaD,0.75),
                       
                       q05_watterson_ave=quantile(watterson_ave,0.05),
                       q05_pairwise_ave=quantile(pairwise_ave,0.05),
                       q05_tajimaD=quantile(tajimaD,0.05),
                       
                       q95_watterson_ave=quantile(watterson_ave,0.95),
                       q95_pairwise_ave=quantile(pairwise_ave,0.95),
                       q95_tajimaD=quantile(tajimaD,0.95),
                       
                       IQR_watterson_ave = q75_watterson_ave - q25_watterson_ave,
                       IQR_pairwise_ave  = q75_pairwise_ave  - q25_pairwise_ave,
                       IQR_tajimaD       = q75_tajimaD       - q25_tajimaD ,
                       
                       lowerw_watterson_ave = q25_watterson_ave - 1.5* IQR_watterson_ave,
                       lowerw_pairwise_ave  = q25_pairwise_ave  - 1.5* IQR_pairwise_ave,
                       lowerw_tajimaD       = q25_tajimaD       - 1.5* IQR_tajimaD,
                       
                       upperw_watterson_ave = q75_watterson_ave + 1.5* IQR_watterson_ave,
                       upperw_pairwise_ave  = q75_pairwise_ave  + 1.5* IQR_pairwise_ave) %>% 
  mutate (., Populations =  ifelse (pop == "c_ll_po_n008", "Bialowieza",
                            ifelse (pop == "c_ll_ki_n013", "Kirov", 
                            ifelse (pop == "c_ll_ki_n008", "Kirov_subsampled", 
                            ifelse (pop == "c_ll_no_n008", "Norway", 
                            ifelse (pop == "c_lp_sm_n019", "Andujar", 
                            ifelse (pop == "c_lp_sm_n012", "Andujar_subsampled", 
                            ifelse (pop == "c_lp_do_n012", "Donana", NA)))))))) %>% 
  mutate (., Species =ifelse (pop == "c_ll_po_n008", "L.lynx",
                            ifelse (pop == "c_ll_ki_n013", "L.lynx", 
                            ifelse (pop == "c_ll_ki_n008", "L.lynx",
                            ifelse (pop == "c_ll_no_n008", "L.lynx", 
                            ifelse (pop == "c_lp_sm_n019", "L.pardinus", 
                            ifelse (pop == "c_lp_sm_n019", "L.pardinus", 
                            ifelse (pop == "c_lp_do_n012", "L.pardinus", NA))))))))



diversity_stats_df_grouped$Populations <- factor (diversity_stats_df_grouped$Populations, levels=c("Kirov","Kirov_subsampled","Bialowieza","Norway", "Andujar", "Andujar_subsampled","Donana"))


 if("chr" %in% colnames(diversity_stats_df_grouped))
{diversity_stats_df_grouped$chr <- factor(diversity_stats_df_grouped$chr, levels = c("chrA1","chrC1","chrB1","chrA2","chrC2","chrB2","chrB3","chrB4","chrA3","chrD1","chrD3","chrD4","chrD2","chrF2","chrF1","chrE2","chrE1","chrX"))}


  dataframename <- paste("stats_df", GROUP, sep ='_')
  assign(dataframename,diversity_stats_df_grouped,.GlobalEnv)
  return (dataframename)

}


for (GROUP in GROUPS)
{
 diversity_stats_grouped(data_diversity_filtered, GROUP) 
}


# Lo calculamos con la tabla filtrada. 

write.table(stats_df_chr, paste(wd_output,"stats_diversity_per_pop_per_chr.tsv",sep=""),quote = F,  row.names = F, sep =";", dec="," )

write.table(stats_df_feature, paste(wd_output,"stats_diversity_per_pop_per_feature.tsv",sep=""),quote = F,  row.names = F, sep =";", dec="," )

write.table(stats_df_region, paste(wd_output,"stats_diversity_per_pop_per_region.tsv",sep=""),quote = F,  row.names = F, sep =";", dec="," )

```

#### Empirical valuer per pop & different groups- group (feature-chr, feature-region, chr-region) 

```{r}

GROUPS=c("feature", "region", "chr")

all_combinations <- combn(GROUPS, 2)

for (ITERATION in 1:ncol(all_combinations))
  {
   GROUP1=all_combinations[, ITERATION][1]
   GROUP2=all_combinations[, ITERATION][2]
    
    print (GROUP1)
    print (GROUP2)
  
 diversity_stats_df_grouped <- 
             data_diversity_filtered %>% 
             dplyr::group_by_("pop",GROUP1, GROUP2) %>%
             dplyr::summarise(
                       total_count=n(), 

                       mean_watterson_ave=mean(watterson_ave),
                       mean_pairwise_ave=mean(pairwise_ave),
                       mean_tajimaD=mean(tajimaD),
                       
                       median_watterson_ave=median(watterson_ave),
                       median_pairwise_ave=median(pairwise_ave),
                       median_tajimaD=median(tajimaD),
                       
                       wmean_watterson_ave=weighted.mean(watterson_ave,w=informative_sites),
                       wmean_pairwise_ave=weighted.mean(pairwise_ave,w=informative_sites),
                       wmean_tajimaD=mean(tajimaD,w =informative_sites),
  
                       wmean_var_watterson_ave = wtd.var(watterson_ave,informative_sites),
                       wmean_var_pairwise_ave = wtd.var(pairwise_ave,informative_sites),
                       wmean_var_tajimaD = wtd.var(tajimaD,informative_sites),
  
                       
                       wmean_sd_watterson_ave =  sqrt(wmean_var_watterson_ave),
                       wmean_sd_pairwise_ave =  sqrt(wmean_var_pairwise_ave),
                       wmean_sd_tajimaD =  sqrt(wmean_var_tajimaD),

                       q25_watterson_ave=quantile(watterson_ave,0.25),
                       q25_pairwise_ave=quantile(pairwise_ave,0.25),
                       q25_tajimaD=quantile(tajimaD,0.25),
                       
                       q75_watterson_ave=quantile(watterson_ave,0.75),
                       q75_pairwise_ave=quantile(pairwise_ave,0.75),
                       q75_tajimaD=quantile(tajimaD,0.75),
                       
                       q05_watterson_ave=quantile(watterson_ave,0.05),
                       q05_pairwise_ave=quantile(pairwise_ave,0.05),
                       q05_tajimaD=quantile(tajimaD,0.05),
                       
                       q95_watterson_ave=quantile(watterson_ave,0.95),
                       q95_pairwise_ave=quantile(pairwise_ave,0.95),
                       q95_tajimaD=quantile(tajimaD,0.95),
                       
                       IQR_watterson_ave = q75_watterson_ave - q25_watterson_ave,
                       IQR_pairwise_ave  = q75_pairwise_ave  - q25_pairwise_ave,
                       IQR_tajimaD       = q75_tajimaD       - q25_tajimaD ,
                       
                       lowerw_watterson_ave = q25_watterson_ave - 1.5* IQR_watterson_ave,
                       lowerw_pairwise_ave  = q25_pairwise_ave  - 1.5* IQR_pairwise_ave,
                       lowerw_tajimaD       = q25_tajimaD       - 1.5* IQR_tajimaD,
                       
                       upperw_watterson_ave = q75_watterson_ave + 1.5* IQR_watterson_ave,
                       upperw_pairwise_ave  = q75_pairwise_ave  + 1.5* IQR_pairwise_ave) %>% 
  mutate (., Populations =  ifelse (pop == "c_ll_po_n008", "Bialowieza",
                            ifelse (pop == "c_ll_ki_n013", "Kirov", 
                            ifelse (pop == "c_ll_ki_n008", "Kirov_subsampled", 
                            ifelse (pop == "c_ll_no_n008", "Norway", 
                            ifelse (pop == "c_lp_sm_n019", "Andujar", 
                            ifelse (pop == "c_lp_sm_n012", "Andujar_subsampled", 
                            ifelse (pop == "c_lp_do_n012", "Donana", NA)))))))) %>% 
  mutate (., Species =ifelse (pop == "c_ll_po_n008", "L.lynx",
                            ifelse (pop == "c_ll_ki_n013", "L.lynx", 
                            ifelse (pop == "c_ll_ki_n008", "L.lynx",
                            ifelse (pop == "c_ll_no_n008", "L.lynx", 
                            ifelse (pop == "c_lp_sm_n019", "L.pardinus", 
                            ifelse (pop == "c_lp_sm_n019", "L.pardinus", 
                            ifelse (pop == "c_lp_do_n012", "L.pardinus", NA))))))))



diversity_stats_df_grouped$Populations <- factor (diversity_stats_df_grouped$Populations, levels=c("Kirov","Bialowieza","Norway", "Andujar", "Donana"))


 if("chr" %in% colnames(diversity_stats_df_grouped)){diversity_stats_df_grouped$chr <- factor(diversity_stats_df_grouped$chr, levels = c("chrA1","chrC1","chrB1","chrA2","chrC2","chrB2","chrB3","chrB4","chrA3","chrD1","chrD3","chrD4","chrD2","chrF2","chrF1","chrE2","chrE1","chrX"))}


  dataframename <- paste("stats_df", GROUP1, GROUP2, sep ='_')
  assign(dataframename,diversity_stats_df_grouped,.GlobalEnv)

}

# Lo calculamos con la tabla filtrada. 

write.table(stats_df_feature_region, paste(wd_output,"stats_diversity_per_pop_per_feature_per_region.tsv",sep=""),quote = F,  row.names = F, sep =";", dec="," )
write.table(stats_df_feature_chr, paste(wd_output,"stats_diversity_per_pop_per_feature_per_chr.tsv",sep=""),quote = F,  row.names = F, sep =";", dec="," )
write.table(stats_df_region_chr, paste(wd_output,"stats_diversity_per_pop_per_region_per_chr.tsv",sep=""),quote = F,  row.names = F, sep =";", dec="," )

```

#### Bootstraped mean 


```{r}

library(boot)
library(broom)

# Cargo la funcion de bootstrap
boot_mean <- function(original_vector, resample_vector) {
   mean(original_vector[resample_vector])
}

# Cargo la función global
diversity_stats_grouped_boot <- function(DATAFRAME ,GROUP)
{ 
  
    DATAFRAME_nested <- DATAFRAME %>% dplyr::group_by_("pop", GROUP) %>% tidyr::nest()
  pairwise_average <- DATAFRAME_nested %<>% dplyr::mutate(booted = purrr::map(.x = data, ~ boot::boot(data = as.numeric(.x$pairwise_ave), statistic = boot_mean, R = ITERATION))) %>% 
    dplyr::mutate(boot_matrix = purrr::map(.x = booted, ~ .x$t)) %>% 
    dplyr::select(-data, -booted) %>% tidyr::unnest(boot_matrix)  
  names(pairwise_average)[3] <- "values"
  pairwise_average_summary <- pairwise_average  %>% 
    group_by_(.dots=c("pop", GROUP)) %>%
    dplyr::summarise(mean=mean(values),sd=sd(values)) %>% 
    mutate (., Populations =  ifelse (pop == "c_ll_po_n008", "Bialowieza",
                              ifelse (pop == "c_ll_ki_n013", "Kirov",
                              ifelse (pop == "c_ll_ki_n008", "Kirov_subsampled", 
                              ifelse (pop == "c_ll_no_n008", "Norway", 
                              ifelse (pop == "c_lp_sm_n019", "Andujar",
                              ifelse (pop == "c_lp_sm_n012", "Andujar_subsampled", 
                              ifelse (pop == "c_lp_do_n012", "Donana", NA)))))))) %>% 
    mutate (., Species =ifelse (pop == "c_ll_po_n008", "L.lynx",
                              ifelse (pop == "c_ll_ki_n013", "L.lynx",                               
                              ifelse (pop == "c_ll_ki_n008", "L.lynx", 
                              ifelse (pop == "c_ll_no_n008", "L.lynx", 
                              ifelse (pop == "c_lp_sm_n019", "L.pardinus", 
                              ifelse (pop == "c_lp_sm_n012", "L.pardinus", 
                              ifelse (pop == "c_lp_do_n012", "L.pardinus", NA))))))))
  names(pairwise_average_summary)[3] <- "mean_pairwise_boot"
  names(pairwise_average_summary)[4] <- "sd_pairwise_boot"
  
  
        DATAFRAME_nested <- DATAFRAME %>% dplyr::group_by_("pop", GROUP) %>% tidyr::nest()
  watterson_average <- DATAFRAME_nested %<>% dplyr::mutate(booted = purrr::map(.x = data, ~ boot::boot(data = as.numeric(.x$watterson_ave), statistic = boot_mean, R = ITERATION))) %>% 
    dplyr::mutate(boot_matrix = purrr::map(.x = booted, ~ .x$t)) %>% 
    dplyr::select(-data, -booted) %>% tidyr::unnest(boot_matrix)  
  names(watterson_average)[3] <- "values"
  watterson_average_summary <- watterson_average  %>% 
    group_by_(.dots=c("pop", GROUP)) %>%
    dplyr::summarise(mean=mean(values),sd=sd(values)) %>%     
    mutate (., Populations =  ifelse (pop == "c_ll_po_n008", "Bialowieza",
                              ifelse (pop == "c_ll_ki_n013", "Kirov",
                              ifelse (pop == "c_ll_ki_n008", "Kirov_subsampled", 
                              ifelse (pop == "c_ll_no_n008", "Norway", 
                              ifelse (pop == "c_lp_sm_n019", "Andujar",
                              ifelse (pop == "c_lp_sm_n012", "Andujar_subsampled", 
                              ifelse (pop == "c_lp_do_n012", "Donana", NA)))))))) %>% 
    mutate (., Species =ifelse (pop == "c_ll_po_n008", "L.lynx",
                              ifelse (pop == "c_ll_ki_n013", "L.lynx",                               
                              ifelse (pop == "c_ll_ki_n008", "L.lynx", 
                              ifelse (pop == "c_ll_no_n008", "L.lynx", 
                              ifelse (pop == "c_lp_sm_n019", "L.pardinus", 
                              ifelse (pop == "c_lp_sm_n012", "L.pardinus", 
                              ifelse (pop == "c_lp_do_n012", "L.pardinus", NA))))))))
  names(watterson_average_summary)[3] <- "mean_watterson_boot"
  names(watterson_average_summary)[4] <- "sd_watterson_boot"
  
  
  
  DATAFRAME_nested <- DATAFRAME %>% dplyr::group_by_("pop", GROUP) %>% tidyr::nest()
  tajimaD <- DATAFRAME_nested %<>% dplyr::mutate(booted = purrr::map(.x = data, ~ boot::boot(data = as.numeric(.x$tajimaD), statistic = boot_mean, R = ITERATION))) %>% 
    dplyr::mutate(boot_matrix = purrr::map(.x = booted, ~ .x$t)) %>% 
    dplyr::select(-data, -booted) %>% tidyr::unnest(boot_matrix)  
  names(tajimaD)[3] <- "values"
  tajimaD_summary <- tajimaD  %>% 
    group_by_(.dots=c("pop", GROUP)) %>%
    dplyr::summarise(mean=mean(values),sd=sd(values)) %>% 
        mutate (., Populations =  ifelse (pop == "c_ll_po_n008", "Bialowieza",
                              ifelse (pop == "c_ll_ki_n013", "Kirov",
                              ifelse (pop == "c_ll_ki_n008", "Kirov_subsampled", 
                              ifelse (pop == "c_ll_no_n008", "Norway", 
                              ifelse (pop == "c_lp_sm_n019", "Andujar",
                              ifelse (pop == "c_lp_sm_n012", "Andujar_subsampled", 
                              ifelse (pop == "c_lp_do_n012", "Donana", NA)))))))) %>% 
    mutate (., Species =ifelse (pop == "c_ll_po_n008", "L.lynx",
                              ifelse (pop == "c_ll_ki_n013", "L.lynx",                               
                              ifelse (pop == "c_ll_ki_n008", "L.lynx", 
                              ifelse (pop == "c_ll_no_n008", "L.lynx", 
                              ifelse (pop == "c_lp_sm_n019", "L.pardinus", 
                              ifelse (pop == "c_lp_sm_n012", "L.pardinus", 
                              ifelse (pop == "c_lp_do_n012", "L.pardinus", NA))))))))
  names(tajimaD_summary)[3] <- "mean_tajimaD_boot"
  names(tajimaD_summary)[4] <- "sd_tajimaD_boot"
  
  
  
  
  
  tmp_group <- full_join (watterson_average_summary, pairwise_average_summary, by = c("pop", GROUP, "Populations", "Species")) %>% full_join (., tajimaD_summary, by = c("pop", GROUP, "Populations", "Species"))
  tmp_group$Populations <- factor (tmp_group$Populations, levels=c("Kirov","Kirov_subsampled","Bialowieza","Norway", "Andujar","Andujar_subsampled", "Donana"))
  
  
  if("chr" %in% colnames(tmp_group))
{tmp_group$chr <- factor(tmp_group$chr, levels = c("chrA1","chrC1","chrB1","chrA2","chrC2","chrB2","chrB3","chrB4","chrA3","chrD1","chrD3","chrD4","chrD2","chrF2","chrF1","chrE2","chrE1","chrX"))}
  
  tmp_group_name <- paste ("stats_df", GROUP,"boostrapped", sep="_")
  assign (tmp_group_name, tmp_group,.GlobalEnv)    
  
  rm (tmp_group)
  rm (watterson_average)
  rm (watterson_average_summary)
  rm (pairwise_average)
  rm (pairwise_average_summary)
  rm (tajimaD)
  rm (tajimaD_summary)
  rm (DATAFRAME_nested)
  
}


ITERATION=10
GROUPS=c("feature", "region", "chr")

 for (GROUP in GROUPS)
{
 diversity_stats_grouped_boot(data_diversity_filtered, GROUP) 
 }


# Lo guardo 

write.table(stats_df_chr_boostrapped, paste(wd_output,"stats_diversity_per_pop_per_chr_bootstrapped.tsv",sep=""), quote = F,  row.names = F, sep =";", dec="," )
write.table(stats_df_feature_boostrapped, paste(wd_output,"stats_diversity_per_pop_per_feature_boostrapped.tsv",sep=""), quote = F,  row.names = F, sep =";", dec="," )
write.table(stats_df_region_boostrapped, paste(wd_output,"stats_diversity_per_pop_per_region_boostrapped.tsv",sep=""), quote = F,  row.names = F, sep =";", dec="," )


```

Nota:
He comprobado la normalidad en un caso concreto, pero creo que puede ser aplicable a todo.

Para el caso:
kk <- DATAFRAME_nested %>%  filter(pop=="c_ll_ki_n013") %>% filter(feature=="CDS")
shapiro.test(kk$boot_matrix)

Shapiro-Wilk normality test

data:  kk$boot_matrix
W = 0.99973, p-value = 0.8071

From the output, the p-value > 0.05 implying that the distribution of the data are not significantly different from normal distribution. In other words, we can assume the normality.

Así que perfecto!

#### Graph empirical boxplot & violin plot 

```{r}
library(rlang)

# Diversidad por población para cada especie y para cada indice
## BOXPLOT

INDICES=c("watterson_ave", "pairwise_ave", "tajimaD")
GROUPS=c("feature", "region", "chr")

for (SPECIE in unique(data_diversity_filtered$specie))
{print (SPECIE)
  for (INDEX in INDICES)
  {print (INDEX)
    for (GROUP in GROUPS)
    {print (GROUP)
      # BOXPLOT
      ggplot(data=filter(data_diversity_filtered,specie==!!SPECIE), aes_string(x="Populations", y = INDEX, fill="Populations")) +
        geom_boxplot(outlier.shape=NA, color="black") +
        facet_grid(reformulate(GROUP), scales="free", switch="x") +
        scale_y_continuous(trans = 'log10') +
        scale_x_discrete(GROUP) +
        xlab(label = "Genomic region") + #x title
        ylab(label = paste(INDEX)) + # y title
        theme_bw() +  #theme selection for background and lines
        # scale_x_continuous(expand = c(0, 1)) +# + scale_y_continuous(expand = c(0, 0)) +
        theme(axis.line.x = element_line(color="black", size = 0.5),
              axis.line.y = element_line(color="black", size = 0.5),
              # axis.text.x = element_text(angle = 60, vjust = 0.8, hjust = 1),
              axis.text.x = element_blank(),
              axis.text.y = element_text(vjust = 0.2, hjust = 0.2),
              axis.title.y = element_text(margin=margin(r=0.3, unit="cm")),
              axis.title.x = element_text(margin=margin(t=0.5, unit="cm")),
              panel.grid.major = element_blank(),
              panel.grid.minor = element_blank(),
              panel.border = element_blank(),
              panel.background = element_blank(),
              strip.background = element_blank(),
              strip.text = element_text(angle = 90))
             # panel.spacing = unit(1.5, "lines"))
      ggsave(paste (wd_output, "boxplot_violin_plot/",INDEX,"_per_",GROUP,"_",SPECIE,"_boxplot.pdf", sep="" ),  width = 50, height = 60, units = "cm", device="pdf")
      
      # VIOLIN
      
      ggplot(data=filter(data_diversity_filtered,specie==!!SPECIE), aes_string(x="Populations", y = INDEX, fill="Populations")) +
        geom_violin() +
        facet_grid(reformulate(GROUP), scales="free", switch="x") +
        scale_y_continuous(trans = 'log10') +
        scale_x_discrete(GROUP) +
        xlab(label = "Genomic region") + #x title
        ylab(label = paste(INDEX)) + # y title
        theme_bw() +  #theme selection for background and lines
        # scale_x_continuous(expand = c(0, 1)) +# + scale_y_continuous(expand = c(0, 0)) +
        theme(axis.line.x = element_line(color="black", size = 0.5),
              axis.line.y = element_line(color="black", size = 0.5),
              # axis.text.x = element_text(angle = 60, vjust = 0.8, hjust = 1),
              axis.text.x = element_blank(),
              axis.text.y = element_text(vjust = 0.2, hjust = 0.2),
              axis.title.y = element_text(margin=margin(r=0.3, unit="cm")),
              axis.title.x = element_text(margin=margin(t=0.5, unit="cm")),
              panel.grid.major = element_blank(),
              panel.grid.minor = element_blank(),
              panel.border = element_blank(),
              panel.background = element_blank(),
              strip.background = element_blank(),
              strip.text = element_text(angle = 90))
            #  panel.spacing = unit(1.5, "lines"))
      ggsave(paste (wd_output,"boxplot_violin_plot/", INDEX,"_per_",GROUP,"_",SPECIE,"_violin.pdf", sep="" ),  width = 50, height = 60, units = "cm", device="pdf")
      
    }
  }
}



```

#### Graph empirical weighted average


```{r}

DATAFRAMES=c("stats_df_chr", "stats_df_feature","stats_df_region")
SPECIES=c("L.lynx","L.pardinus")

for (DATAFRAME in DATAFRAMES)
  {
  for (SPECIE in SPECIES)
  {
    print (SPECIE)
  ggplot(data=get(DATAFRAME) %>% filter(.,Species==!!SPECIE), aes_string(x="Populations",y="wmean_pairwise_ave", fill="Populations", color="Populations")) +
      geom_point() +
      facet_grid(reformulate(colnames(get(DATAFRAME))[2]), scales="free", switch="x") +
      #scale_y_continuous(trans = 'log10') +
      theme_bw() +  #theme selection for background and lines
      theme(axis.line.x = element_line(color="black", size = 0.5),
            axis.text.x = element_blank(),strip.text = element_text(angle = 90))
    ggsave(paste (wd_output,"wmean_pairwise_per_",colnames(get(DATAFRAME))[2],"_",SPECIE,".pdf", sep="" ), device="pdf")
    
    ggplot(data=get(DATAFRAME) %>% filter(.,Species==!!SPECIE), aes_string(x="Populations",y="wmean_watterson_ave", fill="Populations", color="Populations")) +
      geom_point() +
      facet_grid(reformulate(colnames(get(DATAFRAME))[2]), scales="free", switch="x") +
      #scale_y_continuous(trans = 'log10') +
      theme_bw() +  #theme selection for background and lines
      theme(axis.line.x = element_line(color="black", size = 0.5),
            axis.text.x = element_blank(),strip.text = element_text(angle = 90))
    ggsave(paste (wd_output,"wmean_watterson_per_",colnames(get(DATAFRAME))[2],"_",SPECIE,".pdf", sep="" ), device="pdf")
    
    
    ggplot(data=get(DATAFRAME) %>% filter(.,Species==!!SPECIE), aes_string(x="Populations",y="wmean_tajimaD", fill="Populations", color="Populations")) +
      geom_point() +
      facet_grid(reformulate(colnames(get(DATAFRAME))[2]), scales="free", switch="x") +
      #scale_y_continuous(trans = 'log10') +
      theme_bw() +  #theme selection for background and lines
      theme(axis.line.x = element_line(color="black", size = 0.5),
            axis.text.x = element_blank(),strip.text = element_text(angle = 90))
    ggsave(paste (wd_output,"wmean_tajimaD_per_",colnames(get(DATAFRAME))[2],"_",SPECIE,".pdf", sep="" ), device="pdf")
   }
}

  
```

#### Graph bootstrapped data


```{r}

DATAFRAMES=c("stats_df_chr_boostrapped", "stats_df_feature_boostrapped","stats_df_region_boostrapped")
SPECIES=c("L.lynx","L.pardinus")

for (DATAFRAME in DATAFRAMES)
  {
  for (SPECIE in SPECIES)
  {
    print (SPECIE)
  ggplot(data=get(DATAFRAME) %>% filter(.,Species==!!SPECIE), aes_string(x="Populations",y="mean_pairwise_boot", fill="Populations", color="Populations")) +
      geom_point() +
      geom_errorbar(aes(ymin=(mean_pairwise_boot-sd_pairwise_boot), ymax=(mean_pairwise_boot+sd_pairwise_boot))) +
      facet_grid(reformulate(colnames(get(DATAFRAME))[2]), scales="free", switch="x") +
      #scale_y_continuous(trans = 'log10') +
      theme_bw() +  #theme selection for background and lines
      theme(axis.line.x = element_line(color="black", size = 0.5),
            axis.text.x = element_blank(),strip.text = element_text(angle = 90))
    ggsave(paste (wd_output,"mean_pairwise_boot_per_",colnames(get(DATAFRAME))[2],"_",SPECIE,".pdf", sep="" ), device="pdf")
    
    ggplot(data=get(DATAFRAME) %>% filter(.,Species==!!SPECIE), aes_string(x="Populations",y="mean_watterson_boot", fill="Populations", color="Populations")) +
      geom_point() +
      geom_errorbar(aes(ymin=(mean_watterson_boot-sd_pairwise_boot), ymax=(mean_watterson_boot+sd_pairwise_boot))) +
      facet_grid(reformulate(colnames(get(DATAFRAME))[2]), scales="free", switch="x") +
      #scale_y_continuous(trans = 'log10') +
      theme_bw() +  #theme selection for background and lines
      theme(axis.line.x = element_line(color="black", size = 0.5),
            axis.text.x = element_blank(),strip.text = element_text(angle = 90))
    ggsave(paste (wd_output,"mean_watterson_boot_per_",colnames(get(DATAFRAME))[2],"_",SPECIE,".pdf", sep="" ), device="pdf")
    
     ggplot(data=get(DATAFRAME) %>% filter(.,Species==!!SPECIE), aes_string(x="Populations",y="mean_tajimaD_boot", fill="Populations", color="Populations")) +
      geom_point() +
      geom_errorbar(aes(ymin=(mean_tajimaD_boot-sd_tajimaD_boot), ymax=(mean_tajimaD_boot+sd_tajimaD_boot))) +
      facet_grid(reformulate(colnames(get(DATAFRAME))[2]), scales="free", switch="x") +
      #scale_y_continuous(trans = 'log10') +
      theme_bw() +  #theme selection for background and lines
      theme(axis.line.x = element_line(color="black", size = 0.5),
            axis.text.x = element_blank(),strip.text = element_text(angle = 90))
    ggsave(paste (wd_output,"mean_tajimaD_boot_per_",colnames(get(DATAFRAME))[2],"_",SPECIE,".pdf", sep="" ), device="pdf")

   }
}

  
```


## Tablas corregidas para la media de intergénico.


General


```{r}

# Saco las tablas para cada población y añado una columna en la que corrijo por la media de pairwise. 
"c_ll_ki_n008"

POPS=c("c_ll_ki_n013","c_ll_no_n008","c_ll_po_n008", "c_lp_sm_n019", "c_lp_sm_n012", "c_lp_do_n012")

for (POP in POPS)
{
  print (POP)

  # 1.- Media por unidad de intergénico.
  pairwise_average_intergenic_per_unit <- as.numeric(stats_df_feature %>% filter (pop==as.character(POP)) %>% filter (feature=="intergenic") %>% select (mean_pairwise_ave) %>% .[1,2])
  watterson_average_intergenic_per_unit <- as.numeric(stats_df_feature %>% filter (pop==as.character(POP)) %>% filter (feature=="intergenic") %>% select (mean_watterson_ave) %>% .[1,2])
  
  # 2.- Media ponderada de intergénico.
  # pairwise_average_intergenic_weighted <- as.numeric(stats_df_feature %>% filter (pop==as.character(POP)) %>% filter (feature=="intergenic") %>% select (wmean_pairwise_ave) %>% .[1,2])
  # watterson_average_intergenic_weighted <- as.numeric(stats_df_feature %>% filter (pop==as.character(POP)) %>% filter (feature=="intergenic") %>% select (wmean_watterson_ave) %>% .[1,2])
  
   data.frame <-  data_diversity_filtered %>%  filter (pop==as.character(POP)) %>% 
  mutate(corrected_by_pairwise_average_intergenic_per_unit = pairwise_ave/pairwise_average_intergenic_per_unit) %>%  
  mutate(corrected_by_watterson_average_intergenic_per_unit = watterson_ave/watterson_average_intergenic_per_unit) #%>% 
  # mutate(corrected_by_pairwise_average_intergenic_weighted = pairwise_ave/pairwise_average_intergenic_weighted) %>%  
  # mutate(corrected_by_watterson_average_intergenic_weighted = watterson_ave/watterson_average_intergenic_weighted)
  
  
  assign(paste(POP,"_diversity", sep=""), data.frame)
  rm (data.frame)
  
}

# Hago una tabla con la media para cada grupo y para cada población. 
c_ll_ki_n008_diversity


data_diversity_filtered_ave_corrected <- rbind (c_ll_ki_n013_diversity,  c_ll_no_n008_diversity, c_ll_po_n008_diversity,  c_lp_sm_n019_diversity, c_lp_sm_n012_diversity, c_lp_do_n012_diversity)


INDICES=c("corrected_by_pairwise_average_intergenic_per_unit", "corrected_by_watterson_average_intergenic_per_unit")

GROUPS=c("feature")



diversity_stats_df_feature_corrected <- 
  data_diversity_filtered_ave_corrected %>% 
  dplyr::group_by(pop, feature) %>%
  dplyr::summarise(corrected_by_watterson_average_intergenic_per_unit=mean(corrected_by_watterson_average_intergenic_per_unit),
            corrected_by_pairwise_average_intergenic_per_unit=mean(corrected_by_pairwise_average_intergenic_per_unit)) %>% 
  mutate (., specie = ifelse (pop=="c_ll_ki_n013" | pop=="c_ll_po_n008" | pop=="c_ll_no_n008", "ll", "lp")) %>% 
  mutate (., Populations =  ifelse (pop == "c_ll_po_n008", "Bialowieza",
                            ifelse (pop == "c_ll_ki_n013", "Kirov", 
                            ifelse (pop == "c_ll_ki_n008", "Kirov_subsampled", 
                            ifelse (pop == "c_ll_no_n008", "Norway", 
                            ifelse (pop == "c_lp_sm_n019", "Andujar",
                            ifelse (pop == "c_lp_sm_n012", "Andujar_subsampled",
                            ifelse (pop == "c_lp_do_n012", "Donana", NA)))))))) %>% 
  mutate (., Species =ifelse (pop == "c_ll_po_n008", "L.lynx",
                            ifelse (pop == "c_ll_ki_n013", "L.lynx",
                            ifelse (pop == "c_ll_ki_n008", "L.lynx", 
                            ifelse (pop == "c_ll_no_n008", "L.lynx", 
                            ifelse (pop == "c_lp_sm_n019", "L.pardinus",
                            ifelse (pop == "c_lp_sm_n012", "L.pardinus", 
                            ifelse (pop == "c_lp_do_n012", "L.pardinus", NA))))))))

diversity_stats_df_feature_corrected$Populations <- factor (diversity_stats_df_feature_corrected$Populations, levels=c("Kirov","Kirov_subsampled", "Bialowieza","Norway", "Andujar","Andujar_subsampled", "Donana"))



write.table(diversity_stats_df_feature_corrected, paste(wd_output,"stats_diversity_per_pop_per_feature_corrected.tsv",sep=""),quote = F,  row.names = F, sep =";", dec="," )


```


## Represento  media corregida 
Opcionalmente también boxplot y violin plot.

```{r}

library(rlang)
# Diversidad por población para cada especie y para cada indice
## BOXPLOT

INDICES=c("corrected_by_pairwise_average_intergenic_per_unit", "corrected_by_watterson_average_intergenic_per_unit")
GROUPS=c("feature")

for (SPECIE in unique(diversity_stats_df_feature_corrected$specie))
{print (SPECIE)
  for (INDEX in INDICES)
  {print (INDEX)
    for (GROUP in GROUPS)
    {print (GROUP)
      
      ggplot(data=filter(diversity_stats_df_feature_corrected,specie==!!SPECIE), aes_string(x="Populations", y = INDEX, fill="Populations", color="Populations")) +
        geom_point() +
        facet_grid(~feature, scales="free", switch="x") +
        scale_x_discrete(GROUP) +
        xlab(label = "Genomic region") + #x title
        ylab(label = paste(INDEX)) + # y title
        theme_bw() +  #theme selection for background and lines
        # scale_x_continuous(expand = c(0, 1)) +# + scale_y_continuous(expand = c(0, 0)) +
        theme(axis.line.x = element_line(color="black", size = 0.5),
              axis.line.y = element_line(color="black", size = 0.5),
              # axis.text.x = element_text(angle = 60, vjust = 0.8, hjust = 1),
              axis.text.x = element_blank(),
              axis.text.y = element_text(vjust = 0.2, hjust = 0.2),
              axis.title.y = element_text(margin=margin(r=0.3, unit="cm")),
              axis.title.x = element_text(margin=margin(t=0.5, unit="cm")),
              #panel.grid.major = element_blank(),
              #panel.grid.minor = element_blank(),
              panel.border = element_blank(),
              panel.background = element_blank(),
              strip.background = element_blank(),
              strip.text = element_text(angle = 90))
              #panel.spacing = unit(1.5, "lines"))
      ggsave(paste (wd_output, INDEX,"_per_",GROUP,"_",SPECIE,".pdf", sep="" ),  device="pdf")
      
      
      # BOXPLOT
      # ggplot() +
      #   geom_boxplot(data=filter(data_diversity_filtered_ave_corrected,specie==!!SPECIE), aes_string(x="Populations", y = INDEX, fill="Populations"), color="black") +
      #   facet_grid(reformulate(GROUP), scales="free", switch="x") +
      #   scale_y_continuous(trans = 'log10') +
      #   scale_x_discrete(GROUP) +
      #   xlab(label = "Genomic region") + #x title
      #   ylab(label = paste(INDEX)) + # y title
      #   theme_bw() +  #theme selection for background and lines
      #   # scale_x_continuous(expand = c(0, 1)) +# + scale_y_continuous(expand = c(0, 0)) +
      #   theme(axis.line.x = element_line(color="black", size = 0.5),
      #         axis.line.y = element_line(color="black", size = 0.5),
      #         # axis.text.x = element_text(angle = 60, vjust = 0.8, hjust = 1),
      #         axis.text.x = element_blank(),
      #         axis.text.y = element_text(vjust = 0.2, hjust = 0.2),
      #         axis.title.y = element_text(margin=margin(r=0.3, unit="cm")),
      #         axis.title.x = element_text(margin=margin(t=0.5, unit="cm")),
      #         panel.grid.major = element_blank(),
      #         panel.grid.minor = element_blank(),
      #         panel.border = element_blank(),
      #         panel.background = element_blank(),
      #         strip.background = element_blank(),
      #         strip.text = element_text(angle = 90),
      #         panel.spacing = unit(1.5, "lines"))
      # ggsave(paste (wd_output, INDEX,"_per_",GROUP,"_",SPECIE,"_boxplot.pdf", sep="" ),  width = 50, height = 60, units = "cm", device="pdf")
      # 
      # # VIOLIN
      # 
      # ggplot(data=filter(data_diversity_filtered_ave_corrected,specie==!!SPECIE), aes_string(x="Populations", y = INDEX, fill="Populations")) +
      #   geom_violin(color="black") +
      #   facet_grid(reformulate(GROUP), scales="free", switch="x") +
      #   scale_y_continuous(trans = 'log10') +
      #   scale_x_discrete(GROUP) +
      #   xlab(label = "Genomic region") + #x title
      #   ylab(label = paste(INDEX)) + # y title
      #   theme_bw() +  #theme selection for background and lines
      #   # scale_x_continuous(expand = c(0, 1)) +# + scale_y_continuous(expand = c(0, 0)) +
      #   theme(axis.line.x = element_line(color="black", size = 0.5),
      #         axis.line.y = element_line(color="black", size = 0.5),
      #         # axis.text.x = element_text(angle = 60, vjust = 0.8, hjust = 1),
      #         axis.text.x = element_blank(),
      #         axis.text.y = element_text(vjust = 0.2, hjust = 0.2),
      #         axis.title.y = element_text(margin=margin(r=0.3, unit="cm")),
      #         axis.title.x = element_text(margin=margin(t=0.5, unit="cm")),
      #         panel.grid.major = element_blank(),
      #         panel.grid.minor = element_blank(),
      #         panel.border = element_blank(),
      #         panel.background = element_blank(),
      #         strip.background = element_blank(),
      #         strip.text = element_text(angle = 90),
      #         panel.spacing = unit(1.5, "lines"))
      # ggsave(paste (wd_output, INDEX,"_per_",GROUP,"_",SPECIE,"_violin.pdf", sep="" ),  width = 50, height = 60, units = "cm", device="pdf")
      # 
      
      
      
    }
  }
}



```

## Creo tablas interacción entre non-bootleneck vs bootleneck



###Sobre la general

```{r}

# Dataframe Kirov-Norway

# Function:

# Cuidado porque aquí estoy asumiendo que la region es igual, y esto parece que es asi, pero no estoy 100% segura de que no haya podido pasar que en una población se haya definido como telomérico y en otra no por falta de bases. 

calculus_data_diversity_bootleneck_vs_non_bootleneck <- function(POP1, POP2, name_POP1, name_POP2){
  data_diversity_POP1_POP2 <- inner_join (POP1, POP2, by = c("scaffold", "start_cero_based", "end", "length", "feature", "strandness", "frame", "id_gene", "id", "region", "id_feature", "chr")) %>%  
    mutate (ratio_pairwise_average = pairwise_ave.y/pairwise_ave.x) %>% 
    mutate (ratio_watterson_average = watterson_ave.y/watterson_ave.x) %>% 
    mutate (ratio_corrected_by_pairwise_average_intergenic_per_unit = corrected_by_pairwise_average_intergenic_per_unit.y/corrected_by_pairwise_average_intergenic_per_unit.x) %>%
    mutate (ratio_corrected_by_watterson_average_intergenic_per_unit = corrected_by_watterson_average_intergenic_per_unit.y/corrected_by_watterson_average_intergenic_per_unit.x) #%>% 
    #mutate (ratio_corrected_by_pairwise_average_intergenic_weighted = corrected_by_pairwise_average_intergenic_weighted.y/corrected_by_pairwise_average_intergenic_weighted.x) %>% 
    #mutate (ratio_corrected_by_watterson_average_intergenic_weighted = corrected_by_watterson_average_intergenic_weighted.y/corrected_by_watterson_average_intergenic_weighted.x) 
  
  dataframename <- paste ("data_diversity", name_POP1, name_POP2, sep="_")
  assign (dataframename, data_diversity_POP1_POP2,.GlobalEnv)
}

# Create data frame



## CREARLAS!!!!




# Dataframe Kirov-Norway
calculus_data_diversity_bootleneck_vs_non_bootleneck(c_ll_ki_n013_diversity,c_ll_no_n008_diversity, deparse(substitute(c_ll_ki_n013_diversity)),deparse(substitute(c_ll_no_n008_diversity)))

calculus_data_diversity_bootleneck_vs_non_bootleneck(c_ll_ki_n008_diversity,c_ll_no_n008_diversity, deparse(substitute(c_ll_ki_n008_diversity)),deparse(substitute(c_ll_no_n008_diversity)))

# Dataframe Kirov-Poland
calculus_data_diversity_bootleneck_vs_non_bootleneck(c_ll_ki_n013_diversity,c_ll_po_n008_diversity, deparse(substitute(c_ll_ki_n013_diversity)),deparse(substitute(c_ll_po_n008_diversity)))

calculus_data_diversity_bootleneck_vs_non_bootleneck(c_ll_ki_n008_diversity,c_ll_po_n008_diversity, deparse(substitute(c_ll_ki_n008_diversity)),deparse(substitute(c_ll_po_n008_diversity)))


# Dataframe Sierra_Morena-Doñana
calculus_data_diversity_bootleneck_vs_non_bootleneck(c_lp_sm_n019_diversity,c_lp_do_n012_diversity, deparse(substitute(c_lp_sm_n019_diversity)),deparse(substitute(c_lp_do_n012_diversity)))

calculus_data_diversity_bootleneck_vs_non_bootleneck(c_lp_sm_n012_diversity,c_lp_do_n012_diversity, deparse(substitute(c_lp_sm_n012_diversity)),deparse(substitute(c_lp_do_n012_diversity)))


write.table(data_diversity_c_ll_ki_n013_diversity_c_ll_no_n008_diversity, paste(wd_output,"data_diversity_c_ll_ki_n013_c_ll_no_n008.tsv",sep=""),quote = F,  row.names = F, sep =";", dec="," )
write.table(data_diversity_c_ll_ki_n008_diversity_c_ll_no_n008_diversity, paste(wd_output,"data_diversity_c_ll_ki_n008_c_ll_no_n008.tsv",sep=""),quote = F,  row.names = F, sep =";", dec="," )
write.table(data_diversity_c_ll_ki_n013_diversity_c_ll_po_n008_diversity, paste(wd_output,"data_diversity_c_ll_ki_n013_c_ll_po_n008.tsv",sep=""),quote = F,  row.names = F, sep =";", dec="," )
write.table(data_diversity_c_ll_ki_n008_diversity_c_ll_po_n008_diversity, paste(wd_output,"data_diversity_c_ll_ki_n008_c_ll_po_n008.tsv",sep=""),quote = F,  row.names = F, sep =";", dec="," )
write.table(data_diversity_c_lp_sm_n019_diversity_c_lp_do_n012_diversity, paste(wd_output,"data_diversity_c_lp_sm_n019_c_lp_do_n012.tsv",sep=""),quote = F,  row.names = F, sep =";", dec="," )
write.table(data_diversity_c_lp_sm_n012_diversity_c_lp_do_n012_diversity, paste(wd_output,"data_diversity_c_lp_sm_n012_c_lp_do_n012.tsv",sep=""),quote = F,  row.names = F, sep =";", dec="," )


# También las creo a lo largo para poder lanzar modelos

c_ll_ki_n013_diversity_c_ll_po_n008_diversity_for_model <- rbind(c_ll_ki_n013_diversity, c_ll_po_n008_diversity)


# rm(data_diversity)
# rm(c_ll_ki_n013_diversity)
# rm(c_ll_po_n008_diversity)
# rm(c_ll_no_n008_diversity)
# rm(c_lp_sm_n019_diversity)
# rm(c_lp_do_n012_diversity)

```



#### Separate by pop 

```{r}

# Saco las tablas para cada población y añado una columna en la que corrijo por la media de pairwise. 

"c_ll_ki_n008"
POPS=c("c_ll_ki_n013","c_ll_no_n008","c_ll_po_n008", "c_lp_do_n012", "c_lp_sm_n019","c_lp_sm_n012")

# Feature
for (POP in POPS)
{print (POP)
    data.frame <-  stats_df_feature  %>%  filter (pop==as.character(POP))
    assign(paste(POP,"_ave_diversity_feature", sep=""), data.frame)
  rm (data.frame)}

# Chr
for (POP in POPS)
{print (POP)
    data.frame <-  stats_df_chr  %>%  filter (pop==as.character(POP))
    assign(paste(POP,"_ave_diversity_chr", sep=""), data.frame)
  rm (data.frame)}

# Region
for (POP in POPS)
{print (POP)
    data.frame <-  stats_df_region  %>%  filter (pop==as.character(POP))
    assign(paste(POP,"_ave_diversity_region", sep=""), data.frame)
  rm (data.frame)}


## INTERACCION
# Feature - chr
for (POP in POPS)
{print (POP)
    data.frame <-  stats_df_feature_chr  %>%  filter (pop==as.character(POP))
    assign(paste(POP,"_ave_diversity_feature_chr", sep=""), data.frame)
  rm (data.frame)}

# Feature - region
for (POP in POPS)
{ print (POP)
    data.frame <-  stats_df_feature_region  %>%  filter (pop==as.character(POP))
    assign(paste(POP,"_ave_diversity_feature_region", sep=""), data.frame)
  rm (data.frame)}

# Region - chr
for (POP in POPS)
{print (POP)
    data.frame <-  stats_df_region_chr  %>%  filter (pop==as.character(POP))
    assign(paste(POP,"_ave_diversity_region_chr", sep=""), data.frame)
  rm (data.frame)}

```

###Sobre la de stats

#### Median

```{r}

### FEATURE ###

# Calculo el ratio para la media:

bootleneck_vs_non_bootleneck_ratio_median_feature <- function(POP1, POP2, name_POP1, name_POP2){
  ratio_averages_POP1_POP2 <- full_join ( select ( POP1, pop, feature, total_count, median_pairwise_ave, median_watterson_ave), select ( POP2, pop, feature, total_count, median_pairwise_ave, median_watterson_ave), by="feature") %>% 
    mutate (ratio_median_pairwise=median_pairwise_ave.y/median_pairwise_ave.x) %>% 
    mutate (ratio_median_watterson=median_watterson_ave.y/median_watterson_ave.x)
  
  dataframename <- paste ("ratio_median", name_POP1, name_POP2,  sep="_")
  assign (dataframename, ratio_averages_POP1_POP2,.GlobalEnv)
}

# Create data frame

# Dataframe Kirov-Norway

bootleneck_vs_non_bootleneck_ratio_median_feature(c_ll_ki_n013_ave_diversity_feature,c_ll_no_n008_ave_diversity_feature, deparse(substitute(c_ll_ki_n013_ave_diversity_feature)),deparse(substitute(c_ll_no_n008_ave_diversity_feature)))
write.table(ratio_median_c_ll_ki_n013_ave_diversity_feature_c_ll_no_n008_ave_diversity_feature, paste(wd_output,"ratio_median_feature_c_ll_ki_n013_c_ll_no_n008.tsv",sep=""),quote = F,  row.names = F, sep =";", dec="," )

bootleneck_vs_non_bootleneck_ratio_median_feature(c_ll_ki_n008_ave_diversity_feature,c_ll_no_n008_ave_diversity_feature, deparse(substitute(c_ll_ki_n008_ave_diversity_feature)),deparse(substitute(c_ll_no_n008_ave_diversity_feature)))
write.table(ratio_median_c_ll_ki_n008_ave_diversity_feature_c_ll_no_n008_ave_diversity_feature, paste(wd_output,"ratio_median_feature_c_ll_ki_n008_c_ll_no_n008.tsv",sep=""),quote = F,  row.names = F, sep =";", dec="," )


# Dataframe Kirov-Poland

bootleneck_vs_non_bootleneck_ratio_median_feature(c_ll_ki_n013_ave_diversity_feature,c_ll_po_n008_ave_diversity_feature, deparse(substitute(c_ll_ki_n013_ave_diversity_feature)),deparse(substitute(c_ll_po_n008_ave_diversity_feature)))
write.table(ratio_median_c_ll_ki_n013_ave_diversity_feature_c_ll_po_n008_ave_diversity_feature, paste(wd_output,"ratio_median_feature_c_ll_ki_n013_c_ll_po_n008.tsv",sep=""),quote = F,  row.names = F, sep =";", dec="," )

bootleneck_vs_non_bootleneck_ratio_median_feature(c_ll_ki_n008_ave_diversity_feature,c_ll_po_n008_ave_diversity_feature, deparse(substitute(c_ll_ki_n008_ave_diversity_feature)),deparse(substitute(c_ll_po_n008_ave_diversity_feature)))
write.table(ratio_median_c_ll_ki_n008_ave_diversity_feature_c_ll_po_n008_ave_diversity_feature, paste(wd_output,"ratio_median_feature_c_ll_ki_n008_c_ll_po_n008.tsv",sep=""),quote = F,  row.names = F, sep =";", dec="," )


# Dataframe Sierra_Morena-Doñana

bootleneck_vs_non_bootleneck_ratio_median_feature(c_lp_sm_n019_ave_diversity_feature, c_lp_do_n012_ave_diversity_feature, deparse(substitute(c_lp_sm_n019_ave_diversity_feature)),deparse(substitute(c_lp_do_n012_ave_diversity_feature)))
write.table(ratio_median_c_lp_sm_n019_ave_diversity_feature_c_lp_do_n012_ave_diversity_feature, paste(wd_output,"ratio_median_feature_c_lp_sm_n019_c_lp_do_n012.tsv",sep=""),quote = F,  row.names = F, sep =";", dec="," )

bootleneck_vs_non_bootleneck_ratio_median_feature(c_lp_sm_n012_ave_diversity_feature, c_lp_do_n012_ave_diversity_feature, deparse(substitute(c_lp_sm_n012_ave_diversity_feature)),deparse(substitute(c_lp_do_n012_ave_diversity_feature)))
write.table(ratio_median_c_lp_sm_n012_ave_diversity_feature_c_lp_do_n012_ave_diversity_feature, paste(wd_output,"ratio_median_feature_c_lp_sm_n012_c_lp_do_n012.tsv",sep=""),quote = F,  row.names = F, sep =";", dec="," )


### REGION ###

# Calculo el ratio para la media:

bootleneck_vs_non_bootleneck_ratio_median_region <- function(POP1, POP2, name_POP1, name_POP2){
  ratio_averages_POP1_POP2 <- full_join ( select ( POP1, pop, region, total_count,  median_pairwise_ave, median_watterson_ave), select ( POP2, pop, region, total_count,  median_pairwise_ave, median_watterson_ave), by="region") %>% 
    mutate (ratio_median_pairwise=median_pairwise_ave.y/median_pairwise_ave.x) %>% 
    mutate (ratio_median_watterson=median_watterson_ave.y/median_watterson_ave.x)
  
  dataframename <- paste ("ratio_median", name_POP1, name_POP2,  sep="_")
  assign (dataframename, ratio_averages_POP1_POP2,.GlobalEnv)
}

# Create data frame

# Dataframe Kirov-Norway

bootleneck_vs_non_bootleneck_ratio_median_region(c_ll_ki_n013_ave_diversity_region,c_ll_no_n008_ave_diversity_region, deparse(substitute(c_ll_ki_n013_ave_diversity_region)),deparse(substitute(c_ll_no_n008_ave_diversity_region)))
write.table(ratio_median_c_ll_ki_n013_ave_diversity_region_c_ll_no_n008_ave_diversity_region, paste(wd_output,"ratio_median_region_c_ll_ki_n013_c_ll_no_n008.tsv",sep=""),quote = F,  row.names = F, sep =";", dec="," )

bootleneck_vs_non_bootleneck_ratio_median_region(c_ll_ki_n008_ave_diversity_region,c_ll_no_n008_ave_diversity_region, deparse(substitute(c_ll_ki_n008_ave_diversity_region)),deparse(substitute(c_ll_no_n008_ave_diversity_region)))
write.table(ratio_median_c_ll_ki_n008_ave_diversity_region_c_ll_no_n008_ave_diversity_region, paste(wd_output,"ratio_median_region_c_ll_ki_n008_c_ll_no_n008.tsv",sep=""),quote = F,  row.names = F, sep =";", dec="," )

# Dataframe Kirov-Poland

bootleneck_vs_non_bootleneck_ratio_median_region(c_ll_ki_n013_ave_diversity_region,c_ll_po_n008_ave_diversity_region, deparse(substitute(c_ll_ki_n013_ave_diversity_region)),deparse(substitute(c_ll_po_n008_ave_diversity_region)))
write.table(ratio_median_c_ll_ki_n013_ave_diversity_region_c_ll_po_n008_ave_diversity_region, paste(wd_output,"ratio_median_region_c_ll_ki_n013_c_ll_po_n008.tsv",sep=""),quote = F,  row.names = F, sep =";", dec="," )

bootleneck_vs_non_bootleneck_ratio_median_region(c_ll_ki_n008_ave_diversity_region,c_ll_po_n008_ave_diversity_region, deparse(substitute(c_ll_ki_n008_ave_diversity_region)),deparse(substitute(c_ll_po_n008_ave_diversity_region)))
write.table(ratio_median_c_ll_ki_n008_ave_diversity_region_c_ll_po_n008_ave_diversity_region, paste(wd_output,"ratio_median_region_c_ll_ki_n008_c_ll_po_n008.tsv",sep=""),quote = F,  row.names = F, sep =";", dec="," )


# Dataframe Sierra_Morena-Doñana

bootleneck_vs_non_bootleneck_ratio_median_region(c_lp_sm_n019_ave_diversity_region, c_lp_do_n012_ave_diversity_region, deparse(substitute(c_lp_sm_n019_ave_diversity_region)),deparse(substitute(c_lp_do_n012_ave_diversity_region)))
write.table(ratio_median_c_lp_sm_n019_ave_diversity_region_c_lp_do_n012_ave_diversity_region, paste(wd_output,"ratio_median_region_c_lp_sm_n019_c_lp_do_n012.tsv",sep=""),quote = F,  row.names = F, sep =";", dec="," )

bootleneck_vs_non_bootleneck_ratio_median_region(c_lp_sm_n012_ave_diversity_region, c_lp_do_n012_ave_diversity_region, deparse(substitute(c_lp_sm_n012_ave_diversity_region)),deparse(substitute(c_lp_do_n012_ave_diversity_region)))
write.table(ratio_median_c_lp_sm_n012_ave_diversity_region_c_lp_do_n012_ave_diversity_region, paste(wd_output,"ratio_median_region_c_lp_sm_n012_c_lp_do_n012.tsv",sep=""),quote = F,  row.names = F, sep =";", dec="," )


### chr ###

# Calculo el ratio para la media:

bootleneck_vs_non_bootleneck_ratio_median_chr <- function(POP1, POP2, name_POP1, name_POP2){
  ratio_averages_POP1_POP2 <- full_join ( select ( POP1, pop, chr, total_count,  median_pairwise_ave, median_watterson_ave), select ( POP2, pop, chr,  total_count, median_pairwise_ave, median_watterson_ave), by="chr") %>% 
    mutate (ratio_median_pairwise=median_pairwise_ave.y/median_pairwise_ave.x) %>% 
    mutate (ratio_median_watterson=median_watterson_ave.y/median_watterson_ave.x)
  
  dataframename <- paste ("ratio_median", name_POP1, name_POP2,  sep="_")
  assign (dataframename, ratio_averages_POP1_POP2,.GlobalEnv)
}

# Create data frame

# Dataframe Kirov-Norway
bootleneck_vs_non_bootleneck_ratio_median_chr(c_ll_ki_n013_ave_diversity_chr,c_ll_no_n008_ave_diversity_chr, deparse(substitute(c_ll_ki_n013_ave_diversity_chr)),deparse(substitute(c_ll_no_n008_ave_diversity_chr)))
write.table(ratio_median_c_ll_ki_n013_ave_diversity_chr_c_ll_no_n008_ave_diversity_chr, paste(wd_output,"ratio_median_chr_c_ll_ki_n013_c_ll_no_n008.tsv",sep=""),quote = F,  row.names = F, sep =";", dec="," )

bootleneck_vs_non_bootleneck_ratio_median_chr(c_ll_ki_n008_ave_diversity_chr,c_ll_no_n008_ave_diversity_chr, deparse(substitute(c_ll_ki_n008_ave_diversity_chr)),deparse(substitute(c_ll_no_n008_ave_diversity_chr)))
write.table(ratio_median_c_ll_ki_n008_ave_diversity_chr_c_ll_no_n008_ave_diversity_chr, paste(wd_output,"ratio_median_chr_c_ll_ki_n008_c_ll_no_n008.tsv",sep=""),quote = F,  row.names = F, sep =";", dec="," )

# Dataframe Kirov-Poland
bootleneck_vs_non_bootleneck_ratio_median_chr(c_ll_ki_n013_ave_diversity_chr,c_ll_po_n008_ave_diversity_chr, deparse(substitute(c_ll_ki_n013_ave_diversity_chr)),deparse(substitute(c_ll_po_n008_ave_diversity_chr)))
write.table(ratio_median_c_ll_ki_n013_ave_diversity_chr_c_ll_po_n008_ave_diversity_chr, paste(wd_output,"ratio_median_chr_c_ll_ki_n013_c_ll_po_n008.tsv",sep=""),quote = F,  row.names = F, sep =";", dec="," )

bootleneck_vs_non_bootleneck_ratio_median_chr(c_ll_ki_n008_ave_diversity_chr,c_ll_po_n008_ave_diversity_chr, deparse(substitute(c_ll_ki_n008_ave_diversity_chr)),deparse(substitute(c_ll_po_n008_ave_diversity_chr)))
write.table(ratio_median_c_ll_ki_n008_ave_diversity_chr_c_ll_po_n008_ave_diversity_chr, paste(wd_output,"ratio_median_chr_c_ll_ki_n008_c_ll_po_n008.tsv",sep=""),quote = F,  row.names = F, sep =";", dec="," )

# Dataframe Sierra_Morena-Doñana
bootleneck_vs_non_bootleneck_ratio_median_chr(c_lp_sm_n019_ave_diversity_chr, c_lp_do_n012_ave_diversity_chr, deparse(substitute(c_lp_sm_n019_ave_diversity_chr)),deparse(substitute(c_lp_do_n012_ave_diversity_chr)))

write.table(ratio_median_c_lp_sm_n019_ave_diversity_chr_c_lp_do_n012_ave_diversity_chr, paste(wd_output,"ratio_median_chr_c_lp_sm_n019_c_lp_do_n012.tsv",sep=""),quote = F,  row.names = F, sep =";", dec="," )

bootleneck_vs_non_bootleneck_ratio_median_chr(c_lp_sm_n012_ave_diversity_chr, c_lp_do_n012_ave_diversity_chr, deparse(substitute(c_lp_sm_n012_ave_diversity_chr)),deparse(substitute(c_lp_do_n012_ave_diversity_chr)))
write.table(ratio_median_c_lp_sm_n012_ave_diversity_chr_c_lp_do_n012_ave_diversity_chr, paste(wd_output,"ratio_median_chr_c_lp_sm_n012_c_lp_do_n012.tsv",sep=""),quote = F,  row.names = F, sep =";", dec="," )


############## INTERACCIONES ################

# $POR AQUI VOY!!!

### FEATURE - CHR ###

# Calculo el ratio para la media:

bootleneck_vs_non_bootleneck_ratio_median_feature_chr <- function(POP1, POP2, name_POP1, name_POP2){
  ratio_averages_POP1_POP2 <- full_join ( select ( POP1, pop, feature, chr,  total_count, median_pairwise_ave, median_watterson_ave), select ( POP2, pop, feature, chr,   total_count, median_pairwise_ave, median_watterson_ave), by=c("feature","chr")) %>% 
    mutate (ratio_median_pairwise=median_pairwise_ave.y/median_pairwise_ave.x) %>% 
    mutate (ratio_median_watterson=median_watterson_ave.y/median_watterson_ave.x)
  
  dataframename <- paste ("ratio_median", name_POP1, name_POP2,  sep="_")
  assign (dataframename, ratio_averages_POP1_POP2,.GlobalEnv)
}

# Create data frame

# Dataframe Kirov-Norway

bootleneck_vs_non_bootleneck_ratio_median_feature_chr(c_ll_ki_n013_ave_diversity_feature_chr,c_ll_no_n008_ave_diversity_feature_chr, deparse(substitute(c_ll_ki_n013_ave_diversity_feature_chr)),deparse(substitute(c_ll_no_n008_ave_diversity_feature_chr)))
write.table(ratio_median_c_ll_ki_n013_ave_diversity_feature_chr_c_ll_no_n008_ave_diversity_feature_chr, paste(wd_output,"ratio_median_feature_chr_c_ll_ki_n013_c_ll_no_n008.tsv",sep=""),quote = F,  row.names = F, sep =";", dec="," )

bootleneck_vs_non_bootleneck_ratio_median_feature_chr(c_ll_ki_n008_ave_diversity_feature_chr,c_ll_no_n008_ave_diversity_feature_chr, deparse(substitute(c_ll_ki_n008_ave_diversity_feature_chr)),deparse(substitute(c_ll_no_n008_ave_diversity_feature_chr)))
write.table(ratio_median_c_ll_ki_n008_ave_diversity_feature_chr_c_ll_no_n008_ave_diversity_feature_chr, paste(wd_output,"ratio_median_feature_chr_c_ll_ki_n008_c_ll_no_n008.tsv",sep=""),quote = F,  row.names = F, sep =";", dec="," )


# Dataframe Kirov-Poland

bootleneck_vs_non_bootleneck_ratio_median_feature_chr(c_ll_ki_n013_ave_diversity_feature_chr,c_ll_po_n008_ave_diversity_feature_chr, deparse(substitute(c_ll_ki_n013_ave_diversity_feature_chr)),deparse(substitute(c_ll_po_n008_ave_diversity_feature_chr)))
write.table(ratio_median_c_ll_ki_n013_ave_diversity_feature_chr_c_ll_po_n008_ave_diversity_feature_chr, paste(wd_output,"ratio_median_feature_chr_c_ll_ki_n013_c_ll_po_n008.tsv",sep=""),quote = F,  row.names = F, sep =";", dec="," )

bootleneck_vs_non_bootleneck_ratio_median_feature_chr(c_ll_ki_n008_ave_diversity_feature_chr,c_ll_po_n008_ave_diversity_feature_chr, deparse(substitute(c_ll_ki_n008_ave_diversity_feature_chr)),deparse(substitute(c_ll_po_n008_ave_diversity_feature_chr)))
write.table(ratio_median_c_ll_ki_n008_ave_diversity_feature_chr_c_ll_po_n008_ave_diversity_feature_chr, paste(wd_output,"ratio_median_feature_chr_c_ll_ki_n008_c_ll_po_n008.tsv",sep=""),quote = F,  row.names = F, sep =";", dec="," )


# Dataframe Sierra_Morena-Doñana

bootleneck_vs_non_bootleneck_ratio_median_feature_chr(c_lp_sm_n019_ave_diversity_feature_chr, c_lp_do_n012_ave_diversity_feature_chr, deparse(substitute(c_lp_sm_n019_ave_diversity_feature_chr)),deparse(substitute(c_lp_do_n012_ave_diversity_feature_chr)))
write.table(ratio_median_c_lp_sm_n019_ave_diversity_feature_chr_c_lp_do_n012_ave_diversity_feature_chr, paste(wd_output,"ratio_median_feature_chr_c_lp_sm_n019_c_lp_do_n012.tsv",sep=""),quote = F,  row.names = F, sep =";", dec="," )

bootleneck_vs_non_bootleneck_ratio_median_feature_chr(c_lp_sm_n012_ave_diversity_feature_chr, c_lp_do_n012_ave_diversity_feature_chr, deparse(substitute(c_lp_sm_n012_ave_diversity_feature_chr)),deparse(substitute(c_lp_do_n012_ave_diversity_feature_chr)))
write.table(ratio_median_c_lp_sm_n012_ave_diversity_feature_chr_c_lp_do_n012_ave_diversity_feature_chr, paste(wd_output,"ratio_median_feature_chr_c_lp_sm_n012_c_lp_do_n012.tsv",sep=""),quote = F,  row.names = F, sep =";", dec="," )


### FEATURE - REGION ###

# Calculo el ratio para la media:

bootleneck_vs_non_bootleneck_ratio_median_feature_region <- function(POP1, POP2, name_POP1, name_POP2){
  ratio_averages_POP1_POP2 <- full_join ( select ( POP1, pop, feature, region, total_count,  median_pairwise_ave, median_watterson_ave), select ( POP2, pop, feature, region,  total_count,  median_pairwise_ave, median_watterson_ave), by=c("feature","region")) %>% 
    mutate (ratio_median_pairwise=median_pairwise_ave.y/median_pairwise_ave.x) %>% 
    mutate (ratio_median_watterson=median_watterson_ave.y/median_watterson_ave.x)
  
  dataframename <- paste ("ratio_median", name_POP1, name_POP2,  sep="_")
  assign (dataframename, ratio_averages_POP1_POP2,.GlobalEnv)
}

# Create data frame

# Dataframe Kirov-Norway

bootleneck_vs_non_bootleneck_ratio_median_feature_region(c_ll_ki_n013_ave_diversity_feature_region,c_ll_no_n008_ave_diversity_feature_region, deparse(substitute(c_ll_ki_n013_ave_diversity_feature_region)),deparse(substitute(c_ll_no_n008_ave_diversity_feature_region)))
write.table(ratio_median_c_ll_ki_n013_ave_diversity_feature_region_c_ll_no_n008_ave_diversity_feature_region, paste(wd_output,"ratio_median_feature_region_c_ll_ki_n013_c_ll_no_n008.tsv",sep=""),quote = F,  row.names = F, sep =";", dec="," )

bootleneck_vs_non_bootleneck_ratio_median_feature_region(c_ll_ki_n008_ave_diversity_feature_region,c_ll_no_n008_ave_diversity_feature_region, deparse(substitute(c_ll_ki_n008_ave_diversity_feature_region)),deparse(substitute(c_ll_no_n008_ave_diversity_feature_region)))
write.table(ratio_median_c_ll_ki_n008_ave_diversity_feature_region_c_ll_no_n008_ave_diversity_feature_region, paste(wd_output,"ratio_median_feature_region_c_ll_ki_n008_c_ll_no_n008.tsv",sep=""),quote = F,  row.names = F, sep =";", dec="," )


# Dataframe Kirov-Poland

bootleneck_vs_non_bootleneck_ratio_median_feature_region(c_ll_ki_n013_ave_diversity_feature_region,c_ll_po_n008_ave_diversity_feature_region, deparse(substitute(c_ll_ki_n013_ave_diversity_feature_region)),deparse(substitute(c_ll_po_n008_ave_diversity_feature_region)))
write.table(ratio_median_c_ll_ki_n013_ave_diversity_feature_region_c_ll_po_n008_ave_diversity_feature_region, paste(wd_output,"ratio_median_feature_region_c_ll_ki_n013_c_ll_po_n008.tsv",sep=""),quote = F,  row.names = F, sep =";", dec="," )

bootleneck_vs_non_bootleneck_ratio_median_feature_region(c_ll_ki_n008_ave_diversity_feature_region,c_ll_po_n008_ave_diversity_feature_region, deparse(substitute(c_ll_ki_n008_ave_diversity_feature_region)),deparse(substitute(c_ll_po_n008_ave_diversity_feature_region)))
write.table(ratio_median_c_ll_ki_n008_ave_diversity_feature_region_c_ll_po_n008_ave_diversity_feature_region, paste(wd_output,"ratio_median_feature_region_c_ll_ki_n008_c_ll_po_n008.tsv",sep=""),quote = F,  row.names = F, sep =";", dec="," )

# Dataframe Sierra_Morena-Doñana

bootleneck_vs_non_bootleneck_ratio_median_feature_region(c_lp_sm_n019_ave_diversity_feature_region, c_lp_do_n012_ave_diversity_feature_region, deparse(substitute(c_lp_sm_n019_ave_diversity_feature_region)),deparse(substitute(c_lp_do_n012_ave_diversity_feature_region)))
write.table(ratio_median_c_lp_sm_n019_ave_diversity_feature_region_c_lp_do_n012_ave_diversity_feature_region, paste(wd_output,"ratio_median_feature_region_c_lp_sm_n019_c_lp_do_n012.tsv",sep=""),quote = F,  row.names = F, sep =";", dec="," )

bootleneck_vs_non_bootleneck_ratio_median_feature_region(c_lp_sm_n012_ave_diversity_feature_region, c_lp_do_n012_ave_diversity_feature_region, deparse(substitute(c_lp_sm_n012_ave_diversity_feature_region)),deparse(substitute(c_lp_do_n012_ave_diversity_feature_region)))
write.table(ratio_median_c_lp_sm_n012_ave_diversity_feature_region_c_lp_do_n012_ave_diversity_feature_region, paste(wd_output,"ratio_median_feature_region_c_lp_sm_n012_c_lp_do_n012.tsv",sep=""),quote = F,  row.names = F, sep =";", dec="," )



### REGION - CHR ###

# Calculo el ratio para la media:

bootleneck_vs_non_bootleneck_ratio_median_region_chr <- function(POP1, POP2, name_POP1, name_POP2){
  ratio_averages_POP1_POP2 <- full_join ( select ( POP1, pop, region, chr,total_count,  median_pairwise_ave, median_watterson_ave), select ( POP2, pop, region, chr, total_count, median_pairwise_ave, median_watterson_ave), by=c("region","chr")) %>% 
    mutate (ratio_median_pairwise=median_pairwise_ave.y/median_pairwise_ave.x) %>% 
    mutate (ratio_median_watterson=median_watterson_ave.y/median_watterson_ave.x)
  
  dataframename <- paste ("ratio_median", name_POP1, name_POP2,  sep="_")
  assign (dataframename, ratio_averages_POP1_POP2,.GlobalEnv)
}

# Create data frame

# Dataframe Kirov-Norway

bootleneck_vs_non_bootleneck_ratio_median_region_chr(c_ll_ki_n013_ave_diversity_region_chr,c_ll_no_n008_ave_diversity_region_chr, deparse(substitute(c_ll_ki_n013_ave_diversity_region_chr)),deparse(substitute(c_ll_no_n008_ave_diversity_region_chr)))
write.table(ratio_median_c_ll_ki_n013_ave_diversity_region_chr_c_ll_no_n008_ave_diversity_region_chr, paste(wd_output,"ratio_median_region_chr_c_ll_ki_n013_c_ll_no_n008.tsv",sep=""),quote = F,  row.names = F, sep =";", dec="," )

bootleneck_vs_non_bootleneck_ratio_median_region_chr(c_ll_ki_n008_ave_diversity_region_chr,c_ll_no_n008_ave_diversity_region_chr, deparse(substitute(c_ll_ki_n008_ave_diversity_region_chr)),deparse(substitute(c_ll_no_n008_ave_diversity_region_chr)))
write.table(ratio_median_c_ll_ki_n008_ave_diversity_region_chr_c_ll_no_n008_ave_diversity_region_chr, paste(wd_output,"ratio_median_region_chr_c_ll_ki_n008_c_ll_no_n008.tsv",sep=""),quote = F,  row.names = F, sep =";", dec="," )


# Dataframe Kirov-Poland

bootleneck_vs_non_bootleneck_ratio_median_region_chr(c_ll_ki_n013_ave_diversity_region_chr,c_ll_po_n008_ave_diversity_region_chr, deparse(substitute(c_ll_ki_n013_ave_diversity_region_chr)),deparse(substitute(c_ll_po_n008_ave_diversity_region_chr)))
write.table(ratio_median_c_ll_ki_n013_ave_diversity_region_chr_c_ll_po_n008_ave_diversity_region_chr, paste(wd_output,"ratio_median_region_chr_c_ll_ki_n013_c_ll_po_n008.tsv",sep=""),quote = F,  row.names = F, sep =";", dec="," )


bootleneck_vs_non_bootleneck_ratio_median_region_chr(c_ll_ki_n008_ave_diversity_region_chr,c_ll_po_n008_ave_diversity_region_chr, deparse(substitute(c_ll_ki_n008_ave_diversity_region_chr)),deparse(substitute(c_ll_po_n008_ave_diversity_region_chr)))
write.table(ratio_median_c_ll_ki_n008_ave_diversity_region_chr_c_ll_po_n008_ave_diversity_region_chr, paste(wd_output,"ratio_median_region_chr_c_ll_ki_n008_c_ll_po_n008.tsv",sep=""),quote = F,  row.names = F, sep =";", dec="," )


# Dataframe Sierra_Morena-Doñana

bootleneck_vs_non_bootleneck_ratio_median_region_chr(c_lp_sm_n019_ave_diversity_region_chr, c_lp_do_n012_ave_diversity_region_chr, deparse(substitute(c_lp_sm_n019_ave_diversity_region_chr)),deparse(substitute(c_lp_do_n012_ave_diversity_region_chr)))
write.table(ratio_median_c_lp_sm_n019_ave_diversity_region_chr_c_lp_do_n012_ave_diversity_region_chr, paste(wd_output,"ratio_median_region_chr_c_lp_sm_n019_c_lp_do_n012.tsv",sep=""),quote = F,  row.names = F, sep =";", dec="," )

bootleneck_vs_non_bootleneck_ratio_median_region_chr(c_lp_sm_n012_ave_diversity_region_chr, c_lp_do_n012_ave_diversity_region_chr, deparse(substitute(c_lp_sm_n012_ave_diversity_region_chr)),deparse(substitute(c_lp_do_n012_ave_diversity_region_chr)))
write.table(ratio_median_c_lp_sm_n012_ave_diversity_region_chr_c_lp_do_n012_ave_diversity_region_chr, paste(wd_output,"ratio_median_region_chr_c_lp_sm_n012_c_lp_do_n012.tsv",sep=""),quote = F,  row.names = F, sep =";", dec="," )

```

#### Mean
```{r}
# Calculo el ratio para la media:

# bootleneck_vs_non_bootleneck_ratio_averages <- function(POP1, POP2, name_POP1, name_POP2){
#   ratio_averages_POP1_POP2 <- full_join ( select ( POP1, pop, feature, mean_pairwise_ave, mean_watterson_ave), select ( POP2, pop, feature, mean_pairwise_ave, mean_watterson_ave), by ="feature") %>% 
#     mutate (ratio_averages_pairwise=mean_pairwise_ave.y/mean_pairwise_ave.x) %>% 
#     mutate (ratio_averages_watterson=mean_watterson_ave.y/mean_watterson_ave.x)
#    dataframename <- paste ("ratio_averages", name_POP1, name_POP2, sep="_")
#   assign (dataframename, ratio_averages_POP1_POP2,.GlobalEnv)
# }
# 
# # Create data frame
# 
# # Dataframe Kirov-Norway
# bootleneck_vs_non_bootleneck_ratio_averages(c_ll_ki_n013_ave_diversity_feature,c_ll_no_n008_ave_diversity_feature, deparse(substitute(c_ll_ki_n013_ave_diversity_feature)),deparse(substitute(c_ll_no_n008_ave_diversity_feature)))
# 
# # Dataframe Kirov-Poland
# bootleneck_vs_non_bootleneck_ratio_averages(c_ll_ki_n013_ave_diversity_feature,c_ll_po_n008_ave_diversity_feature, deparse(substitute(c_ll_ki_n013_ave_diversity_feature)),deparse(substitute(c_ll_po_n008_ave_diversity_feature)))
# 
# # Dataframe Sierra_Morena-Doñana
# bootleneck_vs_non_bootleneck_ratio_averages(c_lp_sm_n019_ave_diversity_feature, c_lp_do_n012_ave_diversity_feature, deparse(substitute(c_lp_sm_n019_ave_diversity_feature)),deparse(substitute(c_lp_do_n012_ave_diversity_feature)))
# 
# 
# write.table(ratio_averages_c_ll_ki_n013_ave_diversity_feature_c_ll_no_n008_ave_diversity_feature, paste(wd_output,"ratio_averages_c_ll_ki_n013_c_ll_no_n008.tsv",sep=""),quote = F,  row.names = F, sep =";", dec="," )
# 
# write.table(ratio_averages_c_ll_ki_n013_ave_diversity_feature_c_ll_po_n008_ave_diversity_feature, paste(wd_output,"ratio_averages_c_ll_ki_n013_c_ll_po_n008.tsv",sep=""),quote = F,  row.names = F, sep =";", dec="," )
# 
# write.table(ratio_averages_c_lp_sm_n019_ave_diversity_feature_c_lp_do_n012_ave_diversity_feature, paste(wd_output,"ratio_averages_c_lp_sm_n019_c_lp_do_n012.tsv",sep=""),quote = F,  row.names = F, sep =";", dec="," )

```



## Modelo mixto generalizado


```{r}
library(lme4)
library(MuMIn)
library(gvlma)

data=c_ll_ki_n013_diversity_c_ll_po_n008_diversity_for_model


ki.po.model_iteraction <- lmer (watterson_ave ~ Populations * feature * region * chr + ( 1 | unique_id ), data=c_ll_ki_n013_diversity_c_ll_po_n008_diversity_for_model, REML=FALSE)

r.squaredGLMM(ki.po.model_iteraction)

summary (ki.po.model_iteraction)

# Of course you should do model simplification or model averaging in an attempt to get a parsimonious model before you do any of this, but I just wanted to flag this up.
anova_model1 <- anova(ki.po.model_iteraction)


mod <- lm(watterson_ave ~ Populations,data=c_ll_ki_n013_diversity_c_ll_po_n008_diversity_for_model)

# Do I understand correctly it is fine the outcome variable  does not need to be normally distributed itself ?
# Just like general linear models, your outcome variable does not need to be normally distributed as a univariate variable. However, LME models assume that the residuals of the model are normally distributed. So a transformation or adding weights to the model would be a way of taking care of this (and checking with diagnostic plots, of course).
par(mfrow = c(2, 2))
plot(mod)
# problema de convergencia!

gvlma::gvlma(mod)

anova(ki.po.model_nointeraction,ki.po.model_iteraction)

mean(ki.po.model_iteraction$residuals)
# https://www.ncbi.nlm.nih.gov/pubmed/21077903


# Super util para iterpretar los plots: https://stats.stackexchange.com/questions/58141/interpreting-plot-lm/65864#65864
mod.stdres = rstandard(mod)
plot(c_ll_ki_n013_diversity_c_ll_po_n008_diversity_for_model$Populations, mod.stdres, 
  ylab="Standardized Residuals") 

ggplot (data=c_ll_ki_n013_diversity, aes(watterson_ave)) +
  geom_histogram()

kk <- c_ll_ki_n013_diversity %>% filter(feature=="CDS")

shapiro.test(randomssubset$watterson_ave)

a <- seq(1, 700000, by = 150) # option 2
randomssubset <- kk[a,]


```


https://stats.stackexchange.com/questions/101274/how-to-interpret-a-qq-plot
https://www.google.com/search?client=safari&rls=en&q=how+to+correct+for+right+skewness+residuals&ie=UTF-8&oe=UTF-8
https://stats.stackexchange.com/questions/135008/how-to-deal-with-non-normally-distributed-residuals
https://stats.stackexchange.com/questions/58141/interpreting-plot-lm/65864#65864
https://stats.stackexchange.com/questions/76226/interpreting-the-residuals-vs-fitted-values-plot-for-verifying-the-assumptions
https://brownmath.com/stat/shape.htm
https://www.researchgate.net/post/how_to_reduce_skewness_and_kurtosis
https://www.researchgate.net/post/how_to_deal_with_skewness


### Assumptions of the model

Assumption 1
The regression model is linear in parameters

```{r}

```

Assumption 2
The mean of residuals is zero





Again, let’s work through this: First, the output reminds you of the model that you
fit. Then, there’s some general summary statistics such as Akaike’s Information
Criterion, the log-Likelihood etc. We won’t go into the meaning of these different
values in this tutorial because these are conceptually a little bit more involved.
Let’s focus on the output for the random effects first:


You’re being reminded of the formula of the two models that you’re comparing.
Then, you find a Chi-Square value, the associated degrees of freedom and the pvalue2.
You would report this result the following way:
“… politeness affected pitch (χ2(1)=11.62, 

## Graph boxplot interacción

No corro esto porque creo que NO es útil 


Los ratios que voy a representar son:
"ratio_pairwise_average", "ratio_watterson_average", "ratio_corrected_by_pairwise_average_intergenic_per_unit", "ratio_corrected_by_watterson_average_intergenic_per_unit"
No estoy haciendo esta vez: 
"ratio_corrected_by_pairwise_average_intergenic_weighted", "ratio_corrected_by_watterson_average_intergenic_weighted"
Primero los voy a representar en boxplot para chr, feature, y región , luego voy a hacer una tabla con la media de los ratios y su bootstrap. 

```{r}

library(rlang)
# Ratio de la diversidad población bootleneck frente non bootleneck
## BOXPLOT

DATAFRAMES=c("data_diversity_c_ll_ki_n013_diversity_c_ll_no_n008_diversity","data_diversity_c_ll_ki_n008_diversity_c_ll_no_n008_diversity", "data_diversity_c_ll_ki_n013_diversity_c_ll_po_n008_diversity","data_diversity_c_ll_ki_n008_diversity_c_ll_po_n008_diversity", "data_diversity_c_lp_sm_n019_diversity_c_lp_do_n012_diversity","data_diversity_c_lp_sm_n012_diversity_c_lp_do_n012_diversity")

INDICES=c("ratio_pairwise_average", "ratio_watterson_average")

GROUPS=c("feature", "region", "chr")


myplot <- function(data, title){
  for (INDEX in INDICES){
    for (GROUP in GROUPS){
  pdf(paste(wd_output, "boxplot_violin_plot/",title,"_",GROUP,"_",INDEX,"_boxplot.pdf", sep=""))
  plot <- ggplot(data=data, aes_string(x=GROUP, y = INDEX)) +
        geom_boxplot() +
        #scale_y_continuous(trans = 'log10') +
        theme_bw() +  #theme selection for background and lines
        theme(axis.line.x = element_line(color="black", size = 0.5),
              axis.text.x = element_text(angle = 60, vjust = 0.8, hjust = 1))+
  labs(title = paste (title,  sep=""))
  print (plot)
  dev.off()
}}}



myplot2 <- function(data, title){
  for (INDEX in INDICES){
    for (GROUP in GROUPS){
  pdf(paste(wd_output, "boxplot_violin_plot/",title,"_",GROUP,"_",INDEX,"_violin.pdf", sep=""))
  plot <- ggplot(data=data, aes_string(x=GROUP, y = INDEX)) +
        geom_violin() +
        #scale_y_continuous(trans = 'log10') +
        theme_bw() +  #theme selection for background and lines
        theme(axis.line.x = element_line(color="black", size = 0.5),
              axis.text.x = element_text(angle = 60, vjust = 0.8, hjust = 1))+
  labs(title = paste (title, sep=""))
  print (plot)
  dev.off()
}}}



for(i in DATAFRAMES){
  print(myplot(get(i), i))
  print(myplot2(get(i), i))

}


```


## Stats de los ratios.


### Average

```{r}
library("Hmisc")
library(modeest)



diversity_stats_pop <- function(diversity_df)
{
  diversity_stats_df <- 
    diversity_df %>% 
    dplyr::group_by(feature) %>% 
    dplyr::summarise(mode_ratio_pairwise_average=mlv(ratio_pairwise_average, method="mfv")[['M']],
                     mean_ratio_pairwise_average=mean(ratio_pairwise_average),
                     median_ratio_pairwise_average=median(ratio_pairwise_average),
                     q25_ratio_pairwise_average=quantile(ratio_pairwise_average,0.25),
                     q75_ratio_pairwise_average=quantile(ratio_pairwise_average,0.75),
                     q05_ratio_pairwise_average=quantile(ratio_pairwise_average,0.05),
                     q95_ratio_pairwise_average=quantile(ratio_pairwise_average,0.95),
                     IQR_ratio_pairwise_average = q75_ratio_pairwise_average - q25_ratio_pairwise_average,
                     lowerw_ratio_pairwise_average = q25_ratio_pairwise_average - 1.5* IQR_ratio_pairwise_average,
                     upperw_ratio_pairwise_average = q75_ratio_pairwise_average + 1.5* IQR_ratio_pairwise_average, 
                     
                     mean_ratio_watterson_average=mean(ratio_watterson_average),
                     mean_ratio_corrected_by_pairwise_average_intergenic_per_unit=mean(ratio_corrected_by_pairwise_average_intergenic_per_unit),
                     mean_ratio_corrected_by_watterson_average_intergenic_per_unit=mean(ratio_corrected_by_watterson_average_intergenic_per_unit)) 
  
  return(diversity_stats_df)
}

stats_data_diversity_c_ll_ki_n013_diversity_c_ll_no_n008_diversity <- diversity_stats_pop(data_diversity_c_ll_ki_n013_diversity_c_ll_no_n008_diversity)

stats_data_diversity_c_ll_ki_n008_diversity_c_ll_no_n008_diversity <- diversity_stats_pop(data_diversity_c_ll_ki_n008_diversity_c_ll_no_n008_diversity)


stats_data_diversity_c_ll_ki_n013_diversity_c_ll_po_n008_diversity <- diversity_stats_pop(data_diversity_c_ll_ki_n013_diversity_c_ll_po_n008_diversity)

stats_data_diversity_c_ll_ki_n008_diversity_c_ll_po_n008_diversity <- diversity_stats_pop(data_diversity_c_ll_ki_n008_diversity_c_ll_po_n008_diversity)


stats_data_diversity_data_diversity_c_lp_sm_n019_diversity_c_lp_do_n012_diversity <- diversity_stats_pop(data_diversity_c_lp_sm_n019_diversity_c_lp_do_n012_diversity)

stats_data_diversity_data_diversity_c_lp_sm_n012_diversity_c_lp_do_n012_diversity <- diversity_stats_pop(data_diversity_c_lp_sm_n012_diversity_c_lp_do_n012_diversity)




```


### Bootstraping


```{r}

library(plyr)
library(boot)
library(broom)
library(magrittr)
library(dplyr)

wd_output <- "/Users/marialucenaperez/Owncloud/publico/WG_diversity/ANGSD/sfs/"

# https://uoftcoders.github.io/studyGroup/lessons/r/resampling/lesson/
# Explicación del paquete boot y boot_mean
# https://www.painblogr.org/2017-10-18-purrring-through-bootstraps.html
# Explicación interesantisima sobre la integración de boot con dplyr group by. 


# Cargo la funcion de bootstrap
boot_mean <- function(original_vector, resample_vector) {
   mean(original_vector[resample_vector])
}


# Leo las tablas

# data_diversity_c_ll_ki_n013_diversity_c_ll_no_n008_diversity <- read.table(paste(wd_output,"data_diversity_c_ll_ki_n013_c_ll_no_n008.tsv", sep=""), sep=";", dec = ",", header = T)
# data_diversity_c_ll_ki_n013_diversity_c_ll_po_n008_diversity <- read.table(paste(wd_output,"data_diversity_c_ll_ki_n013_c_ll_po_n008.tsv", sep=""), sep=";", dec = ",", header = T)
# data_diversity_c_lp_sm_n019_diversity_c_lp_do_n012_diversity <- read.table(paste(wd_output,"data_diversity_c_lp_sm_n019_c_lp_do_n012.tsv", sep=""), sep=";", dec = ",", header = T)



# Defino variables para iterar

DATAFRAMES=c("data_diversity_c_ll_ki_n013_diversity_c_ll_no_n008_diversity","data_diversity_c_ll_ki_n008_diversity_c_ll_no_n008_diversity", "data_diversity_c_ll_ki_n013_diversity_c_ll_po_n008_diversity","data_diversity_c_ll_ki_n008_diversity_c_ll_po_n008_diversity", "data_diversity_c_lp_sm_n019_diversity_c_lp_do_n012_diversity","data_diversity_c_lp_sm_n012_diversity_c_lp_do_n012_diversity")
GROUPS=c("feature", "region", "chr")
ITERATION=100




for (DATAFRAME in DATAFRAMES)
{
  print (DATAFRAME)
  for (GROUP in GROUPS)
  {
    print(GROUP)
    
    # Hacemos un group_by y despues un nest que nos devuelve para cada GROUP una variable en la que está condensada la dataframe.
    
    ###################
    #1.  ratio_watterson_average
    DATAFRAME_filtered <- get(DATAFRAME) %>%
      # First I group the data by species 
      dplyr::group_by_(GROUP) %>%
      # Then I nest the dataframe
      tidyr::nest()
    
      ratio_watterson_average <- DATAFRAME_filtered %<>%
      # Aquí aplicamos el bootstrapping sobre la columna de data (la que tiene la dataframe condensada) concretamente sobre el subset del ratio X.  
      dplyr::mutate(booted = purrr::map(.x = data, # The list-column containing <S3: tibble>
                                        ~ boot::boot(data = as.numeric(.x$ratio_watterson_average), # The <S3 tibble> column being sampled
                                                     statistic = boot_mean, # The user-defined function
                                                     R = ITERATION))) %>% 
      # Del resultado de boot creamos una nueva variable con el valor t, que tiene la matriz de longitud igual al numero de iteraciones. 
      dplyr::mutate(boot_matrix = purrr::map(.x = booted, # The list-column containing <S3 bootci> objects
                                             ~ .x$t)) %>% 
      # Eliminamos la columna data (el dataframe original), y también los valores booted (que tienen todos los resultados del bootstraping). Despues desagregamos por el valor de la matriz. 
      dplyr::select(-data, -booted) %>% tidyr::unnest(boot_matrix)  
    # Cambiamos el nombre de la matriz. 
    names(ratio_watterson_average)[2] <- "values"
ratio_watterson_summary <- ratio_watterson_average  %>% 
    group_by_(.dots=GROUP) %>%
    dplyr::summarise(mean=mean(values),sd=sd(values))
    names(ratio_watterson_summary)[2] <- "mean_ratio_watterson_boot"
    names(ratio_watterson_summary)[3] <- "sd_ratio_watterson_boot"
    
    # Hago lo mismo para cada indice.
    ###################
    #2.  ratio_pairwise_average
    
    DATAFRAME_filtered <- get(DATAFRAME) %>% dplyr::group_by_(GROUP) %>% tidyr::nest()
    ratio_pairwise_average <- DATAFRAME_filtered %<>% dplyr::mutate(booted = purrr::map(.x = data, ~ boot::boot(data = as.numeric(.x$ratio_pairwise_average), statistic = boot_mean, R = ITERATION))) %>% 
      dplyr::mutate(boot_matrix = purrr::map(.x = booted, ~ .x$t)) %>% 
      dplyr::select(-data, -booted) %>% tidyr::unnest(boot_matrix)  
    names(ratio_pairwise_average)[2] <- "values"
    ratio_pairwise_average_summary <- ratio_pairwise_average  %>% 
    group_by_(.dots=GROUP) %>%
    dplyr::summarise(mean=mean(values),sd=sd(values))
    names(ratio_pairwise_average_summary)[2] <- "mean_ratio_pairwise_boot"
    names(ratio_pairwise_average_summary)[3] <- "sd_ratio_pairwise_boot"
    
    ###################
    #3.  ratio_corrected_by_pairwise_average_intergenic_per_unit
    
    DATAFRAME_filtered <- get(DATAFRAME) %>% dplyr::group_by_(GROUP) %>% tidyr::nest()
    ratio_corrected_by_pairwise_average_intergenic_per_unit <- DATAFRAME_filtered %<>% dplyr::mutate(booted = purrr::map(.x = data, ~ boot::boot(data = as.numeric(.x$ratio_corrected_by_pairwise_average_intergenic_per_unit), statistic = boot_mean, R = ITERATION))) %>% 
      dplyr::mutate(boot_matrix = purrr::map(.x = booted, ~ .x$t)) %>% 
      dplyr::select(-data, -booted) %>% tidyr::unnest(boot_matrix)  
    names(ratio_corrected_by_pairwise_average_intergenic_per_unit)[2] <- "values"
     ratio_corrected_by_pairwise_average_intergenic_per_unit_summary <- ratio_corrected_by_pairwise_average_intergenic_per_unit  %>% 
    group_by_(.dots=GROUP) %>%
    dplyr::summarise(mean=mean(values),sd=sd(values))    
     names(ratio_corrected_by_pairwise_average_intergenic_per_unit_summary)[2] <- "mean_ratio_corrected_by_pairwise_average_intergenic_per_unit_boot"
    names(ratio_corrected_by_pairwise_average_intergenic_per_unit_summary)[3] <- "sd_ratio_corrected_by_pairwise_average_intergenic_per_unit_boot"
    
    ###################
    #4.  ratio_corrected_by_watterson_average_intergenic_per_unit
    
    DATAFRAME_filtered <- get(DATAFRAME) %>% dplyr::group_by_(GROUP) %>% tidyr::nest()
    ratio_corrected_by_watterson_average_intergenic_per_unit <- DATAFRAME_filtered %<>% dplyr::mutate(booted = purrr::map(.x = data, ~ boot::boot(data = as.numeric(.x$ratio_corrected_by_watterson_average_intergenic_per_unit), statistic = boot_mean, R = ITERATION))) %>% 
      dplyr::mutate(boot_matrix = purrr::map(.x = booted, ~ .x$t)) %>% 
      dplyr::select(-data, -booted) %>% tidyr::unnest(boot_matrix)  
    names(ratio_corrected_by_watterson_average_intergenic_per_unit)[2] <- "values"
    ratio_corrected_by_watterson_average_intergenic_per_unit_summary <- ratio_corrected_by_watterson_average_intergenic_per_unit  %>% 
    group_by_(.dots=GROUP) %>%
    dplyr::summarise(mean=mean(values),sd=sd(values))  
    names(ratio_corrected_by_watterson_average_intergenic_per_unit_summary)[2] <- "mean_ratio_corrected_by_watterson_average_intergenic_per_unit_boot"
    names(ratio_corrected_by_watterson_average_intergenic_per_unit_summary)[3] <- "sd_ratio_corrected_by_watterson_average_intergenic_per_unit_boot"

    ###################
    #5.  ratio_corrected_by_pairwise_average_intergenic_weighted
    # 
    # DATAFRAME_filtered <- get(DATAFRAME) %>% dplyr::group_by_(GROUP) %>% tidyr::nest()
    # ratio_corrected_by_pairwise_average_intergenic_weighted <- DATAFRAME_filtered %<>% dplyr::mutate(booted = purrr::map(.x = data, ~ boot::boot(data = as.numeric(.x$ratio_corrected_by_pairwise_average_intergenic_weighted), statistic = boot_mean, R = ITERATION))) %>% 
    #   dplyr::mutate(boot_matrix = purrr::map(.x = booted, ~ .x$t)) %>% 
    #   dplyr::select(-data, -booted) %>% tidyr::unnest(boot_matrix)  
    # names(ratio_corrected_by_pairwise_average_intergenic_weighted)[2] <- "values"
    # ratio_corrected_by_pairwise_average_intergenic_weighted_summary <- ratio_corrected_by_pairwise_average_intergenic_weighted  %>% 
    # group_by_(.dots=GROUP) %>%
    # dplyr::summarise(mean=mean(values),sd=sd(values))  
    # names(ratio_corrected_by_pairwise_average_intergenic_weighted_summary)[2] <- "mean_ratio_corrected_by_pairwise_average_intergenic_weighted_boot"
    # names(ratio_corrected_by_pairwise_average_intergenic_weighted_summary)[3] <- "sd_ratio_corrected_by_pairwise_average_intergenic_weighted_boot"
    # 
    # ###################
    # #6.  ratio_corrected_by_watterson_average_intergenic_weighted
    # 
    # DATAFRAME_filtered <- get(DATAFRAME) %>% dplyr::group_by_(GROUP) %>% tidyr::nest()
    # ratio_corrected_by_watterson_average_intergenic_weighted <- DATAFRAME_filtered %<>% dplyr::mutate(booted = purrr::map(.x = data, ~ boot::boot(data = as.numeric(.x$ratio_corrected_by_watterson_average_intergenic_weighted), statistic = boot_mean, R = ITERATION))) %>% 
    #   dplyr::mutate(boot_matrix = purrr::map(.x = booted, ~ .x$t)) %>% 
    #   dplyr::select(-data, -booted) %>% tidyr::unnest(boot_matrix)  
    # names(ratio_corrected_by_watterson_average_intergenic_weighted)[2] <- "values"
    # ratio_corrected_by_watterson_average_intergenic_weighted_summary <- ratio_corrected_by_watterson_average_intergenic_weighted  %>% 
    # group_by_(.dots=GROUP) %>%
    # dplyr::summarise(mean=mean(values),sd=sd(values))  
    # names(ratio_corrected_by_watterson_average_intergenic_weighted_summary)[2] <- "mean_ratio_corrected_by_watterson_average_intergenic_weighted_boot"
    # names(ratio_corrected_by_watterson_average_intergenic_weighted_summary)[3] <- "sd_ratio_corrected_by_watterson_average_intergenic_weighted_boot"
    # 
    ###################
    # Agrego los datos
    
    tmp_group <- purrr::reduce(list(ratio_watterson_summary, ratio_pairwise_average_summary,
                                    ratio_corrected_by_pairwise_average_intergenic_per_unit_summary,
                                    ratio_corrected_by_watterson_average_intergenic_per_unit_summary), #,
                                    #ratio_corrected_by_pairwise_average_intergenic_weighted_summary,
                                    #ratio_corrected_by_watterson_average_intergenic_weighted_summary), 
                                    full_join, by = GROUP)
    
    tmp_group_name <- paste (DATAFRAME, GROUP, sep="_")
    
    assign (tmp_group_name, tmp_group,.GlobalEnv)    
    
    ###################
    # Borro tablas antiguas para que no haya confusión 
    
    rm (tmp_group)
    rm (tmp_group_name)
    rm (ratio_watterson_average)
    rm (ratio_pairwise_average)
    rm (ratio_corrected_by_pairwise_average_intergenic_per_unit)
    rm (ratio_corrected_by_watterson_average_intergenic_per_unit)
    #rm (ratio_corrected_by_pairwise_average_intergenic_weighted)
    #rm (ratio_corrected_by_watterson_average_intergenic_weighted)
    rm (ratio_watterson_summary)
    rm (ratio_pairwise_average_summary)
    rm (ratio_corrected_by_pairwise_average_intergenic_per_unit_summary)
    rm (ratio_corrected_by_watterson_average_intergenic_per_unit_summary)
    #rm (ratio_corrected_by_pairwise_average_intergenic_weighted_summary)
    #rm (ratio_corrected_by_watterson_average_intergenic_weighted_summary)
    
  }
}


# Ahora agrupamos por dos caracteristicas a la vez

all_combinations <- combn(GROUPS, 2)


for (DATAFRAME in DATAFRAMES) 
{
  print (DATAFRAME)
  for (ITERATION in 1:ncol(all_combinations))
  { 
    GROUP1=all_combinations[, ITERATION][1]
    GROUP2=all_combinations[, ITERATION][2]
    
    print (GROUP1)
    print (GROUP2)
    # Hacemos un group_by y despues un nest que nos devuelve para cada GROUP una variable en la que está condensada la dataframe.
    
    ###################
    #1.  ratio_watterson_average
    DATAFRAME_filtered <- get(DATAFRAME) %>%
      # First I group the data by species 
      dplyr::group_by_(GROUP1,GROUP2) %>%
      # Then I nest the dataframe
      tidyr::nest()
    
    ratio_watterson_average <- DATAFRAME_filtered %<>%
      # Aquí aplicamos el bootstrapping sobre la columna de data (la que tiene la dataframe condensada) concretamente sobre el subset del ratio X.  
      dplyr::mutate(booted = purrr::map(.x = data, # The list-column containing <S3: tibble>
                                        ~ boot::boot(data = as.numeric(.x$ratio_watterson_average), # The <S3 tibble> column being sampled
                                                     statistic = boot_mean, # The user-defined function
                                                     R = ITERATION))) %>% 
      # Del resultado de boot creamos una nueva variable con el valor t, que tiene la matriz de longitud igual al numero de iteraciones. 
      dplyr::mutate(boot_matrix = purrr::map(.x = booted, # The list-column containing <S3 bootci> objects
                                             ~ .x$t)) %>% 
      # Eliminamos la columna data (el dataframe original), y también los valores booted (que tienen todos los resultados del bootstraping). Despues desagregamos por el valor de la matriz. 
      dplyr::select(-data, -booted) %>% tidyr::unnest(boot_matrix) 
    # Cambiamos el nombre de la matriz. 
    names(ratio_watterson_average)[3] <- "values"
    ratio_watterson_summary <- ratio_watterson_average  %>% 
      group_by_(.dots=c(GROUP1,GROUP2)) %>%
      dplyr::summarise(mean=mean(as.numeric(values)),sd=sd(as.numeric(values)))
    names(ratio_watterson_summary)[3] <- "mean_ratio_watterson_boot"
    names(ratio_watterson_summary)[4] <- "sd_ratio_watterson_boot"
    
    # Hago lo mismo para cada indice.
    ###################
    #2.  ratio_pairwise_average
    
    DATAFRAME_filtered <- get(DATAFRAME) %>% dplyr::group_by_(GROUP1, GROUP2) %>% tidyr::nest()
    ratio_pairwise_average <- DATAFRAME_filtered %<>% dplyr::mutate(booted = purrr::map(.x = data, ~ boot::boot(data = as.numeric(.x$ratio_pairwise_average), statistic = boot_mean, R = ITERATION))) %>% 
      dplyr::mutate(boot_matrix = purrr::map(.x = booted, ~ .x$t)) %>% 
      dplyr::select(-data, -booted) %>% tidyr::unnest(boot_matrix)  
    names(ratio_pairwise_average)[3] <- "values"
    ratio_pairwise_average_summary <- ratio_pairwise_average  %>% 
      group_by_(.dots=c(GROUP1,GROUP2)) %>%
      dplyr::summarise(mean=mean(values),sd=sd(values))
    names(ratio_pairwise_average_summary)[3] <- "mean_ratio_pairwise_boot"
    names(ratio_pairwise_average_summary)[4] <- "sd_ratio_pairwise_boot"
    
    ###################
    #3.  ratio_corrected_by_pairwise_average_intergenic_per_unit
    
    DATAFRAME_filtered <- get(DATAFRAME) %>% dplyr::group_by_(GROUP1, GROUP2) %>% tidyr::nest()
    ratio_corrected_by_pairwise_average_intergenic_per_unit <- DATAFRAME_filtered %<>% dplyr::mutate(booted = purrr::map(.x = data, ~ boot::boot(data = as.numeric(.x$ratio_corrected_by_pairwise_average_intergenic_per_unit), statistic = boot_mean, R = ITERATION))) %>% 
      dplyr::mutate(boot_matrix = purrr::map(.x = booted, ~ .x$t)) %>% 
      dplyr::select(-data, -booted) %>% tidyr::unnest(boot_matrix)  
    names(ratio_corrected_by_pairwise_average_intergenic_per_unit)[3] <- "values"
    ratio_corrected_by_pairwise_average_intergenic_per_unit_summary <- ratio_corrected_by_pairwise_average_intergenic_per_unit  %>% 
      group_by_(.dots=c(GROUP1,GROUP2)) %>%
      dplyr::summarise(mean=mean(values),sd=sd(values))    
    names(ratio_corrected_by_pairwise_average_intergenic_per_unit_summary)[3] <- "mean_ratio_corrected_by_pairwise_average_intergenic_per_unit_boot"
    names(ratio_corrected_by_pairwise_average_intergenic_per_unit_summary)[4] <- "sd_ratio_corrected_by_pairwise_average_intergenic_per_unit_boot"
    
    ###################
    #4.  ratio_corrected_by_watterson_average_intergenic_per_unit
    
    DATAFRAME_filtered <- get(DATAFRAME) %>% dplyr::group_by_(GROUP1, GROUP2) %>% tidyr::nest()
    ratio_corrected_by_watterson_average_intergenic_per_unit <- DATAFRAME_filtered %<>% dplyr::mutate(booted = purrr::map(.x = data, ~ boot::boot(data = as.numeric(.x$ratio_corrected_by_watterson_average_intergenic_per_unit), statistic = boot_mean, R = ITERATION))) %>% 
      dplyr::mutate(boot_matrix = purrr::map(.x = booted, ~ .x$t)) %>% 
      dplyr::select(-data, -booted) %>% tidyr::unnest(boot_matrix)  
    names(ratio_corrected_by_watterson_average_intergenic_per_unit)[3] <- "values"
    ratio_corrected_by_watterson_average_intergenic_per_unit_summary <- ratio_corrected_by_watterson_average_intergenic_per_unit  %>% 
      group_by_(.dots=c(GROUP1,GROUP2)) %>%
      dplyr::summarise(mean=mean(values),sd=sd(values))  
    names(ratio_corrected_by_watterson_average_intergenic_per_unit_summary)[3] <- "mean_ratio_corrected_by_watterson_average_intergenic_per_unit_boot"
    names(ratio_corrected_by_watterson_average_intergenic_per_unit_summary)[4] <- "sd_ratio_corrected_by_watterson_average_intergenic_per_unit_boot"
    
    # ###################
    # #5.  ratio_corrected_by_pairwise_average_intergenic_weighted
    # 
    # DATAFRAME_filtered <- get(DATAFRAME) %>% dplyr::group_by_(GROUP1, GROUP2) %>% tidyr::nest()
    # ratio_corrected_by_pairwise_average_intergenic_weighted <- DATAFRAME_filtered %<>% dplyr::mutate(booted = purrr::map(.x = data, ~ boot::boot(data = as.numeric(.x$ratio_corrected_by_pairwise_average_intergenic_weighted), statistic = boot_mean, R = ITERATION))) %>% 
    #   dplyr::mutate(boot_matrix = purrr::map(.x = booted, ~ .x$t)) %>% 
    #   dplyr::select(-data, -booted) %>% tidyr::unnest(boot_matrix)  
    # names(ratio_corrected_by_pairwise_average_intergenic_weighted)[3] <- "values"
    # ratio_corrected_by_pairwise_average_intergenic_weighted_summary <- ratio_corrected_by_pairwise_average_intergenic_weighted  %>% 
    #   group_by_(.dots=c(GROUP1,GROUP2)) %>%
    #   dplyr::summarise(mean=mean(values),sd=sd(values))  
    # names(ratio_corrected_by_pairwise_average_intergenic_weighted_summary)[3] <- "mean_ratio_corrected_by_pairwise_average_intergenic_weighted_boot"
    # names(ratio_corrected_by_pairwise_average_intergenic_weighted_summary)[4] <- "sd_ratio_corrected_by_pairwise_average_intergenic_weighted_boot"
    # 
    # ###################
    # #6.  ratio_corrected_by_watterson_average_intergenic_weighted
    # 
    # DATAFRAME_filtered <- get(DATAFRAME) %>% dplyr::group_by_(GROUP1, GROUP2) %>% tidyr::nest()
    # ratio_corrected_by_watterson_average_intergenic_weighted <- DATAFRAME_filtered %<>% dplyr::mutate(booted = purrr::map(.x = data, ~ boot::boot(data = as.numeric(.x$ratio_corrected_by_watterson_average_intergenic_weighted), statistic = boot_mean, R = ITERATION))) %>% 
    #   dplyr::mutate(boot_matrix = purrr::map(.x = booted, ~ .x$t)) %>% 
    #   dplyr::select(-data, -booted) %>% tidyr::unnest(boot_matrix)  
    # names(ratio_corrected_by_watterson_average_intergenic_weighted)[3] <- "values"
    # ratio_corrected_by_watterson_average_intergenic_weighted_summary <- ratio_corrected_by_watterson_average_intergenic_weighted  %>% 
    #   group_by_(.dots=c(GROUP1,GROUP2)) %>%
    #   dplyr::summarise(mean=mean(values),sd=sd(values))  
    # names(ratio_corrected_by_watterson_average_intergenic_weighted_summary)[3] <- "mean_ratio_corrected_by_watterson_average_intergenic_weighted_boot"
    # names(ratio_corrected_by_watterson_average_intergenic_weighted_summary)[4] <- "sd_ratio_corrected_by_watterson_average_intergenic_weighted_boot"
    # 
    # ###################
    # Agrego los datos
    
    tmp_group <- purrr::reduce(list(ratio_watterson_summary, 
                                    ratio_pairwise_average_summary, 
                                    ratio_corrected_by_pairwise_average_intergenic_per_unit_summary, 
                                    ratio_corrected_by_watterson_average_intergenic_per_unit_summary),#, 
                                    #ratio_corrected_by_pairwise_average_intergenic_weighted_summary, 
                                    #ratio_corrected_by_watterson_average_intergenic_weighted_summary), 
                               full_join, by = c(GROUP1,GROUP2))
    
    tmp_group_name <- paste (DATAFRAME, GROUP1, GROUP2, sep="_")
    
    assign (tmp_group_name, tmp_group,.GlobalEnv)    
    
    ###################
    # Borro tablas antiguas para que no haya confusión 
    
    rm (tmp_group)
    rm (tmp_group_name)
    rm (ratio_watterson_average)
    rm (ratio_pairwise_average)
    rm (ratio_corrected_by_pairwise_average_intergenic_per_unit)
    rm (ratio_corrected_by_watterson_average_intergenic_per_unit)
    #rm (ratio_corrected_by_pairwise_average_intergenic_weighted)
    #rm (ratio_corrected_by_watterson_average_intergenic_weighted)
    rm (ratio_watterson_summary)
    rm (ratio_pairwise_average_summary)
    rm (ratio_corrected_by_pairwise_average_intergenic_per_unit_summary)
    rm (ratio_corrected_by_watterson_average_intergenic_per_unit_summary)
    #rm (ratio_corrected_by_pairwise_average_intergenic_weighted_summary)
    #rm (ratio_corrected_by_watterson_average_intergenic_weighted_summary)
    
    
  }
}



   
```


   

## Representation average data in the 2 pop. 

```{r}

DATAFRAMES=c("data_diversity_c_ll_ki_n013_diversity_c_ll_no_n008_diversity","data_diversity_c_ll_ki_n008_diversity_c_ll_no_n008_diversity", "data_diversity_c_ll_ki_n013_diversity_c_ll_po_n008_diversity","data_diversity_c_ll_ki_n008_diversity_c_ll_po_n008_diversity", "data_diversity_c_lp_sm_n019_diversity_c_lp_do_n012_diversity","data_diversity_c_lp_sm_n012_diversity_c_lp_do_n012_diversity")

for (DATAFRAME in DATAFRAMES) 
{
 
    ggplot(data=get(DATAFRAME), aes(x=watterson_ave.y ,y=watterson_ave.x, fill=feature, colour=feature)) +
       geom_point(alpha = 1/100) +
    geom_smooth()+
    geom_abline(intercept = 0) +
   facet_wrap(~feature, scales = "free" )
   ggsave (paste(wd_output,DATAFRAME,"_relation_by_feature_watterson_scalefree.pdf", sep=""))
     
   ggplot(data=get(DATAFRAME), aes(x=watterson_ave.y ,y=watterson_ave.x, fill=feature, colour=feature)) +
       geom_point(alpha = 1/100) +
    geom_smooth()+
    geom_abline(intercept = 0) +
   facet_wrap(~feature)
   ggsave (paste(wd_output,DATAFRAME,"_relation_by_feature_watterson.pdf", sep=""))
}      
        
for (DATAFRAME in DATAFRAMES) 
{
 ggplot(data=get(DATAFRAME), aes(x=pairwise_ave.y ,y=pairwise_ave.x, fill=feature, colour=feature)) +
        geom_point(alpha = 1/100) +
        geom_smooth()+
    geom_abline(intercept = 0) +
   facet_wrap(~feature, scales = "free" )
   ggsave (paste(wd_output,DATAFRAME,"_relation_by_feature_pairwise_scalefree.pdf", sep=""))

   ggplot(data=get(DATAFRAME), aes(x=pairwise_ave.y ,y=pairwise_ave.x, fill=feature, colour=feature)) +
        geom_point(alpha = 1/100) +
        geom_smooth()+
    geom_abline(intercept = 0) +
   facet_wrap(~feature )
   ggsave (paste(wd_output,DATAFRAME,"_relation_by_feature_pairwise.pdf", sep=""))
}       
   
```



## Representation plot boot data for each group

```{r}

library(rlang)

DATAFRAMES=c(
"data_diversity_c_ll_ki_n013_diversity_c_ll_no_n008_diversity_feature", "data_diversity_c_ll_ki_n013_diversity_c_ll_no_n008_diversity_region", "data_diversity_c_ll_ki_n013_diversity_c_ll_no_n008_diversity_chr", 
"data_diversity_c_ll_ki_n008_diversity_c_ll_no_n008_diversity_feature", "data_diversity_c_ll_ki_n008_diversity_c_ll_no_n008_diversity_region", "data_diversity_c_ll_ki_n008_diversity_c_ll_no_n008_diversity_chr", 

"data_diversity_c_ll_ki_n013_diversity_c_ll_po_n008_diversity_feature", "data_diversity_c_ll_ki_n013_diversity_c_ll_po_n008_diversity_region", "data_diversity_c_ll_ki_n013_diversity_c_ll_po_n008_diversity_chr", 
"data_diversity_c_ll_ki_n008_diversity_c_ll_po_n008_diversity_feature", "data_diversity_c_ll_ki_n008_diversity_c_ll_po_n008_diversity_region", "data_diversity_c_ll_ki_n008_diversity_c_ll_po_n008_diversity_chr", 

"data_diversity_c_lp_sm_n019_diversity_c_lp_do_n012_diversity_feature", "data_diversity_c_lp_sm_n019_diversity_c_lp_do_n012_diversity_region", "data_diversity_c_lp_sm_n019_diversity_c_lp_do_n012_diversity_chr",
"data_diversity_c_lp_sm_n012_diversity_c_lp_do_n012_diversity_feature", "data_diversity_c_lp_sm_n012_diversity_c_lp_do_n012_diversity_region", "data_diversity_c_lp_sm_n012_diversity_c_lp_do_n012_diversity_chr")

INDICES=c("ratio_watterson_boot", "ratio_pairwise_boot", "ratio_corrected_by_pairwise_average_intergenic_per_unit_boot", "ratio_corrected_by_watterson_average_intergenic_per_unit_boot")

# "ratio_corrected_by_pairwise_average_intergenic_weighted_boot", "ratio_corrected_by_watterson_average_intergenic_weighted_boot"



myplot3 <- function(data,title){
  for (INDEX in INDICES)
  {
    print (INDEX)
  pdf(paste(wd_output,title,"_",INDEX,".pdf", sep=""))
  plot <- ggplot(data=data, aes_string(x=colnames(data)[1],y=paste("mean_",INDEX,sep=""))) +
        geom_point(color="black") +
        geom_errorbar(aes_string(ymin=paste("mean_",INDEX,"-sd_",INDEX,sep=""), ymax=paste("mean_",INDEX,"+sd_",INDEX,sep=""))) +
        #scale_y_continuous(trans = 'log10') +
        theme_bw() +  #theme selection for background and lines
        theme(axis.line.x = element_line(color="black", size = 0.5),
              axis.text.x = element_text(angle = 60, vjust = 0.8, hjust = 1))
  # labs(title = paste (title, "scale_y_transformed", sep=""))
  print (plot)
  dev.off() 
    
  }
  
  
}


for(i in DATAFRAMES){
  print(myplot3(get(i), i))
}



$$$
```

## Heatmap for two groups

```{r}
library( gplots)

# ls(pattern="ratio_median")  y selecciono los que tienen comparación a pares
DATAFRAMES=c("ratio_median_c_ll_ki_n013_ave_diversity_feature_chr_c_ll_no_n008_ave_diversity_feature_chr", "ratio_median_c_ll_ki_n013_ave_diversity_feature_chr_c_ll_po_n008_ave_diversity_feature_chr", "ratio_median_c_ll_ki_n013_ave_diversity_feature_region_c_ll_no_n008_ave_diversity_feature_region", "ratio_median_c_ll_ki_n013_ave_diversity_feature_region_c_ll_po_n008_ave_diversity_feature_region", "ratio_median_c_ll_ki_n013_ave_diversity_region_chr_c_ll_no_n008_ave_diversity_region_chr", "ratio_median_c_ll_ki_n013_ave_diversity_region_chr_c_ll_po_n008_ave_diversity_region_chr", "ratio_median_c_lp_sm_n019_ave_diversity_feature_chr_c_lp_do_n012_ave_diversity_feature_chr", "ratio_median_c_lp_sm_n019_ave_diversity_feature_region_c_lp_do_n012_ave_diversity_feature_region", "ratio_median_c_lp_sm_n019_ave_diversity_region_chr_c_lp_do_n012_ave_diversity_region_chr")
INDICE=c("ratio_median_pairwise", "ratio_median_watterson")
#########################################################
### A) Installing and loading required packages
#########################################################

if (!require("gplots")) {
   install.packages("gplots", dependencies = TRUE)
   library(gplots)

if (!require("RColorBrewer")) {
   install.packages("RColorBrewer", dependencies = TRUE)
   library(RColorBrewer)
   }
library(eply)

#########################################################
### B) Reading in data and transform it into matrix format
#########################################################
for (DATA in DATAFRAMES)
{  
  for (INDEX in INDICE)
  {

    DATAFRAME=as.data.frame(get(DATA))
    
    POP1=as.character(DATAFRAME$pop.x[1])
    POP2=as.character(DATAFRAME$pop.y[1])
    
    temp.data.frame.diversity <- reshape2::acast(DATAFRAME, list(names(DATAFRAME)[2], names(DATAFRAME)[3]), value.var=INDEX)
    
    temp.data.frame.counts <- reshape2::acast(DATAFRAME, list(names(DATAFRAME)[2], names(DATAFRAME)[3]), value.var="total_count.x")

   
   
#########################################################
### C) Customizing and plotting the heat map
#########################################################

# creates a own color palette from red to green
#my_palette <- colorRampPalette(c("red", "yellow", "green"))(n = 299)

# (optional) defines the color breaks manually for a "skewed" color transition
#col_breaks = c(seq(-1,0,length=100),  # for red
#  seq(0.01,0.8,length=100),           # for yellow
#  seq(0.81,1,length=100))             # for green

# creates a 5 x 5 inch image
png(paste(wd_output,"heatmap_",names(DATAFRAME)[2],"_",names(DATAFRAME)[3],"_",INDEX,"_",POP1,"_",POP2,".png", sep=""),    # create PNG for the heat map        
  width = 5*300,        # 5 x 300 pixels
  height = 5*300,
  res = 300,            # 300 pixels per inch
  pointsize = 8)        # smaller font size

heatmap.2(temp.data.frame.diversity,
  cellnote = temp.data.frame.counts,  # same data set for cell labels
  main = paste(INDEX,POP1,POP2, sep=" "), # heat map title
  notecol="black",      # change font color of cell labels to black
  density.info="none",  # turns off density plot inside color legend
  trace="none",         # turns off trace lines inside the heat map
  margins =c(12,9),     # widens margins around plot
  # col=my_palette,       # use on color palette defined earlier
  # breaks=col_breaks,    # enable color transition at specified limits
  dendrogram="none",     # only draw a row dendrogram
  Colv="NA",             # turn off column clustering
  sepwidth=c(0.01,0.01),
  sepcolor="black",
  colsep=1:ncol(temp.data.frame.diversity),
  rowsep=1:nrow(temp.data.frame.diversity),
  cexRow=0.075)            

dev.off()               # close the PNG device

  }
}  
```



# OJO FILTERS

```{r}
# grep "chrF1" c_ll

```













# ------------------------------------------
# OLD (before May 2018)

Correcciones diversity per feature!!

```{bash}

# 8/9/2017: Haciendo un plot me he dado cuenta que hay algunas filas duplicadas en el archivo original .gff3, corresponden a UTR. 
# En la carpeta /home/mlucena/grupolince/notation, están especificadas cuales son. He corregido el gff3, pero como no voy a volver a correr mis archivos para sacar los números, voy a corregir todos los originales para tenerlo todo bien desde el principio. 

# Check:

# sort c_ll_ki_n013.per.unit.averages.tsv | uniq -c | sort -nr | head

# 2 lp23.s36682     338697  338929  232     -1      3UTR    +       .       LYPA23C000008   LYPA23C000008   1.4564973179*10^-06     4.5837356691*10^-06     4.4811971320*10^-07     1.3890498033*10^-06     -3.1562808160*1
# 2 lp23.s36682     328523  328962  439     75      5UTR    -       .       LYPA23C000003   LYPA23C000003   5.0803158132*10^-06     1.2267826914*10^-05     1.8178262696*10^-06     4.0698820431*10^-06     -3.9712504595*1
# 2 lp23.s36682     279677  279811  134     -1      3UTR    +       .       LYPA23C000015   LYPA23C000015   1.0437203878*10^-06     4.1265547119*10^-06     3.1206706874*10^-07     1.2333416354*10^-06     -2.5288512495*1
# 2 lp23.s36682     100823  101610  787     82      3UTR    +       .       LYPA23C000004   LYPA23C000004   3.7669060166*10^-04     9.8624719772*10^-03     3.9965744098*10^-04     1.0556168131*10^-02     3.3103961265*10
# 2 lp23.s26402     92633   95884   3251    166     3UTR    +       .       LYPA23C000017   LYPA23C000017   9.8348133084*10^-06     3.1577310510*10^-04     4.3617798050*10^-06     1.5841654621*10^-04     -3.4633370151*1
# 2 lp23.s10719     96754   96794   40      -1      5UTR    -       .       LYPA23C000041   LYPA23C000041   3.0378406270*10^-06     1.0043475395*10^-06     1.0387353248*10^-06     3.9400597979*10^-07     -3.2716817179*1
# 2 lp23.s10719     156260  156378  118     96      5UTR    -       .       LYPA23C000045   LYPA23C000045   1.3834272997*10^-05     2.2261745529*10^-06     6.3570507930*10^-06     1.2906641862*10^-06     -7.9734277877*1
# 1 scaffold        start   end     length  NAs     feature strandness      frame   id_gene id      watterson_ave   watterson_sd    pairwise_ave    pairwise_sd     tajimaD informative_sites       pop     specie
# 1 lp23.s41700     1       200     199     199     intergenic      -       .       intergenic_region_65795 intergenic_region_65795         0       c_ll_ki_n013    c_ll
# 1 lp23.s41699     1       200     199     199     intergenic      -       .       intergenic_region_65794 intergenic_region_65794         0       c_ll_ki_n013    c_ll


# 491637 c_ll_ki_n013.per.unit.averages.tsv

# Efectivamente aquí también está duplicados, así que corro el loop para corregirlo:

for i in  *.per.unit.averages.tsv
do
echo $i
POP=$(echo ${i} | cut -d "." -f 1 )
SPECIE=$(echo $POP | cut -d"_" -f 1-2)
awk '!seen[$0]++' $i > ${i/.tsv/.corrected.tsv} # remove duplicated rows.
mv ${i/.tsv/.corrected.tsv} $i
done



# Create a unique file with all the information

awk 'FNR==1 && NR!=1{next;}{print}' c_lp*averages.tsv > c_lp_do_n012-c_lp_sm_n019.per.unit.averages.per_specie.tsv
awk 'FNR==1 && NR!=1{next;}{print}' c_ll*averages.tsv > c_ll_ki_n013-c_ll_no_n008-c_ll_po_n008.per.unit.averages.per_specie.tsv


# Compruebo que ha funcionado:

# sort c_ll_ki_n013.per.unit.averages.tsv | uniq -c | sort -nr | head
# 1 scaffold	start	end	length	NAs	feature	strandness	frame	id_gene	id	watterson_ave	watterson_sd	pairwise_ave	pairwise_sd	tajimaD	informative_sites	pop	specie
# 1 lp23.s41700	1	200	
      
# Perfecto!!
      
```


OJO!!

Calcular luego en R medidas pareadas!!! cada unidad con su equivalente en las tres poblaciones!


```{bash}
nota


wc -l c_ll_ki_n013-c_ll_no_n008-c_ll_po_n008.per.unit.averages.per_specie.tsv
1474888 c_ll_ki_n013-c_ll_no_n008-c_ll_po_n008.per.unit.averages.per_specie.tsv


awk '{if ($18 == "") print $0}' c_ll_ki_n013-c_ll_no_n008-c_ll_po_n008.per.unit.averages.per_specie.tsv  | wc -l
134260

# Tengo 134260 lineas que no tienen todos los campos, eso supone un 9,1% de total. 
# ¿A cuantas les faltan todos los campos y por tanto no ha perjudicados los cálculos? (A todas las que tengas informative sites ($11), igual a cero)

awk '{if ($18 == "") print $0}' c_ll_ki_n013-c_ll_no_n008-c_ll_po_n008.per.unit.averages.per_specie.tsv  | awk '{if ($11=="0") print $0}' | wc -l
133589

# Por tanto, de las 134260, a 133589 le faltan todos los datos, por tanto sólo hay 671 lineas con algunos valores sí y otros no. 

133589-134260=671

awk '{if ($18 == "") print $0}' c_ll_ki_n013-c_ll_no_n008-c_ll_po_n008.per.unit.averages.per_specie.tsv  | awk '{if ($11!="0") print $0}' | wc -l
671



# De estos, parece que hay muchos con sitios informativos=1, por lo que asumo que no puede calcular las tajimaD. 

awk '{if ($18 == "") print $0}' c_ll_ki_n013-c_ll_no_n008-c_ll_po_n008.per.unit.averages.per_specie.tsv  | awk '{if ($11!="0") print $0}' | head
lp23.s00005	1472211	1472211	0	-1	CDS	-	0	LYPA23C006624	LYPA23C006624P4_1_1	9.1722861064*10^-06	0.0000000000*10^00	3.7772361236*10^-06	0.0000000000*10^00		1	c_ll_ki_n013	c_ll
lp23.s00006	42798	43028	230	229	intron	+	0	LYPA23C020681	LYPA23C020681T1_intron_13	7.9066451013*10^-06	0.0000000000*10^00	3.1403854839*10^-06	0.0000000000*10^00		1	c_ll_ki_n013	c_ll
lp23.s00010	215823	215849	26	25	lncRNA	+

# Concretamente encontramos,

 awk '{if ($18 == "") print $0}' c_ll_ki_n013-c_ll_no_n008-c_ll_po_n008.per.unit.averages.per_specie.tsv  | awk '{if ($11!="0") print $0}' | awk '{if ($15="1") print $0}' | wc -l
671

# Todo cuadra. 

# 671 supoone un 0.045 del total, así que yo creo que no influye o no mucho en los resultados que tenog. En todo caso, como en algunos casos la coumna de informative site 01 está donde watterson debería estar, ahí si pero en el resto no. 


```




Este script nos devuelve una gráfica por cada población. Hay un segundo script que devuelve una gráfica para lynx lynx y otra para lynx pardinus. 

```{r}

library ("ggplot2")
library ("plyr")
library ("RColorBrewer")
library ("scales")
library ("magrittr")

# wd <- "/Users/marialucenaperez/Owncloud/publico/WG_diversity/ANGSD/sfs/diversity_per_unit"
wd <- "/home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/diversity_per_unit"
# wd <- "/Users/marialucenaperez/Desktop/Diversity_per_unit"

finsdiversity = list.files(path = wd,pattern="*per.unit.averages.tsv$")
INFORMATIVE_SITES=1
PERCENTAGE_COVERED=0.5

for (i in 1:length(finsdiversity))
{
  infile <- finsdiversity[i]
  name_diversity <- unlist(strsplit(finsdiversity[i], "[.]"))

#### Para que almacene varias dataframe con el nombre correcto
#  dataframename <- paste("data_diversity", name_diversity[1], sep ='_')
#  assign(dataframename, read.csv (paste(wd,infile, sep="/"), header = T, sep = '\t',stringsAsFactors = FALSE,  row.names=NULL, na.strings = "", dec=".", colClasses=c("watterson_ave"="character")))

dataframe <- read.csv (paste(wd,infile, sep="/"), header = T, sep = '\t',stringsAsFactors = FALSE,  row.names=NULL, na.strings = "", dec=".")

dataframe = dataframe[-1,]
dataframe$watterson_ave <- as.numeric(gsub("\\*10\\^","e",dataframe$watterson_ave))
dataframe$pairwise_ave <- as.numeric(gsub("\\*10\\^","e",dataframe$pairwise_ave))
dataframe$tajimaD <- as.numeric(gsub("\\*10\\^","e",dataframe$tajimaD))
#dataframe  <- dataframe[complete.cases(dataframe),] ## --> No se que pasa que la columna de specie sale NA

########################################################
# Filtering
dataframe <- dataframe %>% 
  mutate(percentage_covered=ifelse(NAs>0, (as.numeric(length)-as.numeric(NAs))/as.numeric(length), 1)) %>% dplyr::filter(percentage_covered>PERCENTAGE_COVERED) %>% dplyr::filter(informative_sites>INFORMATIVE_SITES)
name_diversity[1] <- paste(name_diversity[1],".percentagecovered",PERCENTAGE_COVERED,"_mininformativesites",INFORMATIVE_SITES, sep="")
########################################################

df.pm <- ddply(dataframe, "feature", summarise, weightedmean=weighted.mean(pairwise_ave, informative_sites))

df.wm <- ddply(dataframe, "feature", summarise, weightedmean=weighted.mean(watterson_ave, informative_sites))


# pdf(file = paste(wd,"/Diversity_by_feature_pairwise",name_diversity[1],".pdf", sep = ""))
ggplot(data = dataframe, aes(x=feature, y = pairwise_ave)) + 
  geom_boxplot()+
  geom_point(data=df.pm,aes(x=feature, y = weightedmean),shape = 23, 
             size = 3, inherit.aes=FALSE) +
  # scale_y_continuous(limits = c(-20,-3), expand =c(0,0), labels = comma) +
  facet_grid(.~feature, scales="free") +
  scale_y_continuous(trans = 'log10') +
  scale_x_discrete("Genomic feature") +
  xlab(label = "Genomic feature") + #x title
  ylab(label = "Pairwise average") + # y title
  theme_bw() +  #theme selection for background and lines
  # scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  ggtitle(paste ("Diversity by feature",name_diversity[1], sep = " " )) +
  theme(axis.line.x = element_line(color="black", size = 0.5),
        axis.line.y = element_line(color="black", size = 0.5),
        axis.text.x = element_text(angle = 60, vjust = 0.8, hjust = 1),
        axis.text.y = element_text(vjust = 0.2, hjust = 0.2),
        axis.title.y = element_text(margin=margin(r=0.3, unit="cm")),
        axis.title.x = element_text(margin=margin(t=0.5, unit="cm")),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.border = element_blank(),
        panel.background = element_blank(),
        strip.background = element_blank(),
        strip.text.x = element_blank())
ggsave (file = paste(wd,"/",name_diversity[1],".diversity_feature_pairwise.pdf", sep = ""), device = "pdf")
# dev.off()



# pdf(file = paste(wd,"/Diversity_by_feature_watterson",name_diversity[1],".pdf", sep = ""))

ggplot(data = dataframe, aes(x=feature, y = watterson_ave)) + 
  geom_boxplot()+
  geom_point(data=df.wm,aes(x=feature, y = weightedmean),shape = 23, 
             size = 3, inherit.aes=FALSE) +
  # scale_y_continuous(limits = c(-20,-3), expand =c(0,0), labels = comma) +
  facet_grid(.~feature, scales="free") +
  scale_y_continuous(trans = 'log10') +
  scale_x_discrete("Genomic feature") +
  xlab(label = "Genomic feature") + #x title
  ylab(label = "Watterson average") + # y title
  theme_bw() +  #theme selection for background and lines
  # scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  ggtitle(paste ("Diversity by feature",name_diversity[1], sep = " " )) +
  theme(axis.line.x = element_line(color="black", size = 0.5),
        axis.line.y = element_line(color="black", size = 0.5),
        axis.text.x = element_text(angle = 60, vjust = 0.8, hjust = 1),
        axis.text.y = element_text(vjust = 0.2, hjust = 0.2),
        axis.title.y = element_text(margin=margin(r=0.3, unit="cm")),
        axis.title.x = element_text(margin=margin(t=0.5, unit="cm")),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.border = element_blank(),
        panel.background = element_blank(),
        strip.background = element_blank(),
        strip.text.x = element_blank())

ggsave(filename = paste(wd,"/", name_diversity[1],".diversity_feature_watterson.pdf", sep = ""), device = "pdf")


ggplot(data = dataframe, aes(x=feature, y = tajimaD)) + 
  geom_boxplot()+
  # scale_y_continuous(limits = c(-20,-3), expand =c(0,0), labels = comma) +
  facet_grid(.~feature, scales="free") +
  #  scale_y_continuous(trans = 'log10') +
  scale_x_discrete("Genomic feature") +
  xlab(label = "Genomic feature") + #x title
  ylab(label = "Watterson average") + # y title
  theme_bw() +  #theme selection for background and lines
  # scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  ggtitle(paste ("Diversity by feature",name_diversity[1], sep = " " )) +
  theme(axis.line.x = element_line(color="black", size = 0.5),
        axis.line.y = element_line(color="black", size = 0.5),
        axis.text.x = element_text(angle = 60, vjust = 0.8, hjust = 1),
        axis.text.y = element_text(vjust = 0.2, hjust = 0.2),
        axis.title.y = element_text(margin=margin(r=0.3, unit="cm")),
        axis.title.x = element_text(margin=margin(t=0.5, unit="cm")),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.border = element_blank(),
        panel.background = element_blank(),
        strip.background = element_blank(),
        strip.text.x = element_blank())
ggsave(filename = paste(wd,"/",name_diversity[1],".percentagecovered",PERCENTAGE_COVERED,"_mininformativesites",INFORMATIVE_SITES,".diversity_feature_tajimaD.pdf", sep = ""), device = "pdf")


# Exploratory plots

jpeg(paste(wd,"/",name_diversity[1],".plot1.jpg", sep=""))
plot (dataframe$informative_sites, dataframe$tajimaD)
dev.off()

jpeg(paste(wd,"/",name_diversity[1],".plot2.jpg", sep=""))
plot (dataframe$pairwise_ave, dataframe$watterson_ave)
dev.off()

jpeg(paste(wd,"/",name_diversity[1],".plot3.jpg", sep=""))
plot (dataframe$pairwise_ave, dataframe$tajimaD)
dev.off()

jpeg(paste(wd,"/",name_diversity[1],"plot4.jpg", sep=""))
plot (dataframe$watterson_ave, dataframe$tajimaD)
dev.off()

jpeg(paste(wd,"/",name_diversity[1],"plot5.jpg", sep=""))
plot (dataframe$informative_sites, dataframe$pairwise_ave)
dev.off()

jpeg(paste(wd,"/",name_diversity[1],"plot6.jpg", sep=""))
plot (dataframe$informative_sites, dataframe$watterson_ave)
dev.off()


}


# Cosas útiles outliers.shape=NA.

```


Comparación por pares unidad


```{r}

library ("ggplot2")
library ("plyr")
library ("RColorBrewer")
library ("scales")
library ("magrittr")
library(dplyr)

# wd <- "/Users/marialucenaperez/Owncloud/publico/WG_diversity/ANGSD/sfs/diversity_per_unit"
wd <- "/Users/marialucenaperez/Owncloud/publico/WG_diversity/ANGSD/sfs/diversity_per_unit"
# wd <- "/Users/marialucenaperez/Desktop/Diversity_per_region"

finsdiversity = list.files(path = wd,pattern="*.per.unit.averages.tsv$")

for (i in 1:length(finsdiversity))
{
  infile <- finsdiversity[i]
  name_diversity <- unlist(strsplit(finsdiversity[i], "[.]"))
dataframe <- read.csv (paste(wd,infile, sep="/"), header = T, sep = '\t',stringsAsFactors = FALSE,  row.names=NULL, na.strings = "", dec=".")

dataframe = dataframe[-1,]
dataframe$watterson_ave <- as.numeric(gsub("\\*10\\^","e",dataframe$watterson_ave))
dataframe$pairwise_ave <- as.numeric(gsub("\\*10\\^","e",dataframe$pairwise_ave))
dataframe$tajimaD <- as.numeric(gsub("\\*10\\^","e",dataframe$tajimaD))

#### Para que almacene varias dataframe con el nombre correcto
  dataframename <- paste("data_diversity", name_diversity[1], sep ='_')
  assign(dataframename, dataframe)
  
  

}


pair_comparison <- full_join (data_diversity_c_ll_ki_n013, data_diversity_c_ll_no_n008, by=c("row.names"="row.names","scaffold"="scaffold","start"="start", "end"="end",  "feature"="feature", "strandness"="strandness", "frame"="frame", "id_gene"="id_gene", "id"="id")  )

colourCount = length(unique(pair_comparison$feature))
getPalette = colorRampPalette(brewer.pal(9, "Paired"))


ggplot (filter(pair_comparison), aes(x= watterson_ave.x, y=watterson_ave.y , fill=feature, colour=feature)) +
  geom_point(alpha=1/100) +
  geom_smooth(method = "lm", se = FALSE) +
  geom_abline(intercept = 0) +
  scale_fill_manual(values = getPalette(colourCount))+
  scale_colour_manual(values = getPalette(colourCount)) +
   theme(legend.position="bottom") +
  guides(fill=guide_legend(nrow=2))

```











OLD (before 18/05/2018)

Global diversity per feature

I made a gff3 file with all the characteristics that we want to evaluate. 
Now that I have the file to run over the calculus per unit. To do it in a efficient way I will split this file into as many files as scaffolds. I will do the same with my thetas file. 
That way I will just have to search over the file that contains the scaffold, and bedtools will be faster.  

ESTE ES EL ARCHIVO (2018/04/03): 

/GRUPOS/grupolince/Lyp_annotation_Apr14_final/LYPA23C.CDS.GENE_promoters.GENE_introns.UTRs.ncRNA.lncRNA.lncRNA_introns.lncRNA_promoters.intergenic.UCR.nr.gff3

First I separate transformedThetas per scaffold. 

```{bash}

cd /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/diversity_per_unit

NEW_FOLDER=("c_ll_ki_separated_by_scaffold" "c_ll_po_separated_by_scaffold" "c_ll_no_separated_by_scaffold" "c_lp_do_separated_by_scaffold" "c_lp_sm_separated_by_scaffold")

for FOLDER in "${NEW_FOLDER[@]}"
do
echo "---------------------------------------------------$FOLDER---------------------------------------------------"
mkdir $FOLDER;
done
 
 
POPS=("c_ll_no_n008" "c_ll_po_n008"  "c_lp_do_n012" "c_lp_sm_n019" "c_ll_ki_n013")

########################


for POP in "${POPS[@]}"
do
echo $POP
cd /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/
mv "$POP".transformedThetas "$POP"_separated_by_scaffold/
cd  /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/"$POP"_separated_by_scaffold
# Create multiple files base on one column: transformedThetas:
awk '{print >> $1; close($1)}' "$POP".transformedThetas
rm scaffold # esto borra la primera linea que forma un archivo llamado scaffold.
# Rename the files: transformedThetas

for file in lp23*
do
echo $file
mv $file ${file/lp23/"$POP".transformedThetas_lp23}
done
done

```


Diversity per feature.

14/05/2018 --> Este script es válido, el problema es que en su momento tiraba de un archivo de notaciónq que no estaba bien 100% (ya si coge el archivo adecuado) y por eso tuve que hacer todas las modificaciones que explico abajo, pero funciona bien. 

De hecho, para las poblaciones de kirov, noruega, polonia y doñana, los archivos de notación que aparecían en este script y con el que se lanzaron estas poblaciones eran otros. Estos archivos era:
el archivo original de notación + los UCR.

A partir de mayo, cuando nos dimos cuenta del fallo (UCR solapaban con intergenic) lo que hicimos fue reemplazar este archivo de notación, por otro en el que intergenic se definiera como todo lo que no es funcional, incluidas las UCR. Este proceso está explicado en el script de notación. En este script también he cambiado el archivo al que se llama (/GRUPOS/grupolince/Lyp_annotation_Apr14_final/LYPA23C.CDS.GENE_promoters.GENE_introns.UTRs.ncRNA.lncRNA.lncRNA_introns.lncRNA_promoters.UCR.intergenic.nr.gff3), porque lo he necesitado relanzar para sierra morena. 

Aun así quiero que quede constancia de que aunque sierra morena se ha lanzado usando este archivo, las otras pobalciones no, si no usando otro antiguo sobre el que despues tuvimos que eliminar las zonas intergénicas y volverlas a correr como explico más abajo. 



```{bash}

# Run pop by pop:

POP=c_lp_sm_n019 # <--- Change pop here!
screen -S "$POP"_scaffold_per_unit
POP=c_lp_sm_n019 # <--- Change pop here!
script log_screen_"$POP"_scaffold_per_unit.log
POP=c_lp_sm_n019 # <--- Change pop here!

# Run a loop: 

# screen -S Scaffold_per_unit
# script log_screen_Scaffold_per_unit
# POPS=("c_ll_ki_n013" "c_ll_no_n008" "c_ll_po_n008" "c_lp_do_n012" "c_lp_sm_n019")


## Variables
SCAFFOLDS_FOLDER=/GRUPOS/grupolince/Lyp_annotation_Apr14_final/LYPA23C.CDS.GENE_promoters.GENE_introns.UTRs.ncRNA.lncRNA.lncRNA_introns.lncRNA_promoters.UCR.intergenic.nr.gff3.PerScaffold/
cd $SCAFFOLDS_FOLDER
SCAFFOLDS=($(find /GRUPOS/grupolince/Lyp_annotation_Apr14_final/LYPA23C.CDS.GENE_promoters.GENE_introns.UTRs.ncRNA.lncRNA.lncRNA_introns.lncRNA_promoters.UCR.intergenic.nr.gff3.PerScaffold/ -name "LYPA23C*" -exec ls {} \; | awk '{ print substr( $0, length($0) - 10, length($0) ) }'  | LANG=en_EN sort | uniq ))
DIVERSITY_PER_UNIT_FOLDER=/home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/diversity_per_unit/
SFS_FOLDER=/home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/


# for POP in "${POPS[@]}"
# do

echo "---------------------------------------------------$POP---------------------------------------------------"

# Create headers for the outfile
echo -e "scaffold\tstart_cero_based\tend\tlength\tNAs\tinformative_sites\tfeature\tstrandness\tframe\tid_gene\tid\twatterson_ave\twatterson_sd\tpairwise_ave\tpairwise_sd\ttajimaD\tpop\tspecie\tepoch" > $DIVERSITY_PER_UNIT_FOLDER"$POP".per.unit.averages.tsv

for SCAFFOLD in "${SCAFFOLDS[@]}"
do
echo "---------------------$SCAFFOLD---------------------"

#For each unit

cd "$SFS_FOLDER""$POP"_separated_by_scaffold

while read LOCATION METHOD FEATURE START_ONEBASED END POINT STRANDNESS FRAME IDRAW;

do

echo "--------$SCAFFOLD:$FEATURE--------"

if [ "$METHOD" = "PipeR" ] 
then
ID_GENE=$(echo $IDRAW | awk -F "_" '{ split ($0, a, "ID="); split (a[2],b,"_"); print b[1]"_"b[2] }')
else
ID_GENE=$(echo $IDRAW | awk '{ split($0,a,"ID="); split (a[2],b,";"); split(b[1],c,"T"); print c[1] }') 
fi

if [ "$FEATURE" = "CDS" ]
then
ID=$(echo $IDRAW | awk -F "_" '{split ($0,a,"Target="); split(a[2],b,";"); print b[1]}' |  awk '{print $1"_"$2"_"$3}' )
else
ID=$(echo $IDRAW | awk '{ split($0,a,"ID="); split (a[2],b,";"); print b[1] }')
fi

START_CEROBASED=($(echo $START_ONEBASED-1 | bc))
LENGTH=($(echo $END-$START_CEROBASED | bc)) 
SPECIE=$(echo $POP | cut -d"_" -f 2)
EPOCH=$(echo $POP | cut -d"_" -f 1)


cat "$POP".transformedThetas_"$SCAFFOLD" | bedtools intersect -a stdin -b <(echo -e "$LOCATION\t$START_CEROBASED\t$END") > "$POP".iter.transformedThetas.borrar
# He comprobado que en el archivo de Thetas hay por lo menos 4 posiciones que empieza en 0. por tanto asumo que este archivo es 0-based.

WATTERSON_AVERAGE_PER_UNIT=$(cut -f 4 "$POP".iter.transformedThetas.borrar | awk -v OFS='\t' '{for(i=1;i<=NF;i++) {sum[i] += $i }} END {for (i=1;i<=NF;i++) {printf ("%.10e\n", sum[i]/NR )}}' | sed 's/[eE]+\{0,1\}/*10^/g')
WATTERSON_SD_PER_UNIT=$(cut -f 4 "$POP".iter.transformedThetas.borrar | awk -v OFS='\t' '{for(i=1;i<=NF;i++) {sum[i] += $i; sumsq[i] += ($i)^2}} END {for (i=1;i<=NF;i++) {printf ("%.10e\n", sqrt((sumsq[i]-sum[i]^2/NR)/NR))}}' | sed 's/[eE]+\{0,1\}/*10^/g')

PAIRWISE_AVERAGE_PER_UNIT=$(cut -f 5 "$POP".iter.transformedThetas.borrar | awk -v OFS='\t' '{for(i=1;i<=NF;i++) {sum[i] += $i }} END {for (i=1;i<=NF;i++) {printf ("%.10e\n", sum[i]/NR )}}' | sed 's/[eE]+\{0,1\}/*10^/g') 
PAIRWISE_SD_PER_UNIT=$(cut -f 5  "$POP".iter.transformedThetas.borrar | awk -v OFS='\t' '{for(i=1;i<=NF;i++) {sum[i] += $i; sumsq[i] += ($i)^2}} END {for (i=1;i<=NF;i++) {printf ("%.10e\n", sqrt((sumsq[i]-sum[i]^2/NR)/NR))}}' | sed 's/[eE]+\{0,1\}/*10^/g') 

DIFFERENCE_PAIRWISE_WATTERSON_SD_PER_UNIT=$(cut -f 6  "$POP".iter.transformedThetas.borrar   |  awk -v OFS='\t' '{for(i=1;i<=NF;i++) {sum[i] += $i; sumsq[i] += ($i)^2}} END {for (i=1;i<=NF;i++) {printf ("%.10e\n", sqrt((sumsq[i]-sum[i]^2/NR)/NR ))}}' | sed 's/[eE]+\{0,1\}/*10^/g') 

TAJIMAS_D_PER_UNIT=$(echo "(($PAIRWISE_AVERAGE_PER_UNIT) - ($WATTERSON_AVERAGE_PER_UNIT))/($DIFFERENCE_PAIRWISE_WATTERSON_SD_PER_UNIT)" | bc -l | awk '{printf ("%.10e\n",$1)}' |  sed 's/[eE]+\{0,1\}/*10^/g' )

INFORMATIVESITES=$(wc -l "$POP".iter.transformedThetas.borrar | cut -d" " -f1)

NAs=($(echo $LENGTH - $INFORMATIVESITES | bc)) 

#Before printing check all variables are full, if empty use NA
if [ -z ${LOCATION} ]; then  LOCATION=NA;  fi
if [ -z ${START_CEROBASED} ]; then  START_CEROBASED=NA;  fi
if [ -z ${END} ]; then  END=NA;  fi
if [ -z ${LENGTH} ]; then  LENGTH=NA;  fi
if [ -z ${NAs} ]; then  NAs=NA;  fi
if [ -z ${INFORMATIVESITES} ]; then  INFORMATIVESITES=NA;  fi
if [ -z ${FEATURE} ]; then  FEATURE=NA;  fi
if [ -z ${STRANDNESS} ]; then  STRANDNESS=NA;  fi
if [ -z ${FRAME} ]; then  FRAME=NA;  fi
if [ -z ${ID_GENE} ]; then  ID_GENE=NA;  fi
if [ -z ${ID} ]; then  ID=NA;  fi
if [ -z ${WATTERSON_AVERAGE_PER_UNIT} ]; then  WATTERSON_AVERAGE_PER_UNIT=NA;  fi
if [ -z ${WATTERSON_SD_PER_UNIT} ]; then  WATTERSON_SD_PER_UNIT=NA;  fi
if [ -z ${PAIRWISE_AVERAGE_PER_UNIT} ]; then  PAIRWISE_AVERAGE_PER_UNIT=NA;  fi
if [ -z ${PAIRWISE_SD_PER_UNIT} ]; then  PAIRWISE_SD_PER_UNIT=NA;  fi
if [ -z ${TAJIMAS_D_PER_UNIT} ]; then  TAJIMAS_D_PER_UNIT=NA;  fi
if [ -z ${POP} ]; then  POP=NA;  fi
if [ -z ${SPECIE} ]; then  SPECIE=NA;  fi
if [ -z ${EPOCH} ]; then  EPOCH=NA;  fi


#Paste averages, and standatd deviations

paste \
<(echo $LOCATION ) \
<(echo $START_CEROBASED ) \
<(echo $END ) \
<(echo $LENGTH ) \
<(echo $NAs) \
<(echo $INFORMATIVESITES) \
<(echo $FEATURE ) \
<(echo $STRANDNESS ) \
<(echo $FRAME ) \
<(echo $ID_GENE ) \
<(echo $ID) \
<(echo $WATTERSON_AVERAGE_PER_UNIT) \
<(echo $WATTERSON_SD_PER_UNIT) \
<(echo $PAIRWISE_AVERAGE_PER_UNIT) \
<(echo $PAIRWISE_SD_PER_UNIT) \
<(echo $TAJIMAS_D_PER_UNIT) \
<(echo $POP) \
<(echo $SPECIE) \
<(echo $EPOCH) |\
sed 's/ /\t/g'| sed 's/\t\+/\t/g'  >>  $DIVERSITY_PER_UNIT_FOLDER"$POP".per.unit.averages.tsv


#Reset all (but POP,SPECIE or EPOCH (those should be the same during the whole iteration and if you remove POP the loop won't work)) variables before next iteration

unset LOCATION
unset START_CEROBASED
unset END
unset LENGTH
unset NAs
unset INFORMATIVESITES
unset FEATURE
unset STRANDNESS
unset FRAME
unset ID_GENE
unset ID
unset WATTERSON_AVERAGE_PER_UNIT
unset WATTERSON_SD_PER_UNIT
unset PAIRWISE_AVERAGE_PER_UNIT
unset PAIRWISE_SD_PER_UNIT
unset TAJIMAS_D_PER_UNIT

done < <(cat "$SCAFFOLDS_FOLDER"LYPA23C.CDS.GENE_promoters.GENE_introns.UTRs.ncRNA.lncRNA.lncRNA_introns.lncRNA_promoters.UCR.intergenic.nr.gff3_"$SCAFFOLD" ) 

done 

rm "$POP".iter.transformedThetas.borrar

```

Copia de seguridad

Hace falta copiar también lo demás que lo había lanzado de nuevo, el printed.stats y el transformedThetas:

```{bash}
# Dentro del servidor B:
scp /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/diversity_per_unit/*per.unit.averages.tsv /backup/mlucena/intermediate_files_ANGSD/whole_genome_analysis/
```

-----------

10/05/2018
y repetido con nuevo archivo el día 17/05/2018

Este script lo corrí en su momento con el archivo: /GRUPOS/grupolince/Lyp_annotation_Apr14_final/LYPA23C.CDS.GENE_promoters.GENE_introns.UTRs.ncRNA.lncRNA.lncRNA_introns.lncRNA_promoters.UCR.intergenic.nr.gff3.PerScaffold/ que no es el definitivo y que ha sido sustituido por: LYPA23C.CDS.GENE_promoters.GENE_introns.UTRs.ncRNA.lncRNA.lncRNA_introns.lncRNA_promoters.UCNE.intergenic.nr.gff3.PerScaffold. Con este corro mi script de nuevo el día 17. 


OJO! Me he dado cuenta que al menos hay un UCR dentro de una región que se considera intergénico!
Modificación de mis archivos y vuelta a correr lo que está mal. 

Lo que ocurre es que UCR están dentro de intergénico, por tanto lo que hemos considerado intergénico estaría mal. 
La forma de proceder va a ser la siguiente:

1. Borrar intergenic y los UCR del archivo de diversidad.
2. Lanzar el cálculo de diversidad sobre el intergénico y los UCNE unicamente.
3. Pegar los nuevos cálculos a nuestro archivo original de diversidad.
4. Ordenar por scaffold y posición.


---> 1. Borrar intergenic del archivo de diversidad.

```{bash}

cd /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/diversity_per_unit

for DIVERSITY_FILE in c_lp_do*.per.unit.averages.tsv
do
echo $DIVERSITY_FILE
wc -l $DIVERSITY_FILE
grep -v intergenic $DIVERSITY_FILE | grep -v UCR  > ${DIVERSITY_FILE/per.unit.averages.tsv/per.unit.averages.no_intergenic_no_UCR.tsv}
wc -l ${DIVERSITY_FILE/per.unit.averages.tsv/per.unit.averages.no_intergenic_no_UCR.tsv}
done


# Sanity
wc -l 

c_ll_ki_n013.per.unit.averages.tsv
# 499065 c_ll_ki_n013.per.unit.averages.tsv
# 425829 c_ll_ki_n013.per.unit.averages.no_intergenic_no_UCR.tsv

c_ll_no_n008.per.unit.averages.tsv
# 499065 c_ll_no_n008.per.unit.averages.tsv
# 425829 c_ll_no_n008.per.unit.averages.no_intergenic_no_UCR.tsv

c_ll_po_n008.per.unit.averages.tsv
# 499065 c_ll_po_n008.per.unit.averages.tsv
# 425829 c_ll_po_n008.per.unit.averages.no_intergenic_no_UCR.tsv

c_lp_do_n012.per.unit.averages.tsv
# 499065 c_lp_do_n012.per.unit.averages.tsv
# 425829 c_lp_do_n012.per.unit.averages.no_intergenic_no_UCR.tsv

```


---> 2. Lanzar el cálculo de diversidad sobre el intergénico únicamente.

```{bash}

# Run pop by pop:

POP=c_lp_do_n012 # <--- Change pop here!
screen -S "$POP"_scaffold_per_unit
POP=c_lp_do_n012 # <--- Change pop here!
script log_screen_"$POP"_scaffold_per_unit.log
POP=c_lp_do_n012 # <--- Change pop here!


## Variables
SCAFFOLDS_FOLDER=/GRUPOS/grupolince/Lyp_annotation_Apr14_final/LYPA23C.CDS.GENE_promoters.GENE_introns.UTRs.ncRNA.lncRNA.lncRNA_introns.lncRNA_promoters.UCNE.intergenic.nr.gff3.PerScaffold/
cd $SCAFFOLDS_FOLDER
SCAFFOLDS=($(find /GRUPOS/grupolince/Lyp_annotation_Apr14_final/LYPA23C.CDS.GENE_promoters.GENE_introns.UTRs.ncRNA.lncRNA.lncRNA_introns.lncRNA_promoters.UCNE.intergenic.nr.gff3.PerScaffold/ -name "LYPA23C*" -exec ls {} \; | awk '{ print substr( $0, length($0) - 10, length($0) ) }'  | LANG=en_EN sort | uniq ))
DIVERSITY_PER_UNIT_FOLDER=/home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/diversity_per_unit/
SFS_FOLDER=/home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/



# for POP in "${POPS[@]}"
# do

echo "---------------------------------------------------$POP---------------------------------------------------"

# Create headers for the outfile
# echo -e "scaffold\tstart_cero_based\tend\tlength\tNAs\tinformative_sites\tfeature\tstrandness\tframe\tid_gene\tid\twatterson_ave\twatterson_sd\tpairwise_ave\tpairwise_sd\ttajimaD\tpop\tspecie\tepoch" > $DIVERSITY_PER_UNIT_FOLDER"$POP".per.unit.averages.tsv

for SCAFFOLD in "${SCAFFOLDS[@]}"
do
echo "---------------------$SCAFFOLD---------------------"

#For each unit

cd "$SFS_FOLDER""$POP"_separated_by_scaffold

while read LOCATION METHOD FEATURE START_ONEBASED END POINT STRANDNESS FRAME IDRAW;

do

echo "--------$SCAFFOLD:$FEATURE--------"

if [ "$METHOD" = "PipeR" ] 
then
ID_GENE=$(echo $IDRAW | awk -F "_" '{ split ($0, a, "ID="); split (a[2],b,"_"); print b[1]"_"b[2] }')
else
ID_GENE=$(echo $IDRAW | awk '{ split($0,a,"ID="); split (a[2],b,";"); split(b[1],c,"T"); print c[1] }') 
fi

if [ "$FEATURE" = "CDS" ]
then
ID=$(echo $IDRAW | awk -F "_" '{split ($0,a,"Target="); split(a[2],b,";"); print b[1]}' |  awk '{print $1"_"$2"_"$3}' )
else
ID=$(echo $IDRAW | awk '{ split($0,a,"ID="); split (a[2],b,";"); print b[1] }')
fi

START_CEROBASED=($(echo $START_ONEBASED-1 | bc))
LENGTH=($(echo $END-$START_CEROBASED | bc)) 
SPECIE=$(echo $POP | cut -d"_" -f 2)
EPOCH=$(echo $POP | cut -d"_" -f 1)


cat "$POP".transformedThetas_"$SCAFFOLD" | bedtools intersect -a stdin -b <(echo -e "$LOCATION\t$START_CEROBASED\t$END") > "$POP".iter.transformedThetas.borrar
# He comprobado que en el archivo de Thetas hay por lo menos 4 posiciones que empieza en 0. por tanto asumo que este archivo es 0-based.


WATTERSON_AVERAGE_PER_UNIT=$(cut -f 4 "$POP".iter.transformedThetas.borrar | awk -v OFS='\t' '{for(i=1;i<=NF;i++) {sum[i] += $i }} END {for (i=1;i<=NF;i++) {printf ("%.10e\n", sum[i]/NR )}}' | sed 's/[eE]+\{0,1\}/*10^/g')
WATTERSON_SD_PER_UNIT=$(cut -f 4 "$POP".iter.transformedThetas.borrar | awk -v OFS='\t' '{for(i=1;i<=NF;i++) {sum[i] += $i; sumsq[i] += ($i)^2}} END {for (i=1;i<=NF;i++) {printf ("%.10e\n", sqrt((sumsq[i]-sum[i]^2/NR)/NR))}}' | sed 's/[eE]+\{0,1\}/*10^/g')

PAIRWISE_AVERAGE_PER_UNIT=$(cut -f 5 "$POP".iter.transformedThetas.borrar | awk -v OFS='\t' '{for(i=1;i<=NF;i++) {sum[i] += $i }} END {for (i=1;i<=NF;i++) {printf ("%.10e\n", sum[i]/NR )}}' | sed 's/[eE]+\{0,1\}/*10^/g') 
PAIRWISE_SD_PER_UNIT=$(cut -f 5  "$POP".iter.transformedThetas.borrar | awk -v OFS='\t' '{for(i=1;i<=NF;i++) {sum[i] += $i; sumsq[i] += ($i)^2}} END {for (i=1;i<=NF;i++) {printf ("%.10e\n", sqrt((sumsq[i]-sum[i]^2/NR)/NR))}}' | sed 's/[eE]+\{0,1\}/*10^/g') 

DIFFERENCE_PAIRWISE_WATTERSON_SD_PER_UNIT=$(cut -f 6  "$POP".iter.transformedThetas.borrar   |  awk -v OFS='\t' '{for(i=1;i<=NF;i++) {sum[i] += $i; sumsq[i] += ($i)^2}} END {for (i=1;i<=NF;i++) {printf ("%.10e\n", sqrt((sumsq[i]-sum[i]^2/NR)/NR ))}}' | sed 's/[eE]+\{0,1\}/*10^/g') 

TAJIMAS_D_PER_UNIT=$(echo "(($PAIRWISE_AVERAGE_PER_UNIT) - ($WATTERSON_AVERAGE_PER_UNIT))/($DIFFERENCE_PAIRWISE_WATTERSON_SD_PER_UNIT)" | bc -l | awk '{printf ("%.10e\n",$1)}' |  sed 's/[eE]+\{0,1\}/*10^/g' )

INFORMATIVESITES=$(wc -l "$POP".iter.transformedThetas.borrar | cut -d" " -f1)

NAs=($(echo $LENGTH - $INFORMATIVESITES | bc)) 

#Before printing check all variables are full, if empty use NA
if [ -z ${LOCATION} ]; then  LOCATION=NA;  fi
if [ -z ${START_CEROBASED} ]; then  START_CEROBASED=NA;  fi
if [ -z ${END} ]; then  END=NA;  fi
if [ -z ${LENGTH} ]; then  LENGTH=NA;  fi
if [ -z ${NAs} ]; then  NAs=NA;  fi
if [ -z ${INFORMATIVESITES} ]; then  INFORMATIVESITES=NA;  fi
if [ -z ${FEATURE} ]; then  FEATURE=NA;  fi
if [ -z ${STRANDNESS} ]; then  STRANDNESS=NA;  fi
if [ -z ${FRAME} ]; then  FRAME=NA;  fi
if [ -z ${ID_GENE} ]; then  ID_GENE=NA;  fi
if [ -z ${ID} ]; then  ID=NA;  fi
if [ -z ${WATTERSON_AVERAGE_PER_UNIT} ]; then  WATTERSON_AVERAGE_PER_UNIT=NA;  fi
if [ -z ${WATTERSON_SD_PER_UNIT} ]; then  WATTERSON_SD_PER_UNIT=NA;  fi
if [ -z ${PAIRWISE_AVERAGE_PER_UNIT} ]; then  PAIRWISE_AVERAGE_PER_UNIT=NA;  fi
if [ -z ${PAIRWISE_SD_PER_UNIT} ]; then  PAIRWISE_SD_PER_UNIT=NA;  fi
if [ -z ${TAJIMAS_D_PER_UNIT} ]; then  TAJIMAS_D_PER_UNIT=NA;  fi
if [ -z ${POP} ]; then  POP=NA;  fi
if [ -z ${SPECIE} ]; then  SPECIE=NA;  fi
if [ -z ${EPOCH} ]; then  EPOCH=NA;  fi


#Paste averages, and standatd deviations

paste \
<(echo $LOCATION ) \
<(echo $START_CEROBASED ) \
<(echo $END ) \
<(echo $LENGTH ) \
<(echo $NAs) \
<(echo $INFORMATIVESITES) \
<(echo $FEATURE ) \
<(echo $STRANDNESS ) \
<(echo $FRAME ) \
<(echo $ID_GENE ) \
<(echo $ID) \
<(echo $WATTERSON_AVERAGE_PER_UNIT) \
<(echo $WATTERSON_SD_PER_UNIT) \
<(echo $PAIRWISE_AVERAGE_PER_UNIT) \
<(echo $PAIRWISE_SD_PER_UNIT) \
<(echo $TAJIMAS_D_PER_UNIT) \
<(echo $POP) \
<(echo $SPECIE) \
<(echo $EPOCH) |\
sed 's/ /\t/g'| sed 's/\t\+/\t/g'  >>  $DIVERSITY_PER_UNIT_FOLDER"$POP".per.unit.averages.no_intergenic_no_UCR.tsv

#Reset all (but POP,SPECIE or EPOCH (those should be the same during the whole iteration and if you remove POP the loop won't work)) variables before next iteration

unset LOCATION
unset START_CEROBASED
unset END
unset LENGTH
unset NAs
unset INFORMATIVESITES
unset FEATURE
unset STRANDNESS
unset FRAME
unset ID_GENE
unset ID
unset WATTERSON_AVERAGE_PER_UNIT
unset WATTERSON_SD_PER_UNIT
unset PAIRWISE_AVERAGE_PER_UNIT
unset PAIRWISE_SD_PER_UNIT
unset TAJIMAS_D_PER_UNIT

done < <(cat "$SCAFFOLDS_FOLDER"LYPA23C.CDS.GENE_promoters.GENE_introns.UTRs.ncRNA.lncRNA.lncRNA_introns.lncRNA_promoters.UCNE.intergenic.nr.gff3_"$SCAFFOLD" | grep 'intergenic\|UCNE' ) 

done &

rm "$POP".iter.transformedThetas.borrar

```

---> 4. Muevo los archivos recien creados al nombre inicial.


```{bash}

for diversity_file in *.no_intergenic.tsv
do
echo $diversity_file
mv $diversity_file ${diversity_file/.per.unit.averages.no_intergenic.tsv/.per.unit.averages.tsv}
done

```

---> 5. Ordeno


```{bash}

for file_diversity in *per.unit.averages.tsv
do
echo $file_diversity
echo ${file_diversity/.per.unit.averages.tsv/.per.unit.averages.sorted.tsv}
LANG=en_EN sort -k 1,1 -k 2,2n -k 3,3n $file_diversity  > ${file_diversity/.per.unit.averages.tsv/.per.unit.averages.sorted.tsv}
done


mv c_lp_do_n012.per.unit.averages.sorted.tsv c_lp_do_n012.per.unit.averages.tsv
mv c_ll_po_n008.per.unit.averages.sorted.tsv c_ll_po_n008.per.unit.averages.tsv
mv c_ll_ki_n013.per.unit.averages.sorted.tsv c_ll_ki_n013.per.unit.averages.tsv
mv c_ll_no_n008.per.unit.averages.sorted.tsv c_ll_no_n008.per.unit.averages.tsv
mv c_lp_sm_n019.per.unit.averages.sorted.tsv c_lp_sm_n019.per.unit.averages.tsv


# Hemos cometido un fallo, y es que hemos ordenado el archivo y tenía header, por lo que ahora lo tenemos que buscar, eliminar y ponerlo en donde corresponde. 

grep -v start_cero_based c_ll_ki_n013.per.unit.averages.tsv > c_ll_ki_n013.per.unit.averages.NO_HEADER.tsv
grep -v start_cero_based c_ll_no_n008.per.unit.averages.tsv > c_ll_no_n008.per.unit.averages.NO_HEADER.tsv
grep -v start_cero_based c_ll_po_n008.per.unit.averages.tsv > c_ll_po_n008.per.unit.averages.NO_HEADER.tsv
grep -v start_cero_based c_lp_do_n012.per.unit.averages.tsv > c_lp_do_n012.per.unit.averages.NO_HEADER.tsv


# HEADER:

echo -e "scaffold\tstart_cero_based\tend\tlength\tNAs\tinformative_sites\tfeature\tstrandness\tframe\tid_gene\tid\twatterson_ave\twatterson_sd\tpairwise_ave\tpairwise_sd\ttajimaD\tpop\tspecie\tepoch" > header_global

for file in *.NO_HEADER.tsv
do
echo $file
cat header_global $file > ${file/NO_HEADER./}
done

rm *HEADER*

```


---> 6. Copia de seguridad

Hace falta copiar también lo demás que lo había lanzado de nuevo, el printed.stats y el transformedThetas:

```{bash}
# Dentro del servidor B:
scp /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/diversity_per_unit/*per.unit.averages.tsv /backup/mlucena/intermediate_files_ANGSD/whole_genome_analysis/
```



-----------

Anotación de las unidades: cromosoma y regiones; tel y centr. 

Ahora vamos a anotar esas unidades según pertenezcan o no a distintos cromosomas, y según sean teloméricas, centromérica o no. 


Cromosomas 

Para ello hemos usado el archivo que tienen en común las coordenadas de gato y de lince

```{bash}
head /GRUPOS/grupolince/copia_fabascal/MAPPINGS/lynx2cat_wTiger.sorted.bed
```
  
Creo una carpeta donde voy a guardar los archivos:
```{bash}
mkdir /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/chromosome_annotation
```

Los archivos de los que parto estén en:
```{bash}
/home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/diversity_per_unit/
```

Empiezo a unirlos. 

El primer Bed te devolvería un archivo con una línea por cada posición de sintenia tal que así:

lp23.s00001	0	15011	15011	11950	3061	intergenic	-	.	intergenic_region_1	intergenic_region_1	2.4096547746*10^-06	1.3458559263*10^-05	1.1416658651*10^-06	6.2801913098*10^-06	-1.7606196687*10^-01	c_ll_no_n008	ll	c	lp23.s00001	4071	4072	TCL=TTT:chrA3:15104518-15104519:NO	1

Como no queremos pasar por ahí ni guardar tanto, vamos a intentar usar awk directamente para quedarnos con lo que nos interese. 

```{bash}

screen -S chromosome_annotation

cd /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/diversity_per_unit/

for FILE in *.per.unit.averages.tsv

do
echo $FILE
cd /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/chromosome_annotation
intersectBed -sorted -wo -a <(tail -n +2 /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/diversity_per_unit/$FILE) -b /GRUPOS/grupolince/copia_fabascal/MAPPINGS/lynx2cat_wTiger.sorted.bed | awk -v OFS='\t' '{split ($23, a, ":"); split(a[3],b, "-"); print $1,$2,$3,$4,$5,$6,$7,$8,$9,$10,$11,$12,$13,$14,$15,$16,$17,$18,$19,a[2] }' | awk '!a[$0]++' > ${FILE/.per.unit.averages.tsv/.per.unit.averages.chr.tsv}
done

echo -e "scaffold\tstart_cero_based\tend\tlength\tNAs\tinformative_sites\tfeature\tstrandness\tframe\tid_gene\tid\twatterson_ave\twatterson_sd\tpairwise_ave\tpairwise_sd\ttajimaD\tpop\tspecie\tepoch\tchr" > header.chr.annotation

for FILE in *per.unit.averages.chr.tsv
do
echo $FILE
cat header.chr.annotation $FILE > ${FILE/per.unit.averages.chr.tsv/per.unit.averages.chr.HEADER.tsv} 
mv ${FILE/per.unit.averages.chr.tsv/per.unit.averages.chr.HEADER.tsv} $FILE
done

# De esta manera estamos eliminando filas duplicadas. Si queremos comprobar cuantos cromosomas habría en cada unidad tenemos que postprocesar en R. El merge de abajo va a juntarnos todo auqnue sean distintas unidades si les separa una base, así que no nos interesa.
# bedtools merge -i stdin -c 11,20 -o distinct,distinct -delim ";" > ${FILE/per.unit.averages.tsv/per.unit.averages.chr.tsv}

```

Esto me devuelve algo como así:
lp23.s00001	0	15011	15011	11950	3061	intergenic	-	.	intergenic_region_1	intergenic_region_1	2.4096547746*10^-06	1.3458559263*10^-05	1.1416658651*10^-06	6.2801913098*10^-06	-1.7606196687*10^-01	c_ll_no_n008	ll	c	chrA3


Sanity checks

```{bash}

cd /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/chromosome_annotation

# Calculo qué unidades aparecen más de una vez:

awk '{print $7"_"$11}' c_lp_do_n012.per.unit.averages.chr.tsv | sort | uniq -c | grep -v "1 " | wc -l
# Hay 2534 

# ¿Qué hay en esta lista?

 awk '{print $7"_"$11}' c_lp_do_n012.per.unit.averages.chr.tsv | sort | uniq -c | grep -v "1 " | sed -e 's/^[ \t]*//' | cut -d' ' -f2  | cut -d"_" -f 1 | sort  | uniq -c


      9 3UTR
     14 5UTR
     12 CDS
   1033 intergenic
    414 intron
      2 lncRNA
      1 ncRNA
      9 promoter
   1040 UCR

# Esta lista incluye varias features distintas.

# Parece haber al menos dos casos claros: 

# 1. Por un lado, aquellos UCR que están separados en varios trozos porque no logré unirlo por la sintenia. Ejemplo: ZMF521_Dorian está tres veces

# grep ZNF521_Dorian ../c_lp_do_n012.per.unit.averages.chr.tsv
# lp23.s31342	333553	333644	91	0	91	UCR	-	.	ZNF521_Dorian	ZNF521_Dorian	4.8025515249*10^-07	3.4994486408*10^-06	1.7249711055*10^-08	8.4251477269*10^-09	-1.3232153436*10^-01	c_lp_do_n012	lp	c	chrD3
# lp23.s31342	333645	333762	117	0	117	UCR	-	.	ZNF521_Dorian	ZNF521_Dorian	2.0280707373*10^-07	1.7930201697*10^-07	6.1292390857*10^-08	2.8242579987*10^-08	-7.3264237026*10^-01	c_lp_do_n012	lp	c	chrD3
# lp23.s31342	333769	333873	104	0	104	UCR	-	.	ZNF521_Dorian	ZNF521_Dorian	1.4021552729*10^-07	6.4529679510*10^-08	1.0221886658*10^-07	7.6602016576*10^-07	-4.8985261517*10^-02	c_lp_do_n012	lp	c	chrD3

# ¿Cuantas ultraconserved regions hay en total?

grep UCR /GRUPOS/grupolince/Lyp_annotation_Apr14_final/LYPA23C.CDS.GENE_promoters.GENE_introns.UTRs.ncRNA.lncRNA.lncRNA_introns.lncRNA_promoters.UCR.intergenic.nr.gff3 | wc -l
5539 

# Si tenemos que 1040 aparecen repetidas, vemos que son un quinto aquellas que no tienen contigüidad. 
# Hay que tener cuidado con esto al hacer calculos por unidad! Pero esto no es tan preocupante porque sabíamos que ocurria.

# 2. Por otro lado, parece que dentro de estas features repetidas tengo otras casuisticas:

# Ejemplo: Un intron repetido tres veces:

# grep LYPA23C000420T1 c_lp_do_n012.per.unit.averages.chr.tsv | grep intron_62

# lp23.s05342	577057	603192	26135	15052	11083	intron	+	0	LYPA23C000420	LYPA23C000420T1_intron_62	1.9406089032*10^-04	7.1920669656*10^-03	2.7265147013*10^-04	1.0183730201*10^-02	2.5254946264*10^-02	c_lp_do_n012	lp	cchrB2
# lp23.s05342	577057	603192	26135	15052	11083	intron	+	0	LYPA23C000420	LYPA23C000420T1_intron_62	1.9406089032*10^-04	7.1920669656*10^-03	2.7265147013*10^-04	1.0183730201*10^-02	2.5254946264*10^-02	c_lp_do_n012	lp	cchrX
# lp23.s05342	577057	603192	26135	15052	11083	intron	+	0	LYPA23C000420	LYPA23C000420T1_intron_62	1.9406089032*10^-04	7.1920669656*10^-03	2.7265147013*10^-04	1.0183730201*10^-02	2.5254946264*10^-02	c_lp_do_n012	lp	cchrB1

# Este intrón aparece asignado a tres cromosomas distintos. 

# Compruebo que efectivamente se asigna a tres cromosomas en el archivo de notación

grep lp23.s05342 /GRUPOS/grupolince/copia_fabascal/MAPPINGS/lynx2cat_wTiger.sorted.bed  | grep -v chrB2

# Efectivamente hay más de un cromosoma en ese scaffold. 

# Vamos a ver que número de features presentan este problema. 

# Creo una lista PARA DOÑANA de aquellas features que son identicas también en las posiciones (lo que excluiría a los casos de las UCR que hemos hablado antes). Para ello imprimo también la posición de comienzo y la de fin.

awk -v OFS='\t' '{print $1,$2,$3,$7,$11}' c_lp_do_n012.per.unit.averages.chr.tsv | sort | uniq -c | grep -v "1 " | sort | sed -e 's/^[ \t]*//'  > list_of_repeated_features_do

# Compruebo a qué unidades pertenecen. 

 cut -d' ' -f2 list_of_repeated_features_do | awk '{print $4}' | sort  | uniq -c

      9 3UTR
     14 5UTR
     12 CDS
   1033 intergenic
    384 intron
     30 intron_lncRNA
      2 lncRNA
      1 ncRNA
      1 promoter_gene_1000
      3 promoter_lncRNA_1000
      2 promoter_lncRNA_250
      3 promoter_lncRNA_500
      
      
# Vemos que todos los numeros coinciden con las unidades repetidas que habían salido antes (ojo: promotores=9 (1+3+2+3); intrones=414(384+30)). Por tanto, parece que todas estas regiones repetidas están asignadas a mas de un cromosoma. Ahora vamos a ver cuantas bases son eso.

# Compruebo que en las otras poblaciones son las mismas.

cd /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/chromosome_annotation

awk -v OFS='\t' '{print $1,$2,$3,$7,$11}' c_ll_ki_n013.per.unit.averages.chr.tsv | sort | uniq -c | grep -v "1 " | sort | sed -e 's/^[ \t]*//' > list_of_repeated_features_ki

awk -v OFS='\t' '{print $1,$2,$3,$7,$11}' c_ll_no_n008.per.unit.averages.chr.tsv | sort | uniq -c | grep -v "1 " | sort | sed -e 's/^[ \t]*//' > list_of_repeated_features_no

awk -v OFS='\t' '{print $1,$2,$3,$7,$11}' c_ll_po_n008.per.unit.averages.chr.tsv | sort | uniq -c | grep -v "1 " | sort | sed -e 's/^[ \t]*//' > list_of_repeated_features_po

diff list_of_repeated_features_do list_of_repeated_features_po
diff list_of_repeated_features_do list_of_repeated_features_ki
diff list_of_repeated_features_do list_of_repeated_features_no

# Por suerte son iguales! 

# Así que para futuras comprobaciones puedo coger cualquiera. Borro 3 de ellas. 

rm list_of_repeated_features_no
rm list_of_repeated_features_ki
rm list_of_repeated_features_po

# ¿Cuantas bases perdemos?

mkdir features_repeted
cd features_repeted/

# Primero calculamos cuantas bases totales tenemos:

# Este archivo que genero tiene todas las unidades, no solo las repetidas.
awk -v OFS='\t' '{print $1,$2,$3,$7,$11}' ../c_lp_do_n012.per.unit.averages.chr.tsv |  awk '{print >> $4; close($4)}'

rm feature 

# ¿Cuantas bases por cada unidad?

for file in * 
do
echo $file "$(awk '{sum+=$3-$2}END{print sum}' $file )"
done >> bases_total.txt

mv bases_total.txt ../

# Ahora elimino estos archivos y hago la misma cuenta para aquellas que están repetidas.

# ¿Cuantas repetidas?

awk -v OFS='\t' '{print $1,$2,$3,$7,$11}' ../c_lp_do_n012.per.unit.averages.chr.tsv | sort | uniq -c | grep -v "1 " | sort | sed -e 's/^[ \t]*//' 

awk -v OFS='\t' '{print $1,$2,$3,$7,$11}' ../c_lp_do_n012.per.unit.averages.chr.tsv | sort | uniq -c | grep -v "1 " | sort | sed -e 's/^[ \t]*//' | cut -d' ' -f2-6 | awk '{print >> $4; close($4)}'

rm feature 

# ¿Cuantas bases por cada unidad?

for file in * 
do
echo $file "$(awk '{sum+=$3-$2}END{print sum}' $file )"
done >> bases_repeted.txt

mv bases_repeted.txt ../

rm *

# ¿Qué porcentaje supone del total?

join -1 1 -2 1 bases_total.txt bases_repeted.txt | awk -v OFS='\t' '{print $1,$2,$3,$3/$2*100}' | cat <(echo -e "feature\ttotal_bases\tbases_assinged_to_2_or_more_chr\tpercentage") - > percentage_of_bases_assigned_to_2_chr_or_more.txt


rm bases_repeted.txt
rm bases_total.txt 
rm -r features_repeted/
rm list_of_repeated_features_do 


```


Telomeros2m

El archivo de interés, es: 
```{bash}
/GRUPOS/grupolince/telomers_centromers_definition/tel2m_regions_based_on_synteny_1000bp.bed
```

Creo una carpeta donde voy a guardar los archivos:
```{bash}
mkdir /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/region_annotation
```

Los archivos de los que parto estén en:
```{bash}
/home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/diversity_per_unit/
```

Empiezo a unirlos. 

```{bash}
cd /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/diversity_per_unit/

files_per_pop=($(ls *.per.unit.averages.tsv))

cd /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/region_annotation

echo -e "scaffold\tstart_cero_based\tend\tlength\tNAs\tinformative_sites\tfeature\tstrandness\tframe\tid_gene\tid\twatterson_ave\twatterson_sd\tpairwise_ave\tpairwise_sd\ttajimaD\tpop\tspecie\tepoch\tchr\ttel2m_bases" > header.region.tel2m.tsv

for file_pop in ${files_per_pop[@]}
do
echo $file_pop
intersectBed -sorted -wo -a <(tail -n +2 /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/diversity_per_unit/$file_pop) -b /GRUPOS/grupolince/telomers_centromers_definition/tel2m_regions_based_on_synteny_1000bp.bed | awk -v OFS='\t' '{print $1,$2,$3,$4,$5,$6,$7,$8,$9,$10,$11,$12,$13,$14,$15,$16,$17,$18,$19,$23,$24 }' | awk '!a[$0]++' | cat header.region.tel2m.tsv - > ${file_pop/.per.unit.averages.tsv/.per.unit.averages.region.tel2m.tsv}

done
```

Esto me devuelve algo como así:
lp23.s00006	14749	29606	14857	8272	6585	intergenic	-	.	intergenic_region_445	intergenic_region_445	6.8371515163*10^-04	1.2551611729*10^-02	1.1155938889*10^-03	2.1427002344*10^-02	4.4628203002*10^-02	c_lp_sm_n019	lp	c	ChrB2 485

donde el último número representa las bases que solapan. 

Telomeros10m

El archivo de interés, es: 
```{bash}
/GRUPOS/grupolince/telomers_centromers_definition/tel0-10m_regions_based_on_synteny_1000bp.bed
```

Voy a guardar los archivos en la carpeta anteriormente creada:
```{bash}
/home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/region_annotation
```

Los archivos de los que parto estén en:
```{bash}
/home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/diversity_per_unit/
```

Empiezo a unirlos. 

```{bash}
cd /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/diversity_per_unit/

files_per_pop=($(ls *.per.unit.averages.tsv))

cd /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/region_annotation

echo -e "scaffold\tstart_cero_based\tend\tlength\tNAs\tinformative_sites\tfeature\tstrandness\tframe\tid_gene\tid\twatterson_ave\twatterson_sd\tpairwise_ave\tpairwise_sd\ttajimaD\tpop\tspecie\tepoch\tchr\ttel10m_bases" > header.region.tel10m.tsv


for file_pop in ${files_per_pop[@]}
do
echo $file_pop
intersectBed -sorted -wo -a <(tail -n +2 /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/diversity_per_unit/$file_pop) -b /GRUPOS/grupolince/telomers_centromers_definition/tel0-10m_regions_based_on_synteny_1000bp.bed | awk -v OFS='\t' '{print $1,$2,$3,$4,$5,$6,$7,$8,$9,$10,$11,$12,$13,$14,$15,$16,$17,$18,$19,$23,$24 }' | awk '!a[$0]++' | cat header.region.tel10m.tsv - > ${file_pop/.per.unit.averages.tsv/.per.unit.averages.region.tel10m.tsv}
done

```


Y por último los centrómeros:

Centrómeros:

El archivo de interés, es: 
```{bash}
/GRUPOS/grupolince/telomers_centromers_definition/centr_regions_based_on_synteny_1000bp.bed
```

Voy a guardar los archivos en la carpeta anteriormente creada:
```{bash}
/home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/region_annotation
```

Los archivos de los que parto estén en:
```{bash}
/home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/diversity_per_unit/
```

Empiezo a unirlos. 

```{bash}
cd /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/diversity_per_unit/

files_per_pop=($(ls *.per.unit.averages.tsv))

cd /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/region_annotation

echo -e "scaffold\tstart_cero_based\tend\tlength\tNAs\tinformative_sites\tfeature\tstrandness\tframe\tid_gene\tid\twatterson_ave\twatterson_sd\tpairwise_ave\tpairwise_sd\ttajimaD\tpop\tspecie\tepoch\tchr\tcentr_bases" > header.region.centr.tsv


for file_pop in ${files_per_pop[@]}
do
echo $file_pop
intersectBed -sorted -wo -a <(tail -n +2 /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/diversity_per_unit/$file_pop) -b /GRUPOS/grupolince/telomers_centromers_definition/centr_regions_based_on_synteny_1000bp.bed | awk -v OFS='\t' '{print $1,$2,$3,$4,$5,$6,$7,$8,$9,$10,$11,$12,$13,$14,$15,$16,$17,$18,$19,$23,$24 }' | awk '!a[$0]++' | cat header.region.centr.tsv - > ${file_pop/.per.unit.averages.tsv/.per.unit.averages.region.centr.tsv}
done

```


Ahora borro lo que no necesito

```{bash}
rm header.region.*
```


------------------------------------------


Postprocesado en R

Ahora necesito hacer varias cositas con estos archivos que acabo de crear. En primer lugar tengo que colapsar aquellos que tienen tel o centr, ya que si este centr estaba definido en base a varias regiones pertenecientes a la misma unidad aparece varias veces. Ejemplo:

scaffold1 1 3000 genA

telómeros:

scaffold1 2 200
scaffold1 500 1000
scaffold1 1500  3000

Esto apareceria así:

scaffold1 1 3000 genA scaffold1 2 200 198
scaffold1 1 3000 genA scaffold1 500 1000  500
scaffold1 1 3000 genA scaffold1 1500  3000  1500

Siendo la tercera columna el número de posiciones que solapan.

Por tanto yo lo que querría obtener sería:
scaffold1 1 3000 genA 2198

Con una columna que recoja el número de posiciones que solapan, para que despues yo pueda hacer un porcentaje de cuantas bases están cubiertas. 


Primero me bajo las tablas.


```{bash}

scp mlucena@genomics-b.ebd.csic.es:/home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/diversity_per_unit/*tsv /Users/marialucenaperez/Documents/WG_lynx_diversity_per_unit
scp mlucena@genomics-b.ebd.csic.es:/home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/region_annotation/*tsv /Users/marialucenaperez/Documents/WG_lynx_diversity_per_unit
scp mlucena@genomics-b.ebd.csic.es:/home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/chromosome_annotation/*tsv /Users/marialucenaperez/Documents/WG_lynx_diversity_per_unit

```


Procesamiento de la tabla con region annotation. Esto sólo se hace una vez!
Comprobamos que el archivo tiene información y añadimos un porcentaje con las bases que tiene que son teloméricas / centroméricas.

```{r}

library(dplyr)

wd <- "/Users/marialucenaperez/Documents/WG_lynx_diversity_per_unit/"

# Tel2m
files_tel2m <- list.files(path = wd, pattern = "region.tel2m")

# Esto saca el porcentaje de bases anotadas como de una región determinada para cada población
for (file_tel2m in files_tel2m){
dataframe <- tryCatch({
        if (file.size(paste(wd, file_tel2m, sep="")) > 0)
          {
    read.table(file=paste(wd, file_tel2m, sep=""), header=T) %>% 
            mutate (tel2m_percentage=tel2m_bases/length)
          }
}, error =function(err) {
            # error handler picks up where error was generated
            print(paste("Read.table didn't work!:  ",err))
        })
# Hasta aquí bien, ahora guardo estos archivos en la carpeta de interes.
write.table(dataframe, paste(wd, file_tel2m, sep=""), row.names = F, quote = F, sep = '\t')
}


# Tel10m
files_tel10m <- list.files(path = wd, pattern = "region.tel10m")

# Esto saca el porcentaje de bases anotadas como de una región determinada para cada población
for (file_tel10m in files_tel10m){
dataframe <- tryCatch({
        if (file.size(paste(wd, file_tel10m, sep="")) > 0)
          {
    read.table(file=paste(wd, file_tel10m, sep=""), header=T) %>% 
            mutate (tel10m_percentage=tel10m_bases/length)
          }
}, error =function(err) {
            # error handler picks up where error was generated
            print(paste("Read.table didn't work!:  ",err))
        })
# Hasta aquí bien, ahora guardo estos archivos en la carpeta de interes.
write.table(dataframe, paste(wd, file_tel10m, sep=""), row.names = F, quote = F, sep = '\t')
}

# Centr

files_centr <- list.files(path = wd, pattern = "region.centr")

# Esto saca el porcentaje de bases anotadas como de una región determinada para cada población
for (file_centr in files_centr){
dataframe <- tryCatch({
        if (file.size(paste(wd, file_centr, sep="")) > 0)
          {
    read.table(file=paste(wd, file_centr, sep=""), header=T) %>% 
            mutate (centr_percentage=centr_bases/length)
          }
}, error =function(err) {
            # error handler picks up where error was generated
            print(paste("Read.table didn't work!:  ",err))
        })
# Hasta aquí bien, ahora guardo estos archivos en la carpeta de interes.
write.table(dataframe, paste(wd, file_centr, sep=""), row.names = F, quote = F, sep = '\t')
}


```


------------------------------------------

R analysis



Ahora tengo que descargarme todos los archivos de cada población y despues hacer un super-join para cada población para acabar haciendo una unica tabla con rbind con todas las poblaciones. 

Creación tablas

```{r}

# Ahora hago un join de la tabla principal de diversidad con las de telómeros, centrómeros y cromosoma. 

library(dplyr)
library(ggplot2)
library(tidyr)

wd <- "/Users/marialucenaperez/Documents/WG_lynx_diversity_per_unit/"

# poplist <- c("sm")
poplist <- c("do","po","no","ki")

for (pop in poplist)
{
  if (exists("dataset"))
  {rm (dataset)
  }
  
  files_for_given_pop <- list.files(path = wd, pattern = pop)
  
  for (file_for_given_pop in files_for_given_pop)
  {
    # if the merged dataset doesn't exist, create it
    if (!exists("dataset"))
    {
      dataset <- read.table(paste(wd,file_for_given_pop, sep=""), header=TRUE, sep="\t") 
    }
    
    # if the merged dataset does exist, append to it
    if (exists("dataset"))
    {
      temp_dataset <-read.table(paste(wd,file_for_given_pop, sep=""), header=TRUE, sep="\t")
      dataset<-full_join(dataset, temp_dataset, by = c("scaffold", "start_cero_based", "end", "length", "NAs", "informative_sites", "feature", "strandness", "frame", "id_gene", "id", "watterson_ave", "watterson_sd", "pairwise_ave", "pairwise_sd", "tajimaD", "pop", "specie", "epoch"))   
      rm(temp_dataset)
      assign(pop, dataset) # Con esto quiero ponerle el nombre de la población a la dataframe. 
    }
  }    
}


rm(dataset)

data_diversity <-rbind(do, ki, no, po) %>% 
    mutate( watterson_ave = as.numeric(gsub("\\*10\\^","e",watterson_ave)),
            watterson_sd = as.numeric(gsub("\\*10\\^","e",watterson_sd)),
            pairwise_ave  = as.numeric(gsub("\\*10\\^","e",pairwise_ave)),
            pairwise_sd  = as.numeric(gsub("\\*10\\^","e",pairwise_sd)),
            tajimaD = as.numeric(gsub("\\*10\\^","e",tajimaD)))


write.table (data_diversity, paste(wd, "global.per.unit.averages.chr.all.regions.tsv", sep=""), row.names = F, quote = F, sep = '\t')
write.table (ki, paste(wd, "c_ll_ki_n013.per.unit.averages.chr.all.regions.tsv", sep=""), row.names = F, quote = F, sep = '\t')
write.table (no, paste(wd, "c_ll_no_n008.per.unit.averages.chr.all.regions.tsv", sep=""), row.names = F, quote = F, sep = '\t')
write.table (po, paste(wd, "c_ll_po_n008.per.unit.averages.chr.all.regions.tsv", sep=""), row.names = F, quote = F, sep = '\t')
write.table (do, paste(wd, "c_ll_do_n012.per.unit.averages.chr.all.regions.tsv", sep=""), row.names = F, quote = F, sep = '\t')

```


Cargo las tablas


```{r}

data_diversity <- read.table(paste(wd, "global.per.unit.averages.chr.all.regions.tsv", sep=""), header=T)

# # # 
# Sanity check --> ok!
# lista_ki <- ki$id
# lista_no  <- no$id
# lista_po  <- po$id
# lista_do  <- do$id
# setdiff(lista_ki, lista_no)
# setdiff(lista_ki, lista_po)
# setdiff(lista_ki, lista_do)
# # # 


data_diversity_ki_no <- full_join(ki, no, by = c("scaffold", "start_cero_based", "end", "length", "feature", "strandness", "frame", "id","id_gene")) 

kk <- data_diversity_ki_no %>% mutate (comparison=ifelse(chr.x.x.x==chr.y.x, "equal", "different"))

lista_ki_no <- full_join(ki, no, by = c("scaffold", "start_cero_based", "end", "length", "feature", "strandness", "frame", "id","id_gene")) %>% select (feature, id) %>% unite (feature, id, sep="_", col="prueba") %>% select(prueba)

setdiff(lista_ki, lista_ki_no)
duplicated(lista_ki_no)
duplicated(lista_ki_no) | lista_ki_no(lista_ki_no[nrow(lista_ki_no):1, ])[nrow(lista_ki_no):1]

lista_ki_no[duplicated(lista_ki_no)]


which(data_diversity_ki_no$prueba == 675)

kk <- do %>% filter (id_gene=="intergenic_region_44")
```


Exploracion de los datos

```{r}




ggplot(data = data_diversity, aes(x=feature, y = pairwise_ave, fill=pop)) +
  geom_boxplot() +
  facet_grid(.~chr, scales="free") 





ggplot(data = data_diversity, aes(x=feature, y = pairwise_ave, fill=pop)) +
  geom_boxplot() +
  # geom_point(data=df.pm,aes(x=feature, y = weightedmean, fill=pop),shape = 23, position=position_dodge(width=0.75), size = 3, inherit.aes=FALSE) +
  facet_grid(.~feature, scales="free") +
  scale_y_continuous(trans = 'log10') +
  scale_x_discrete("Genomic region") +
  xlab(label = "Genomic region") + #x title
  ylab(label = "Pairwise average") + # y title
  theme_bw() +  #theme selection for background and lines
 # scale_x_continuous(expand = c(0, 1)) +# + scale_y_continuous(expand = c(0, 0)) +
  ggtitle(paste ("Diversity region",name_diversity[1], sep = " " )) +
  theme(axis.line.x = element_line(color="black", size = 0.5),
        axis.line.y = element_line(color="black", size = 0.5),
        axis.text.x = element_text(angle = 60, vjust = 0.8, hjust = 1),
        axis.text.y = element_text(vjust = 0.2, hjust = 0.2),
        axis.title.y = element_text(margin=margin(r=0.3, unit="cm")),
        axis.title.x = element_text(margin=margin(t=0.5, unit="cm")),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.border = element_blank(),
        panel.background = element_blank(),
        strip.background = element_blank(),
        strip.text.x = element_blank())+ 
        stat_summary(fun.y=mean, colour="darkred", geom="point", shape=18, size=3,show_guide = FALSE) + 
  geom_text(data = df.pm.gathered, aes(feature, percentage_do_sm, label=(percentage_do_sm %>% round(2))),angle = 45, size=2)

```





------------------------------------------

Cosas antiguas!

Correcciones diversity per feature!!

```{bash}

# 8/9/2017: Haciendo un plot me he dado cuenta que hay algunas filas duplicadas en el archivo original .gff3, corresponden a UTR. 
# En la carpeta /home/mlucena/grupolince/notation, están especificadas cuales son. He corregido el gff3, pero como no voy a volver a correr mis archivos para sacar los números, voy a corregir todos los originales para tenerlo todo bien desde el principio. 

# Check:

# sort c_ll_ki_n013.per.unit.averages.tsv | uniq -c | sort -nr | head

# 2 lp23.s36682     338697  338929  232     -1      3UTR    +       .       LYPA23C000008   LYPA23C000008   1.4564973179*10^-06     4.5837356691*10^-06     4.4811971320*10^-07     1.3890498033*10^-06     -3.1562808160*1
# 2 lp23.s36682     328523  328962  439     75      5UTR    -       .       LYPA23C000003   LYPA23C000003   5.0803158132*10^-06     1.2267826914*10^-05     1.8178262696*10^-06     4.0698820431*10^-06     -3.9712504595*1
# 2 lp23.s36682     279677  279811  134     -1      3UTR    +       .       LYPA23C000015   LYPA23C000015   1.0437203878*10^-06     4.1265547119*10^-06     3.1206706874*10^-07     1.2333416354*10^-06     -2.5288512495*1
# 2 lp23.s36682     100823  101610  787     82      3UTR    +       .       LYPA23C000004   LYPA23C000004   3.7669060166*10^-04     9.8624719772*10^-03     3.9965744098*10^-04     1.0556168131*10^-02     3.3103961265*10
# 2 lp23.s26402     92633   95884   3251    166     3UTR    +       .       LYPA23C000017   LYPA23C000017   9.8348133084*10^-06     3.1577310510*10^-04     4.3617798050*10^-06     1.5841654621*10^-04     -3.4633370151*1
# 2 lp23.s10719     96754   96794   40      -1      5UTR    -       .       LYPA23C000041   LYPA23C000041   3.0378406270*10^-06     1.0043475395*10^-06     1.0387353248*10^-06     3.9400597979*10^-07     -3.2716817179*1
# 2 lp23.s10719     156260  156378  118     96      5UTR    -       .       LYPA23C000045   LYPA23C000045   1.3834272997*10^-05     2.2261745529*10^-06     6.3570507930*10^-06     1.2906641862*10^-06     -7.9734277877*1
# 1 scaffold        start   end     length  NAs     feature strandness      frame   id_gene id      watterson_ave   watterson_sd    pairwise_ave    pairwise_sd     tajimaD informative_sites       pop     specie
# 1 lp23.s41700     1       200     199     199     intergenic      -       .       intergenic_region_65795 intergenic_region_65795         0       c_ll_ki_n013    c_ll
# 1 lp23.s41699     1       200     199     199     intergenic      -       .       intergenic_region_65794 intergenic_region_65794         0       c_ll_ki_n013    c_ll


# 491637 c_ll_ki_n013.per.unit.averages.tsv

# Efectivamente aquí también está duplicados, así que corro el loop para corregirlo:

for i in  *.per.unit.averages.tsv
do
echo $i
POP=$(echo ${i} | cut -d "." -f 1 )
SPECIE=$(echo $POP | cut -d"_" -f 1-2)
awk '!seen[$0]++' $i > ${i/.tsv/.corrected.tsv} # remove duplicated rows.
mv ${i/.tsv/.corrected.tsv} $i
done



# Create a unique file with all the information

awk 'FNR==1 && NR!=1{next;}{print}' c_lp*averages.tsv > c_lp_do_n012-c_lp_sm_n019.per.unit.averages.per_specie.tsv
awk 'FNR==1 && NR!=1{next;}{print}' c_ll*averages.tsv > c_ll_ki_n013-c_ll_no_n008-c_ll_po_n008.per.unit.averages.per_specie.tsv


# Compruebo que ha funcionado:

# sort c_ll_ki_n013.per.unit.averages.tsv | uniq -c | sort -nr | head
# 1 scaffold	start	end	length	NAs	feature	strandness	frame	id_gene	id	watterson_ave	watterson_sd	pairwise_ave	pairwise_sd	tajimaD	informative_sites	pop	specie
# 1 lp23.s41700	1	200	
      
# Perfecto!!
      
```

OJO!!

Calcular luego en R medidas pareadas!!! cada unidad con su equivalente en las tres poblaciones!


```{bash}
nota


wc -l c_ll_ki_n013-c_ll_no_n008-c_ll_po_n008.per.unit.averages.per_specie.tsv
1474888 c_ll_ki_n013-c_ll_no_n008-c_ll_po_n008.per.unit.averages.per_specie.tsv


awk '{if ($18 == "") print $0}' c_ll_ki_n013-c_ll_no_n008-c_ll_po_n008.per.unit.averages.per_specie.tsv  | wc -l
134260

# Tengo 134260 lineas que no tienen todos los campos, eso supone un 9,1% de total. 
# ¿A cuantas les faltan todos los campos y por tanto no ha perjudicados los cálculos? (A todas las que tengas informative sites ($11), igual a cero)

awk '{if ($18 == "") print $0}' c_ll_ki_n013-c_ll_no_n008-c_ll_po_n008.per.unit.averages.per_specie.tsv  | awk '{if ($11=="0") print $0}' | wc -l
133589

# Por tanto, de las 134260, a 133589 le faltan todos los datos, por tanto sólo hay 671 lineas con algunos valores sí y otros no. 

133589-134260=671

awk '{if ($18 == "") print $0}' c_ll_ki_n013-c_ll_no_n008-c_ll_po_n008.per.unit.averages.per_specie.tsv  | awk '{if ($11!="0") print $0}' | wc -l
671



# De estos, parece que hay muchos con sitios informativos=1, por lo que asumo que no puede calcular las tajimaD. 

awk '{if ($18 == "") print $0}' c_ll_ki_n013-c_ll_no_n008-c_ll_po_n008.per.unit.averages.per_specie.tsv  | awk '{if ($11!="0") print $0}' | head
lp23.s00005	1472211	1472211	0	-1	CDS	-	0	LYPA23C006624	LYPA23C006624P4_1_1	9.1722861064*10^-06	0.0000000000*10^00	3.7772361236*10^-06	0.0000000000*10^00		1	c_ll_ki_n013	c_ll
lp23.s00006	42798	43028	230	229	intron	+	0	LYPA23C020681	LYPA23C020681T1_intron_13	7.9066451013*10^-06	0.0000000000*10^00	3.1403854839*10^-06	0.0000000000*10^00		1	c_ll_ki_n013	c_ll
lp23.s00010	215823	215849	26	25	lncRNA	+

# Concretamente encontramos,

 awk '{if ($18 == "") print $0}' c_ll_ki_n013-c_ll_no_n008-c_ll_po_n008.per.unit.averages.per_specie.tsv  | awk '{if ($11!="0") print $0}' | awk '{if ($15="1") print $0}' | wc -l
671

# Todo cuadra. 

# 671 supoone un 0.045 del total, así que yo creo que no influye o no mucho en los resultados que tenog. En todo caso, como en algunos casos la coumna de informative site 01 está donde watterson debería estar, ahí si pero en el resto no. 


```


Representation per pop --> I rm this files, but I keep the script in case is useful. 

Este script nos devuelve una gráfica por cada población. Hay un segundo script que devuelve una gráfica para lynx lynx y otra para lynx pardinus. 

```{r}

library ("ggplot2")
library ("plyr")
library ("RColorBrewer")
library ("scales")
library ("magrittr")

# wd <- "/Users/marialucenaperez/Owncloud/publico/WG_diversity/ANGSD/sfs/diversity_per_unit"
wd <- "/home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/diversity_per_unit"
# wd <- "/Users/marialucenaperez/Desktop/Diversity_per_unit"

finsdiversity = list.files(path = wd,pattern="*per.unit.averages.tsv$")
INFORMATIVE_SITES=1
PERCENTAGE_COVERED=0.5

for (i in 1:length(finsdiversity))
{
  infile <- finsdiversity[i]
  name_diversity <- unlist(strsplit(finsdiversity[i], "[.]"))

#### Para que almacene varias dataframe con el nombre correcto
#  dataframename <- paste("data_diversity", name_diversity[1], sep ='_')
#  assign(dataframename, read.csv (paste(wd,infile, sep="/"), header = T, sep = '\t',stringsAsFactors = FALSE,  row.names=NULL, na.strings = "", dec=".", colClasses=c("watterson_ave"="character")))

dataframe <- read.csv (paste(wd,infile, sep="/"), header = T, sep = '\t',stringsAsFactors = FALSE,  row.names=NULL, na.strings = "", dec=".")

dataframe = dataframe[-1,]
dataframe$watterson_ave <- as.numeric(gsub("\\*10\\^","e",dataframe$watterson_ave))
dataframe$pairwise_ave <- as.numeric(gsub("\\*10\\^","e",dataframe$pairwise_ave))
dataframe$tajimaD <- as.numeric(gsub("\\*10\\^","e",dataframe$tajimaD))
#dataframe  <- dataframe[complete.cases(dataframe),] ## --> No se que pasa que la columna de specie sale NA

########################################################
# Filtering
dataframe <- dataframe %>% 
  mutate(percentage_covered=ifelse(NAs>0, (as.numeric(length)-as.numeric(NAs))/as.numeric(length), 1)) %>% dplyr::filter(percentage_covered>PERCENTAGE_COVERED) %>% dplyr::filter(informative_sites>INFORMATIVE_SITES)
name_diversity[1] <- paste(name_diversity[1],".percentagecovered",PERCENTAGE_COVERED,"_mininformativesites",INFORMATIVE_SITES, sep="")
########################################################

df.pm <- ddply(dataframe, "feature", summarise, weightedmean=weighted.mean(pairwise_ave, informative_sites))

df.wm <- ddply(dataframe, "feature", summarise, weightedmean=weighted.mean(watterson_ave, informative_sites))


# pdf(file = paste(wd,"/Diversity_by_feature_pairwise",name_diversity[1],".pdf", sep = ""))
ggplot(data = dataframe, aes(x=feature, y = pairwise_ave)) + 
  geom_boxplot()+
  geom_point(data=df.pm,aes(x=feature, y = weightedmean),shape = 23, 
             size = 3, inherit.aes=FALSE) +
  # scale_y_continuous(limits = c(-20,-3), expand =c(0,0), labels = comma) +
  facet_grid(.~feature, scales="free") +
  scale_y_continuous(trans = 'log10') +
  scale_x_discrete("Genomic feature") +
  xlab(label = "Genomic feature") + #x title
  ylab(label = "Pairwise average") + # y title
  theme_bw() +  #theme selection for background and lines
  # scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  ggtitle(paste ("Diversity by feature",name_diversity[1], sep = " " )) +
  theme(axis.line.x = element_line(color="black", size = 0.5),
        axis.line.y = element_line(color="black", size = 0.5),
        axis.text.x = element_text(angle = 60, vjust = 0.8, hjust = 1),
        axis.text.y = element_text(vjust = 0.2, hjust = 0.2),
        axis.title.y = element_text(margin=margin(r=0.3, unit="cm")),
        axis.title.x = element_text(margin=margin(t=0.5, unit="cm")),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.border = element_blank(),
        panel.background = element_blank(),
        strip.background = element_blank(),
        strip.text.x = element_blank())
ggsave (file = paste(wd,"/",name_diversity[1],".diversity_feature_pairwise.pdf", sep = ""), device = "pdf")
# dev.off()



# pdf(file = paste(wd,"/Diversity_by_feature_watterson",name_diversity[1],".pdf", sep = ""))

ggplot(data = dataframe, aes(x=feature, y = watterson_ave)) + 
  geom_boxplot()+
  geom_point(data=df.wm,aes(x=feature, y = weightedmean),shape = 23, 
             size = 3, inherit.aes=FALSE) +
  # scale_y_continuous(limits = c(-20,-3), expand =c(0,0), labels = comma) +
  facet_grid(.~feature, scales="free") +
  scale_y_continuous(trans = 'log10') +
  scale_x_discrete("Genomic feature") +
  xlab(label = "Genomic feature") + #x title
  ylab(label = "Watterson average") + # y title
  theme_bw() +  #theme selection for background and lines
  # scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  ggtitle(paste ("Diversity by feature",name_diversity[1], sep = " " )) +
  theme(axis.line.x = element_line(color="black", size = 0.5),
        axis.line.y = element_line(color="black", size = 0.5),
        axis.text.x = element_text(angle = 60, vjust = 0.8, hjust = 1),
        axis.text.y = element_text(vjust = 0.2, hjust = 0.2),
        axis.title.y = element_text(margin=margin(r=0.3, unit="cm")),
        axis.title.x = element_text(margin=margin(t=0.5, unit="cm")),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.border = element_blank(),
        panel.background = element_blank(),
        strip.background = element_blank(),
        strip.text.x = element_blank())

ggsave(filename = paste(wd,"/", name_diversity[1],".diversity_feature_watterson.pdf", sep = ""), device = "pdf")


ggplot(data = dataframe, aes(x=feature, y = tajimaD)) + 
  geom_boxplot()+
  # scale_y_continuous(limits = c(-20,-3), expand =c(0,0), labels = comma) +
  facet_grid(.~feature, scales="free") +
  #  scale_y_continuous(trans = 'log10') +
  scale_x_discrete("Genomic feature") +
  xlab(label = "Genomic feature") + #x title
  ylab(label = "Watterson average") + # y title
  theme_bw() +  #theme selection for background and lines
  # scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  ggtitle(paste ("Diversity by feature",name_diversity[1], sep = " " )) +
  theme(axis.line.x = element_line(color="black", size = 0.5),
        axis.line.y = element_line(color="black", size = 0.5),
        axis.text.x = element_text(angle = 60, vjust = 0.8, hjust = 1),
        axis.text.y = element_text(vjust = 0.2, hjust = 0.2),
        axis.title.y = element_text(margin=margin(r=0.3, unit="cm")),
        axis.title.x = element_text(margin=margin(t=0.5, unit="cm")),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.border = element_blank(),
        panel.background = element_blank(),
        strip.background = element_blank(),
        strip.text.x = element_blank())
ggsave(filename = paste(wd,"/",name_diversity[1],".percentagecovered",PERCENTAGE_COVERED,"_mininformativesites",INFORMATIVE_SITES,".diversity_feature_tajimaD.pdf", sep = ""), device = "pdf")


# Exploratory plots

jpeg(paste(wd,"/",name_diversity[1],".plot1.jpg", sep=""))
plot (dataframe$informative_sites, dataframe$tajimaD)
dev.off()

jpeg(paste(wd,"/",name_diversity[1],".plot2.jpg", sep=""))
plot (dataframe$pairwise_ave, dataframe$watterson_ave)
dev.off()

jpeg(paste(wd,"/",name_diversity[1],".plot3.jpg", sep=""))
plot (dataframe$pairwise_ave, dataframe$tajimaD)
dev.off()

jpeg(paste(wd,"/",name_diversity[1],"plot4.jpg", sep=""))
plot (dataframe$watterson_ave, dataframe$tajimaD)
dev.off()

jpeg(paste(wd,"/",name_diversity[1],"plot5.jpg", sep=""))
plot (dataframe$informative_sites, dataframe$pairwise_ave)
dev.off()

jpeg(paste(wd,"/",name_diversity[1],"plot6.jpg", sep=""))
plot (dataframe$informative_sites, dataframe$watterson_ave)
dev.off()


}


# Cosas útiles outliers.shape=NA.

```

Comparación por pares unidad


```{r}

library ("ggplot2")
library ("plyr")
library ("RColorBrewer")
library ("scales")
library ("magrittr")
library(dplyr)

# wd <- "/Users/marialucenaperez/Owncloud/publico/WG_diversity/ANGSD/sfs/diversity_per_unit"
wd <- "/Users/marialucenaperez/Owncloud/publico/WG_diversity/ANGSD/sfs/diversity_per_unit"
# wd <- "/Users/marialucenaperez/Desktop/Diversity_per_region"

finsdiversity = list.files(path = wd,pattern="*.per.unit.averages.tsv$")

for (i in 1:length(finsdiversity))
{
  infile <- finsdiversity[i]
  name_diversity <- unlist(strsplit(finsdiversity[i], "[.]"))
dataframe <- read.csv (paste(wd,infile, sep="/"), header = T, sep = '\t',stringsAsFactors = FALSE,  row.names=NULL, na.strings = "", dec=".")

dataframe = dataframe[-1,]
dataframe$watterson_ave <- as.numeric(gsub("\\*10\\^","e",dataframe$watterson_ave))
dataframe$pairwise_ave <- as.numeric(gsub("\\*10\\^","e",dataframe$pairwise_ave))
dataframe$tajimaD <- as.numeric(gsub("\\*10\\^","e",dataframe$tajimaD))

#### Para que almacene varias dataframe con el nombre correcto
  dataframename <- paste("data_diversity", name_diversity[1], sep ='_')
  assign(dataframename, dataframe)
  
  

}


pair_comparison <- full_join (data_diversity_c_ll_ki_n013, data_diversity_c_ll_no_n008, by=c("row.names"="row.names","scaffold"="scaffold","start"="start", "end"="end",  "feature"="feature", "strandness"="strandness", "frame"="frame", "id_gene"="id_gene", "id"="id")  )

colourCount = length(unique(pair_comparison$feature))
getPalette = colorRampPalette(brewer.pal(9, "Paired"))


ggplot (filter(pair_comparison), aes(x= watterson_ave.x, y=watterson_ave.y , fill=feature, colour=feature)) +
  geom_point(alpha=1/100) +
  geom_smooth(method = "lm", se = FALSE) +
  geom_abline(intercept = 0) +
  scale_fill_manual(values = getPalette(colourCount))+
  scale_colour_manual(values = getPalette(colourCount)) +
   theme(legend.position="bottom") +
  guides(fill=guide_legend(nrow=2))

```


Representation per specie

Esta gráfica representa por especie.

```{r}


# stat_summary(fun.y=mean,

library ("ggplot2")
library ("plyr")
library ("RColorBrewer")
library ("scales")
library ("magrittr")
library("tidyr")
library ("dplyr")
# wd <- "/Users/marialucenaperez/Owncloud/publico/WG_diversity/ANGSD/sfs/diversity_per_unit"
# wd <- "/home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/diversity_per_unit"
# wd <- "/Users/marialucenaperez/Desktop/Diversity_per_unit"

finsdiversity = list.files(path = wd,pattern="*per_specie.tsv$")
INFORMATIVE_SITES=1
PERCENTAGE_COVERED=0


for (i in 1:length(finsdiversity))
{
  infile <- finsdiversity[i]
  name_diversity <- unlist(strsplit(finsdiversity[i], "[.]"))
  dataframe <- read.csv (paste(wd,infile, sep="/"), header = T, sep = '\t',stringsAsFactors = FALSE,  row.names=NULL, na.strings = "", dec=".")
  dataframe = dataframe[-1,]
  dataframe$watterson_ave <- as.numeric(gsub("\\*10\\^","e",dataframe$watterson_ave))
  dataframe$pairwise_ave <- as.numeric(gsub("\\*10\\^","e",dataframe$pairwise_ave))
  dataframe$tajimaD <- as.numeric(gsub("\\*10\\^","e",dataframe$tajimaD))
  dataframe  <- dataframe[complete.cases(dataframe),]
  dataframe$pop <- as.factor(dataframe$pop)
  dataframe$pop <- factor(dataframe$pop, levels=c("c_ll_ki_n013", "c_ll_po_n008", "c_ll_no_n008", "c_lp_sm_n019", "c_lp_do_n012"))
  #### Para que almacene varias dataframe con el nombre correcto
  dataframename <- paste("data_diversity", name_diversity[1], sep ='_')
  assign(dataframename, dataframe)
  
  ########################################################
# Filtering
  
  dataframe <- dataframe %>% 
  mutate(percentage_covered=ifelse(NAs>0, (as.numeric(length)-as.numeric(NAs))/as.numeric(length), 1)) %>% dplyr::filter(percentage_covered>PERCENTAGE_COVERED) %>% dplyr::filter(informative_sites>INFORMATIVE_SITES)
  
}



### LYNX PARDINUS #### 

dataframe <- `data_diversity_c_lp_do_n012-c_lp_sm_n019`
name_diversity[1] <- "c_lp_do_n012-c_lp_sm_n019"
name_diversity[1] <- paste(name_diversity[1],".percentagecovered",PERCENTAGE_COVERED,"_mininformativesites",INFORMATIVE_SITES, sep="")


# Calculo weighted mean y lo guardo en un df.

df.pm <- ddply(dataframe, c("feature","specie","pop"), summarise, weightedmean=weighted.mean(pairwise_ave, informative_sites), mean=mean(pairwise_ave))

df.pm1 <- df.pm %>% select (feature,pop, weightedmean) %>% spread (., pop, weightedmean) %>% select (feature, wm_sm= c_lp_sm_n019,wm_do=c_lp_do_n012)
df.pm2 <- df.pm %>% select (feature,pop, mean) %>% spread (., pop, mean) %>% select (feature, m_sm= c_lp_sm_n019,m_do=c_lp_do_n012)

df.pm_guardar <- full_join(df.pm1, df.pm2)


df.wm <- ddply(dataframe, c("feature","specie","pop"), summarise, weightedmean=weighted.mean(watterson_ave, informative_sites), mean=mean(watterson_ave))

df.wm1 <- df.pm %>% select (feature,pop, weightedmean) %>% spread (., pop, weightedmean) %>% select (feature, wm_sm= c_lp_sm_n019,wm_do=c_lp_do_n012)
df.wm2 <- df.pm %>% select (feature,pop, mean) %>% spread (., pop, mean) %>% select (feature, m_sm= c_lp_sm_n019,m_do=c_lp_do_n012)

df.wm_guardar <- full_join(df.pm1, df.pm2)


# LYNX PARDINUS
# Hago spread para calcular el porcentage que representa do respecto a sm
df.pm_lost <- df.pm %>% select(.,-mean) %>% tidyr::spread(., pop, weightedmean) %>%
 mutate(., percentage_do_sm=(c_lp_do_n012 *100)/c_lp_sm_n019)
# Hago gathered para poder plotearlo.
df.pm.gathered <- tidyr::gather(df.pm_lost, "pop", "weightedmean", -specie, -feature, -percentage_do_sm)
# Hago spread para calcular el porcentage de perdida
df.wm_lost <-df.wm %>% select(.,-mean) %>% tidyr::spread(., pop, weightedmean) %>% mutate(., percentage_do_sm=(c_lp_do_n012 *100)/c_lp_sm_n019)
# Hago gathered para poder plotearlo.
df.wm.gathered <- tidyr::gather(df.wm_lost, "pop", "weightedmean", -specie, -feature, -percentage_do_sm)


# Guardo el porcentage de perdida. 
write.table(df.pm_guardar, paste(wd,"/", name_diversity[1],".pairwise_mean.csv", sep = ""), row.names = F, dec=",", sep=";")
write.table(df.wm_guardar, paste(wd,"/", name_diversity[1],".watterson_mean.csv", sep = ""), row.names = F, dec=",", sep=";")


ggplot(data = dataframe, aes(x=feature, y = pairwise_ave, fill=pop)) +
  geom_boxplot() +
  geom_point(data=df.pm,aes(x=feature, y = weightedmean, fill=pop),shape = 23, position=position_dodge(width=0.75), size = 3, inherit.aes=FALSE) +
  facet_grid(.~feature, scales="free") +
  scale_y_continuous(trans = 'log10') +
  scale_x_discrete("Genomic feature") +
  xlab(label = "Genomic feature") + #x title
  ylab(label = "Pairwise average") + # y title
  theme_bw() +  #theme selection for background and lines
  # scale_x_continuous(expand = c(0, 1)) +# + scale_y_continuous(expand = c(0, 0)) +
  ggtitle(paste ("Diversity feature",name_diversity[1], sep = " " )) +
  theme(axis.line.x = element_line(color="black", size = 0.5),
        axis.line.y = element_line(color="black", size = 0.5),
        axis.text.x = element_text(angle = 60, vjust = 0.8, hjust = 1),
        axis.text.y = element_text(vjust = 0.2, hjust = 0.2),
        axis.title.y = element_text(margin=margin(r=0.3, unit="cm")),
        axis.title.x = element_text(margin=margin(t=0.5, unit="cm")),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.border = element_blank(),
        panel.background = element_blank(),
        strip.background = element_blank(),
        strip.text.x = element_blank())+ 
        stat_summary(fun.y=mean, colour="darkred", geom="point", shape=18, size=3,show_guide = FALSE) + 
  geom_text(data = df.pm.gathered, aes(feature, percentage_do_sm, label=(percentage_do_sm %>% round(2))),angle = 45, size=2)

ggsave (file = paste(wd,"/",name_diversity[1],".diversity_feature_pairwise.eps", sep = ""), device = "eps")
# dev.off()


ggplot(data = dataframe, aes(x=feature, y = watterson_ave, fill =pop)) + 
  geom_boxplot(outlier.shape=NA)+
  geom_point(data=df.wm,aes(x=feature, y = weightedmean, fill=pop),shape = 23, position=position_dodge(width=0.75), size = 3, inherit.aes=FALSE) +
  # scale_y_continuous(limits = c(-20,-3), expand =c(0,0), labels = comma) +
  facet_grid(.~feature, scales="free") +
  scale_y_continuous(trans = 'log10') +
  scale_x_discrete("Genomic feature") +
  xlab(label = "Genomic feature") + #x title
  ylab(label = "Watterson average") + # y title
  theme_bw() +  #theme selection for background and lines
  # scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  ggtitle(paste ("Diversity by feature",name_diversity[1], sep = " " )) +
  theme(axis.line.x = element_line(color="black", size = 0.5),
        axis.line.y = element_line(color="black", size = 0.5),
        axis.text.x = element_text(angle = 60, vjust = 0.8, hjust = 1),
        axis.text.y = element_text(vjust = 0.2, hjust = 0.2),
        axis.title.y = element_text(margin=margin(r=0.3, unit="cm")),
        axis.title.x = element_text(margin=margin(t=0.5, unit="cm")),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.border = element_blank(),
        panel.background = element_blank(),
        strip.background = element_blank(),
        strip.text.x = element_blank()) + 
        stat_summary(fun.y=mean, colour="darkred", geom="point", shape=18, size=3,show_guide = FALSE) + 
  geom_text(data = df.wm.gathered, aes(feature, percentage_do_sm, label=(percentage_do_sm %>% round(2))),angle = 45, size=2)
ggsave(filename = paste(wd,"/",name_diversity[1],".diversity_feature_watterson.eps", sep = ""), device="eps")



### LYNX LYNX ####

dataframe <- `data_diversity_c_ll_ki_n013-c_ll_no_n008-c_ll_po_n008`
name_diversity[1] <- "c_ll_ki_n013-c_ll_no_n008-c_ll_po_n008"
name_diversity[1] <- paste(name_diversity[1],".percentagecovered",PERCENTAGE_COVERED,"_mininformativesites",INFORMATIVE_SITES, sep="")

# Calculo weighted mean y lo guardo en un df.

df.pm <- ddply(dataframe, c("feature","specie","pop"), summarise, weightedmean=weighted.mean(pairwise_ave, informative_sites), mean=mean(pairwise_ave))

df.pm1 <- df.pm %>% select (feature,pop, weightedmean) %>% spread (., pop, weightedmean) %>% select (feature, wm_ki= c_ll_ki_n013,wm_no=c_ll_no_n008,wm_po=c_ll_po_n008)
df.pm2 <- df.pm %>% select (feature,pop, mean) %>% spread (., pop, mean) %>% select (feature, m_ki= c_ll_ki_n013,m_no=c_ll_no_n008,m_po=c_ll_po_n008)

df.pm_guardar <- full_join(df.pm1, df.pm2)


df.wm <- ddply(dataframe, c("feature","specie","pop"), summarise, weightedmean=weighted.mean(watterson_ave, informative_sites), mean=mean(watterson_ave))

df.wm1 <- df.wm %>% select (feature,pop, weightedmean) %>% spread (., pop, weightedmean) %>% select (feature, wm_ki= c_ll_ki_n013,wm_no=c_ll_no_n008,wm_po=c_ll_po_n008)
df.wm2 <- df.wm %>% select (feature,pop, mean) %>% spread (., pop, mean) %>% select (feature, m_ki= c_ll_ki_n013,m_no=c_ll_no_n008,m_po=c_ll_po_n008)

df.wm_guardar <- full_join(df.pm1, df.pm2)


df.pm_lost <- df.pm %>% select(.,-mean) %>% tidyr::spread(., pop, weightedmean) %>% 
  mutate(., percentage_po_ki=(c_ll_po_n008 *100)/c_ll_ki_n013) %>% 
  mutate(., percentage_no_ki=(c_ll_no_n008*100)/c_ll_ki_n013)
df.pm.gathered <- tidyr::gather(df.pm_lost, "pop", "weightedmean", -specie, -feature,  -percentage_po_ki, -percentage_no_ki)

df.wm_lost <- df.wm %>% select(.,-mean) %>% tidyr::spread(., pop, weightedmean) %>% 
  mutate(., percentage_po_ki=(c_ll_po_n008 *100)/c_ll_ki_n013) %>% 
  mutate(., percentage_no_ki=(c_ll_no_n008*100)/c_ll_ki_n013)
df.wm.gathered <- tidyr::gather(df.wm_lost, "pop", "weightedmean", -specie, -feature,  -percentage_po_ki, -percentage_no_ki) 

# Guardo el porcentage de perdida. 
write.table(df.pm_guardar, paste(wd,"/", name_diversity[1],".pairwise_mean.csv", sep = ""), row.names = F, dec=",", sep=";")
write.table(df.wm_guardar, paste(wd,"/", name_diversity[1],".watterson_mean.csv", sep = ""), row.names = F, dec=",", sep=";")



ggplot(data = dataframe, aes(x=feature, y = pairwise_ave, fill=pop)) +
  geom_boxplot(outlier.shape=NA)+
  geom_point(data=df.pm,aes(x=feature, y = weightedmean, fill=pop),shape = 23, position=position_dodge(width=0.75), size = 3, inherit.aes=FALSE) +
  # scale_y_continuous(limits = c(-20,-3), expand =c(0,0), labels = comma) +
  facet_grid(.~feature, scales="free") +
  scale_y_continuous(trans = 'log10') +
  scale_x_discrete("Genomic feature") +
  xlab(label = "Genomic feature") + #x title
  ylab(label = "Pairwise average") + # y title
  theme_bw() +  #theme selection for background and lines
  #scale_x_continuous(expand = c(0, 1)) +# + scale_y_continuous(expand = c(0, 0)) +
  ggtitle(paste ("Diversity feature",name_diversity[1], sep = " " )) +
  theme(axis.line.x = element_line(color="black", size = 0.5),
        axis.line.y = element_line(color="black", size = 0.5),
        axis.text.x = element_text(angle = 60, vjust = 0.8, hjust = 1),
        axis.text.y = element_text(vjust = 0.2, hjust = 0.2),
        axis.title.y = element_text(margin=margin(r=0.3, unit="cm")),
        axis.title.x = element_text(margin=margin(t=0.5, unit="cm")),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.border = element_blank(),
        panel.background = element_blank(),
        strip.background = element_blank(),
        strip.text.x = element_blank())+ 
        stat_summary(fun.y=mean, colour="darkred", geom="point", shape=18, size=3,show_guide = FALSE) + 
  geom_text(data = df.pm.gathered, aes(feature, percentage_po_ki, label=paste((percentage_po_ki %>% round(2)),(percentage_no_ki%>% round(2)), sep='\n')), angle = 45, size=2, vjust="inward",hjust="inward")
ggsave (file = paste(wd,"/",name_diversity[1],".diversity_feature_pairwise.eps", sep = ""), device = "eps")
# dev.off()



ggplot(data = dataframe, aes(x=feature, y = watterson_ave, fill =pop)) + 
  geom_boxplot(outlier.shape=NA, position=position_dodge(width=1.5))+
  geom_point(data=df.wm,aes(x=feature, y = weightedmean, fill=pop),shape = 23, position=position_dodge(width=1.5), size = 3, inherit.aes=FALSE) +
  facet_grid(.~feature, scales="free") +
  scale_y_continuous(trans = 'log10') +
  scale_x_discrete("Genomic feature") +
  xlab(label = "Genomic feature") + #x title
  ylab(label = "Watterson average") + # y title
  theme_bw() +  #theme selection for background and lines
  # scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  ggtitle(paste ("Diversity by feature",name_diversity[1], sep = " " )) +
  theme(axis.line.x = element_line(color="black", size = 0.5),
        axis.line.y = element_line(color="black", size = 0.5),
        axis.text.x = element_text(angle = 60, vjust = 0.8, hjust = 1),
        axis.text.y = element_text(vjust = 0.2, hjust = 0.2),
        axis.title.y = element_text(margin=margin(r=0.3, unit="cm")),
        axis.title.x = element_text(margin=margin(t=0.5, unit="cm")),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.border = element_blank(),
        panel.background = element_blank(),
        strip.background = element_blank(),
        strip.text.x = element_blank()) + 
        stat_summary(fun.y=mean, colour="darkred", geom="point", shape=18, size=3,show_guide = FALSE) + 
        geom_text(data = df.wm.gathered, aes(feature, percentage_po_ki, label=paste((percentage_po_ki %>% round(2)),(percentage_no_ki%>% round(2)), sep='\n')), angle = 45, size=2, vjust="inward",hjust="inward")

ggsave(filename = paste(wd,"/",name_diversity[1],".diversity_feature_watterson.eps", sep =""),device="eps")


# Cosas útiles 

# geom_box(outliers.shape=NA)

# Si quiero plotear todos los indices a la vez:
#index = c("watterson_ave", "pairwise_ave")
#for (i in 1:length(index))
#{
#  ggplot(data = dataframe, aes_string(x='feature', y = index[1], fill='pop'))+



```

 Pairwise Pi diversity

```{r}


library ("ggplot2")
library ("plyr")
library ("dplyr")
library ("RColorBrewer")
library ("scales")
library ("magrittr")
library("tidyr")
library(viridis)


# wd <- "/Users/marialucenaperez/Owncloud/publico/WG_diversity/ANGSD/sfs/diversity_per_unit"
# wd <- "/home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/diversity_per_unit"
wd <- "/Users/marialucenaperez/Desktop/"


# finsdiversity = list.files(path = wd,pattern="*per.unit.averages.per_specie.tsv$") %>% grep("informative_sites", ., invert = T, value=T)

infile <- "c_ll_ki_n013-c_ll_no_n008-c_ll_po_n008.per.unit.averages.per_specie.tsv"

INFORMATIVE_SITES=1
PERCENTAGE_COVERED=0




# for (i in 1:length(finsdiversity))
# {
#  infile <- finsdiversity[i]
  
  dataframe <- read.csv (paste (wd,infile, sep="/"), header = T, sep = '\t', stringsAsFactors = FALSE,  row.names=NULL, na.strings = "", dec=".") 
  dataframe$watterson_ave <- as.numeric(gsub("\\*10\\^","e",dataframe$watterson_ave))
  dataframe$pairwise_ave <- as.numeric(gsub("\\*10\\^","e",dataframe$pairwise_ave))
  dataframe$tajimaD <- as.numeric(gsub("\\*10\\^","e",dataframe$tajimaD))
# Filtering
  dataframe <- dataframe %>% 
  mutate(percentage_covered=ifelse(NAs>0, (as.numeric(length)-as.numeric(NAs))/as.numeric(length), 1)) %>% dplyr::filter(percentage_covered>PERCENTAGE_COVERED) %>% dplyr::filter(informative_sites>INFORMATIVE_SITES)
  

  
# scaffold_start_end_feature_id, length NAs feature strandness frame id_gene watterson_ave watterson_sd pairwise_ave pairwise_sdtajimaD informative_sites pop specie length NAs
distinct_pop <- dataframe %>% distinct(pop)
distinct_vector_pop = distinct_pop$pop

# distinct_vector_pop=c("don","smo","po","cat","bia")

combinations <- t(combn(distinct_vector_pop, 2)) # %>% mutate (combination=paste(V1,V2,sep="/")) %>% select(combination)

# for(a in 1:nrow(combinations)) {
#    row <- NULL
    row <- combinations[a,]
    # do stuff with row
    POP1 <- NULL
    POP2 <- NULL
    POP1 <- row[1]
    POP2 <- row[2]
    NAME <- paste(POP1,"_",POP2,".percentagecovered",PERCENTAGE_COVERED,"_mininformativesites",INFORMATIVE_SITES, sep="")

    
    df_POP1 <- filter (dataframe, pop == POP1) %>% mutate(., feature2=feature) %>% unite (., scaffold, start, end, feature2, id, col="scaffold_start_end_feature_id")
    
    df_POP2 <- filter (dataframe, pop == POP2) %>% mutate(., feature2=feature) %>% unite (., scaffold, start, end, feature2, id, col="scaffold_start_end_feature_id") 
    

    vector_positions_POP1 <- unique(df_POP1$scaffold_start_end_feature_id) %>% sort(.)
    vector_positions_POP2 <- unique(df_POP2$scaffold_start_end_feature_id) %>% sort(.)
    
    vector_common_positions <- as.vector(intersect(vector_positions_POP1, vector_positions_POP2))
    
    df_POP1_filtered <- df_POP1 %>% filter(scaffold_start_end_feature_id  %in% vector_common_positions) %>% arrange(., scaffold_start_end_feature_id) %>% select (., scaffold_start_end_feature_id, feature, id_gene, pairwise_ave, informative_sites, length, NAs, pop )
    
    df_POP2_filtered <- df_POP2 %>% filter(scaffold_start_end_feature_id  %in% vector_common_positions) %>% arrange(., scaffold_start_end_feature_id) %>% select (., scaffold_start_end_feature_id, feature, id_gene, pairwise_ave, informative_sites, length, NAs, pop )

if (identical(df_POP1_filtered$scaffold_start_end_feature_id, df_POP2_filtered$scaffold_start_end_feature_id))
{ df_pairwise <- cbind (
  setNames(data.frame(df_POP1_filtered), c("scaffold_start_end_feature_id", "feature", "id_gene", "pairwise_ave1","informative_sites1", "length1", "NAs1", "pop1")),
  setNames(data.frame(df_POP2_filtered), c("scaffold_start_end_feature_id2", "feature2", "id_gene2", "pairwise_ave2","informative_sites2", "length2", "NAs2", "pop2"))) %>% select_ (., "scaffold_start_end_feature_id", "feature", "id_gene", "pairwise_ave1","informative_sites1", "length1", "NAs1", "pop1","pairwise_ave2","informative_sites2", "length2", "NAs2", "pop2")
           } else {print("First column not matching") }

    
ggplot(df_pairwise, aes(informative_sites1, informative_sites2)) +
  geom_point() +
  labs(x=paste("Informative sites", POP1, sep =" "),y=paste("Informative sites", POP2, sep =" ")) +
  theme_classic()

ggsave(filename = paste(wd,"/",NAME,"_informativesites.eps", sep = ""), device="eps")


ggplot(df_pairwise, aes(pairwise_ave1, pairwise_ave2,  color = informative_sites1)) +
  geom_point() +
  facet_wrap(~feature) +
  # scale_color_gradient(low="red", high="blue") +
  #scale_color_gradientn(colours = rainbow(5)) +
  # scale_colour_gradient(limits = c(0, quantile(df_pairwise$informative_sites1, 0.5))) +
  # scale_colour_gradientn(colours = myPalette(100), values=nrm.vals) +
  labs(colour = paste("Informative sites", POP1, sep =" ")) +
  labs(x=paste("Pi diversity", POP1, sep =" "),y=paste("Pi diversity", POP2, sep =" ")) +
  scale_color_viridis(option="magma",limits = c(0, quantile(df_pairwise$informative_sites1, 0.5))) +
  theme_classic()

ggsave(filename = paste(wd,"/",NAME,"_informative_sites1_Pi_pairwise.eps", sep = ""), device="eps")

}

}

# https://cran.r-project.org/web/packages/viridis/vignettes/intro-to-viridis.html

```


Representation per pop

Este script nos devuelve una gráfica por cada población. Hay un segundo script que devuelve una gráfica para lynx lynx y otra para lynx pardinus. 



```{r}

library ("ggplot2")
library ("plyr")
library ("RColorBrewer")
library ("scales")
library ("magrittr")

# wd <- "/Users/marialucenaperez/Owncloud/publico/WG_diversity/ANGSD/sfs/diversity_per_unit"
wd <- "/home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/diversity_per_region"
# wd <- "/Users/marialucenaperez/Desktop/Diversity_per_region"

finsdiversity = list.files(path = wd,pattern="*.per.region.averages.tsv$")

for (i in 1:length(finsdiversity))
{
  infile <- finsdiversity[i]
  name_diversity <- unlist(strsplit(finsdiversity[i], "[.]"))

#### Para que almacene varias dataframe con el nombre correcto
#  dataframename <- paste("data_diversity", name_diversity[1], sep ='_')
#  assign(dataframename, read.csv (paste(wd,infile, sep="/"), header = T, sep = '\t',stringsAsFactors = FALSE,  row.names=NULL, na.strings = "", dec=".", colClasses=c("watterson_ave"="character")))

dataframe <- read.csv (paste(wd,infile, sep="/"), header = T, sep = '\t',stringsAsFactors = FALSE,  row.names=NULL, na.strings = "", dec=".")

dataframe = dataframe[-1,]
dataframe$watterson_ave <- as.numeric(gsub("\\*10\\^","e",dataframe$watterson_ave))
dataframe$pairwise_ave <- as.numeric(gsub("\\*10\\^","e",dataframe$pairwise_ave))
dataframe$tajimaD <- as.numeric(gsub("\\*10\\^","e",dataframe$tajimaD))
#dataframe  <- dataframe[complete.cases(dataframe),] ## --> No se que pasa que la columna de specie sale NA

########################################################
# Filtering
dataframe <- dataframe %>% 
  mutate(percentage_covered=ifelse(NAs>0, (as.numeric(length)-as.numeric(NAs))/as.numeric(length), 1)) %>% dplyr::filter(percentage_covered>0.5) %>% dplyr::filter(informative_sites>10)
name_diversity[1] <- paste(name_diversity[1],".percentagecovered05_mininformativesites10", sep="")
########################################################

ddply(dataframe, "feature", summarise, weightedmean=mean(pairwise_ave))
df.pm <- ddply(dataframe, "feature", summarise, weightedmean=weighted.mean(pairwise_ave, informative_sites))


ddply(dataframe, "feature", summarise, weightedmean=mean(watterson_ave))
df.wm <- ddply(dataframe, "feature", summarise, weightedmean=weighted.mean(watterson_ave, informative_sites))


# pdf(file = paste(wd,"/Diversity_by_region_pairwise",name_diversity[1],".pdf", sep = ""))
ggplot(data = dataframe, aes(x=feature, y = pairwise_ave)) + 
  geom_boxplot()+
  geom_point(data=df.pm,aes(x=feature, y = weightedmean),shape = 23, 
             size = 3, inherit.aes=FALSE) +
  # scale_y_continuous(limits = c(-20,-3), expand =c(0,0), labels = comma) +
  facet_grid(.~feature, scales="free") +
  scale_y_continuous(trans = 'log10') +
  scale_x_discrete("Genomic region") +
  xlab(label = "Genomic region") + #x title
  ylab(label = "Pairwise average") + # y title
  theme_bw() +  #theme selection for background and lines
  # scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  ggtitle(paste ("Diversity by region",name_diversity[1], sep = " " )) +
  theme(axis.line.x = element_line(color="black", size = 0.5),
        axis.line.y = element_line(color="black", size = 0.5),
        axis.text.x = element_text(angle = 60, vjust = 0.8, hjust = 1),
        axis.text.y = element_text(vjust = 0.2, hjust = 0.2),
        axis.title.y = element_text(margin=margin(r=0.3, unit="cm")),
        axis.title.x = element_text(margin=margin(t=0.5, unit="cm")),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.border = element_blank(),
        panel.background = element_blank(),
        strip.background = element_blank(),
        strip.text.x = element_blank())
ggsave (file = paste(wd,"/",name_diversity[1],".diversity_region_pairwise.pdf", sep = ""), device = "pdf")
# dev.off()



# pdf(file = paste(wd,"/Diversity_by_region_watterson",name_diversity[1],".pdf", sep = ""))

ggplot(data = dataframe, aes(x=feature, y = watterson_ave)) + 
  geom_boxplot()+
  geom_point(data=df.wm,aes(x=feature, y = weightedmean),shape = 23, 
             size = 3, inherit.aes=FALSE) +
  # scale_y_continuous(limits = c(-20,-3), expand =c(0,0), labels = comma) +
  facet_grid(.~feature, scales="free") +
  scale_y_continuous(trans = 'log10') +
  scale_x_discrete("Genomic region") +
  xlab(label = "Genomic region") + #x title
  ylab(label = "Watterson average") + # y title
  theme_bw() +  #theme selection for background and lines
  # scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  ggtitle(paste ("Diversity by region",name_diversity[1], sep = " " )) +
  theme(axis.line.x = element_line(color="black", size = 0.5),
        axis.line.y = element_line(color="black", size = 0.5),
        axis.text.x = element_text(angle = 60, vjust = 0.8, hjust = 1),
        axis.text.y = element_text(vjust = 0.2, hjust = 0.2),
        axis.title.y = element_text(margin=margin(r=0.3, unit="cm")),
        axis.title.x = element_text(margin=margin(t=0.5, unit="cm")),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.border = element_blank(),
        panel.background = element_blank(),
        strip.background = element_blank(),
        strip.text.x = element_blank())

ggsave(filename = paste(wd,"/", name_diversity[1], ".diversity_region_watterson.pdf", sep = ""), device = "pdf")


ggplot(data = dataframe, aes(x=feature, y = tajimaD)) + 
  geom_boxplot()+
  # scale_y_continuous(limits = c(-20,-3), expand =c(0,0), labels = comma) +
  facet_grid(.~feature, scales="free") +
  #  scale_y_continuous(trans = 'log10') +
  scale_x_discrete("Genomic region") +
  xlab(label = "Genomic region") + #x title
  ylab(label = "Watterson average") + # y title
  theme_bw() +  #theme selection for background and lines
  # scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  ggtitle(paste ("Diversity by region",name_diversity[1], sep = " " )) +
  theme(axis.line.x = element_line(color="black", size = 0.5),
        axis.line.y = element_line(color="black", size = 0.5),
        axis.text.x = element_text(angle = 60, vjust = 0.8, hjust = 1),
        axis.text.y = element_text(vjust = 0.2, hjust = 0.2),
        axis.title.y = element_text(margin=margin(r=0.3, unit="cm")),
        axis.title.x = element_text(margin=margin(t=0.5, unit="cm")),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.border = element_blank(),
        panel.background = element_blank(),
        strip.background = element_blank(),
        strip.text.x = element_blank())
ggsave(filename = paste(wd,"/",name_diversity[1],".diversity_region_tajimaD.pdf", sep = ""), device = "pdf")


# Exploratory plots

jpeg(paste(wd,"/",name_diversity[1],".plot1.jpg", sep=""))
plot (dataframe$informative_sites, dataframe$tajimaD)
dev.off()

jpeg(paste(wd,"/",name_diversity[1],".plot2.jpg", sep=""))
plot (dataframe$pairwise_ave, dataframe$watterson_ave)
dev.off()

jpeg(paste(wd,"/",name_diversity[1],".plot3.jpg", sep=""))
plot (dataframe$pairwise_ave, dataframe$tajimaD)
dev.off()

jpeg(paste(wd,"/",name_diversity[1],"plot4.jpg", sep=""))
plot (dataframe$watterson_ave, dataframe$tajimaD)
dev.off()

jpeg(paste(wd,"/",name_diversity[1],"plot5.jpg", sep=""))
plot (dataframe$informative_sites, dataframe$pairwise_ave)
dev.off()

jpeg(paste(wd,"/",name_diversity[1],"plot6.jpg", sep=""))
plot (dataframe$informative_sites, dataframe$watterson_ave)
dev.off()


}


# Cosas útiles outliers.shape=NA.

```

Representation per specie

Esta gráfica representa por especie.

```{r}

library ("ggplot2")
library ("plyr")
library ("RColorBrewer")
library ("scales")
library ("magrittr")
library("tidyr")

# wd <- "/Users/marialucenaperez/Owncloud/publico/WG_diversity/ANGSD/sfs/diversity_per_unit"
wd <- "/home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/diversity_per_region"
# wd <- "/Users/marialucenaperez/Desktop/Diversity_per_unit"

finsdiversity = list.files(path = wd,pattern="*per_specie.tsv$")


for (i in 1:length(finsdiversity))
{
  infile <- finsdiversity[i]
  name_diversity <- unlist(strsplit(finsdiversity[i], "[.]"))
  dataframe <- read.csv (paste(wd,infile, sep="/"), header = T, sep = '\t',stringsAsFactors = FALSE,  row.names=NULL, na.strings = "", dec=".")
  dataframe = dataframe[-1,]
  dataframe$watterson_ave <- as.numeric(gsub("\\*10\\^","e",dataframe$watterson_ave))
  dataframe$pairwise_ave <- as.numeric(gsub("\\*10\\^","e",dataframe$pairwise_ave))
  dataframe$tajimaD <- as.numeric(gsub("\\*10\\^","e",dataframe$tajimaD))
  dataframe  <- dataframe[complete.cases(dataframe),]
  dataframe$pop <- as.factor(dataframe$pop)
  dataframe$pop <- factor(dataframe$pop, levels=c("c_ll_ki_n013", "c_ll_po_n008", "c_ll_no_n008", "c_lp_sm_n019", "c_lp_do_n012"))
  #### Para que almacene varias dataframe con el nombre correcto
  dataframename <- paste("data_diversity", name_diversity[1], sep ='_')
  assign(dataframename, dataframe)
}


##### Lynx pardinus


dataframe <- `data_diversity_c_lp_do_n012-c_lp_sm_n019`
name_diversity[1] <- "c_lp_do_n012-c_lp_sm_n019"


# Calculo weighted mean y lo guardo en un df.

df.pm <- ddply(dataframe, c("feature","specie","pop"), summarise, weightedmean=weighted.mean(pairwise_ave, informative_sites), mean=mean(pairwise_ave))
df.wm <- ddply(dataframe, c("feature","specie","pop"), summarise, weightedmean=weighted.mean(watterson_ave, informative_sites), mean=mean(watterson_ave))



# LYNX PARDINUS
# Hago spread para calcular el porcentage que representa do respecto a sm
df.pm_lost <- df.pm %>%  select (., -mean) %>% tidyr::spread(., pop, weightedmean) %>%
 mutate(., percentage_do_sm=(c_lp_do_n012 *100)/c_lp_sm_n019)
# Hago gathered para poder plotearlo.
df.pm.gathered <- tidyr::gather(df.pm_lost, "pop", "weightedmean", -specie, -feature, -percentage_do_sm)

# Hago spread para calcular el porcentage de perdida
df.wm_lost <- df.pm %>%  select (., -mean) %>% tidyr::spread(., pop, weightedmean) %>% mutate(., percentage_do_sm=(c_lp_do_n012 *100)/c_lp_sm_n019)
# Hago gathered para poder plotearlo.
df.wm.gathered <- tidyr::gather(df.wm_lost, "pop", "weightedmean", -specie, -feature, -percentage_do_sm)


# Guardo el porcentage de perdida. 
write.csv(df.pm, paste(wd,"/", name_diversity[1],".pairwise_mean.csv", sep = ""), row.names = F)
write.csv(df.wm, paste(wd,"/", name_diversity[1],".watterson_mean.csv", sep = ""), row.names = F)


ggplot(data = dataframe, aes(x=feature, y = pairwise_ave, fill=pop)) +
  geom_boxplot(outlier.shape=NA)+
  geom_point(data=df.pm,aes(x=feature, y = weightedmean, fill=pop),shape = 23, position=position_dodge(width=0.75), size = 3, inherit.aes=FALSE) +
  facet_grid(.~feature, scales="free") +
  scale_y_continuous(trans = 'log10') +
  scale_x_discrete("Genomic region") +
  xlab(label = "Genomic region") + #x title
  ylab(label = "Pairwise average") + # y title
  theme_bw() +  #theme selection for background and lines
 # scale_x_continuous(expand = c(0, 1)) +# + scale_y_continuous(expand = c(0, 0)) +
  ggtitle(paste ("Diversity region",name_diversity[1], sep = " " )) +
  theme(axis.line.x = element_line(color="black", size = 0.5),
        axis.line.y = element_line(color="black", size = 0.5),
        axis.text.x = element_text(angle = 60, vjust = 0.8, hjust = 1),
        axis.text.y = element_text(vjust = 0.2, hjust = 0.2),
        axis.title.y = element_text(margin=margin(r=0.3, unit="cm")),
        axis.title.x = element_text(margin=margin(t=0.5, unit="cm")),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.border = element_blank(),
        panel.background = element_blank(),
        strip.background = element_blank(),
        strip.text.x = element_blank())+ 
        stat_summary(fun.y=mean, colour="darkred", geom="point", shape=18, size=3,show_guide = FALSE) + 
  geom_text(data = df.pm.gathered, aes(feature, percentage_do_sm, label=(percentage_do_sm %>% round(2))),angle = 45, size=2)
ggsave (file = paste(wd,"/",name_diversity[1],".diversity_region_pairwise.eps", sep = ""), device = "eps")
# dev.off()


ggplot(data = dataframe, aes(x=feature, y = watterson_ave, fill =pop)) + 
  geom_boxplot(outlier.shape=NA)+
  geom_point(data=df.wm,aes(x=feature, y = weightedmean, fill=pop),shape = 23, position=position_dodge(width=0.75), size = 3, inherit.aes=FALSE) +
  # scale_y_continuous(limits = c(-20,-3), expand =c(0,0), labels = comma) +
  facet_grid(.~feature, scales="free") +
  scale_y_continuous(trans = 'log10') +
  scale_x_discrete("Genomic region") +
  xlab(label = "Genomic region") + #x title
  ylab(label = "Watterson average") + # y title
  theme_bw() +  #theme selection for background and lines
  # scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  ggtitle(paste ("Diversity by region",name_diversity[1], sep = " " )) +
  theme(axis.line.x = element_line(color="black", size = 0.5),
        axis.line.y = element_line(color="black", size = 0.5),
        axis.text.x = element_text(angle = 60, vjust = 0.8, hjust = 1),
        axis.text.y = element_text(vjust = 0.2, hjust = 0.2),
        axis.title.y = element_text(margin=margin(r=0.3, unit="cm")),
        axis.title.x = element_text(margin=margin(t=0.5, unit="cm")),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.border = element_blank(),
        panel.background = element_blank(),
        strip.background = element_blank(),
        strip.text.x = element_blank()) + 
        stat_summary(fun.y=mean, colour="darkred", geom="point", shape=18, size=3,show_guide = FALSE) + 
  geom_text(data = df.wm.gathered, aes(feature, percentage_do_sm, label=(percentage_do_sm %>% round(2))),angle = 45, size=2)
ggsave(filename = paste(wd,"/",name_diversity[1],".diversity_region_watterson.eps", sep = ""), device="eps")

#### Lynx lynx

dataframe <- `data_diversity_c_ll_ki_n013-c_ll_no_n008-c_ll_po_n008`
name_diversity[1] <- "c_ll_ki_n013-c_ll_no_n008-c_ll_po_n008"

# Calculo weighted mean y lo guardo en un df.

df.pm <- ddply(dataframe, c("feature","specie","pop"), summarise, weightedmean=weighted.mean(pairwise_ave, informative_sites), mean=mean(pairwise_ave))
df.wm <- ddply(dataframe, c("feature","specie","pop"), summarise, weightedmean=weighted.mean(watterson_ave, informative_sites), mean=mean(watterson_ave))


df.pm_lost <- df.pm %>%  select (., -mean) %>% tidyr::spread(., pop, weightedmean) %>% 
  mutate(., percentage_po_ki=(c_ll_po_n008 *100)/c_ll_ki_n013) %>% 
  mutate(., percentage_no_ki=(c_ll_no_n008*100)/c_ll_ki_n013)
df.pm.gathered <- tidyr::gather(df.pm_lost, "pop", "weightedmean", -specie, -feature,  -percentage_po_ki, -percentage_no_ki)

df.wm_lost <- df.wm %>%  select (., -mean) %>% tidyr::spread(., pop, weightedmean) %>% 
  mutate(., percentage_po_ki=(c_ll_po_n008 *100)/c_ll_ki_n013) %>% 
  mutate(., percentage_no_ki=(c_ll_no_n008*100)/c_ll_ki_n013)
df.wm.gathered <- tidyr::gather(df.wm_lost, "pop", "weightedmean", -specie, -feature,  -percentage_po_ki, -percentage_no_ki) 



# Guardo el porcentage de perdida. 
write.csv(df.wm_lost, paste(wd,"/", name_diversity[1],".pairwise_mean.csv", sep = ""))
write.csv(df.wm_lost, paste(wd,"/", name_diversity[1],".watterson_mean.csv", sep = ""))

ggplot(data = dataframe, aes(x=feature, y = pairwise_ave, fill=pop)) +
  geom_boxplot(outlier.shape=NA)+
  geom_point(data=df.pm,aes(x=feature, y = weightedmean, fill=pop),shape = 23, position=position_dodge(width=0.75), size = 3, inherit.aes=FALSE) +
  # scale_y_continuous(limits = c(-20,-3), expand =c(0,0), labels = comma) +
  facet_grid(.~feature, scales="free") +
  scale_y_continuous(trans = 'log10') +
  scale_x_discrete("Genomic region") +
  xlab(label = "Genomic region") + #x title
  ylab(label = "Pairwise average") + # y title
  theme_bw() +  #theme selection for background and lines
  #scale_x_continuous(expand = c(0, 1)) +# + scale_y_continuous(expand = c(0, 0)) +
  ggtitle(paste ("Diversity region",name_diversity[1], sep = " " )) +
  theme(axis.line.x = element_line(color="black", size = 0.5),
        axis.line.y = element_line(color="black", size = 0.5),
        axis.text.x = element_text(angle = 60, vjust = 0.8, hjust = 1),
        axis.text.y = element_text(vjust = 0.2, hjust = 0.2),
        axis.title.y = element_text(margin=margin(r=0.3, unit="cm")),
        axis.title.x = element_text(margin=margin(t=0.5, unit="cm")),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.border = element_blank(),
        panel.background = element_blank(),
        strip.background = element_blank(),
        strip.text.x = element_blank())+
  geom_text(data = df.pm.gathered, aes(feature, percentage_po_ki, label=paste((percentage_po_ki %>% round(2)),(percentage_no_ki%>% round(2)), sep='\n')), angle = 45, size=2, vjust="inward",hjust="inward")

ggsave (file = paste(wd,"/",name_diversity[1],".diversity_region_pairwise.eps", sep = ""), device = "eps")
# dev.off()



ggplot(data = dataframe, aes(x=feature, y = watterson_ave, fill =pop)) + 
  geom_boxplot(outlier.shape=NA, position=position_dodge(width=1.5))+
  geom_point(data=df.wm,aes(x=feature, y = weightedmean, fill=pop),shape = 23, position=position_dodge(width=1.5), size = 3, inherit.aes=FALSE) +
  facet_grid(.~feature, scales="free") +
  scale_y_continuous(trans = 'log10') +
  scale_x_discrete("Genomic region") +
  xlab(label = "Genomic region") + #x title
  ylab(label = "Watterson average") + # y title
  theme_bw() +  #theme selection for background and lines
  # scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  ggtitle(paste ("Diversity by region",name_diversity[1], sep = " " )) +
  theme(axis.line.x = element_line(color="black", size = 0.5),
        axis.line.y = element_line(color="black", size = 0.5),
        axis.text.x = element_text(angle = 60, vjust = 0.8, hjust = 1),
        axis.text.y = element_text(vjust = 0.2, hjust = 0.2),
        axis.title.y = element_text(margin=margin(r=0.3, unit="cm")),
        axis.title.x = element_text(margin=margin(t=0.5, unit="cm")),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.border = element_blank(),
        panel.background = element_blank(),
        strip.background = element_blank(),
        strip.text.x = element_blank()) +
        geom_text(data = df.wm.gathered, aes(feature, percentage_po_ki, label=paste((percentage_po_ki %>% round(2)),(percentage_no_ki%>% round(2)), sep='\n')), angle = 45, size=2, vjust="inward",hjust="inward")


ggsave(filename = paste(wd,"/",name_diversity[1],".diversity_region_watterson.eps", sep = ""), device="eps")


# Cosas útiles outliers.shape=NA.



```

Copy

```{bash}

scp mlucena@genomics-b.ebd.csic.es://home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/diversity_per_unit/*eps /Users/marialucenaperez/ownCloud/publico/WG_diversity/diversity_plots/diversity_per_unit 

scp mlucena@genomics-b.ebd.csic.es://home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/diversity_per_unit/*csv /Users/marialucenaperez/ownCloud/publico/WG_diversity/diversity_plots/diversity_per_unit 


scp mlucena@genomics-b.ebd.csic.es:/home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/diversity_per_region/*eps /Users/marialucenaperez/ownCloud/publico/WG_diversity/diversity_plots/diversity_per_region/



```

Diversity per region: Cat Coordinates

He probado con el script de Fede de perl:

```{bash}

# Primero le quito el cabecero porque si no, no puede usarse bedintersect:
# sed '1d' c_ll_ki_n013.per.region.averages.tsv  > c_ll_ki_n013.per.region.averages.NOHEADER.tsv 
# Ahora lo corro tal y como indica el propio script:
# perl lynx2cat.pl c_ll_ki_n013.per.region.averages.NOHEADER.tsv > c_ll_ki_n013.per.region.averages.CatCoordinates.tsv
# Este script me ha dado el siguiente error:
#Use of uninitialized value $s in concatenation (.) or string at lynx2cat.pl line 14, <I> line 156394492.
#Use of uninitialized value $e in concatenation (.) or string at lynx2cat.pl line 14, <I> line 156394492.
# Además me he dado cuenta que solo reporta las coordenadas de uno y otro, y yo quiero toda la información. 

```

Por tanto pruebo con el básico:

```{bash}

intersectBed -sorted -wo -a c_ll_ki_n013.per.region.averages.NOHEADER.tsv -b lynx2cat_wTiger.bed > c_ll_ki_n013.per.region.averages.CatCoordinates.tsv

# Me da el error:

# ERROR: chromomsome sort ordering for file c_ll_ki_n013.per.region.averages.NOHEADER.tsv is inconsistent with other files. Record was:
# lp23.s00015     2903992 3006783 102791  101509  telomers0-10m   na      na      na      na      6.5495647224*10^-06     1.4900294206*10^-05        2.5287837844*10^-06     5.8944110986*10^-06     -4.4603274016*10^-01    1282    c_ll_ki_n013    c_ll

```


Así que ordeno antes:

```{bash}

 bedtools sort c_ll_ki_n013.per.region.averages.NOHEADER.tsv > c_ll_ki_n013.per.region.averages.NOHEADER.SORTED.tsv

```


--------------------------------
Window analysis


```{bash}

RUTA=/home/mlucena/ANGSD_analysis 
cd $RUTA/whole_genome_analysis/sfs/window_analysis

#To launch one by one
POP="c_lp_sm_n019"  # <--CHANGE POP HERE
screen -S "$POP"_sfs_window
POP="c_lp_sm_n019"  # <--CHANGE POP HERE
script "$POP"_sfs_window.log
POP="c_lp_sm_n019"  # <--CHANGE POP HERE

THREADS=10                     # no. of computer cores used by bwa and samtools. 20 = OK, >20 = ask people first!

NGSTOOLS="/opt/angsd/angsd/misc"

WINDOWSIZE=50000
WINDOWSTEP=10000 

RUTA=/home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs

echo "-------$POP---------- Window analysis -----------------------------------------"

$NGSTOOLS/thetaStat do_stat $RUTA/$POP.unfolded-lr.postprob.thetas.idx -win $WINDOWSIZE -step $WINDOWSTEP  -outnames $POP.unfolded-lr.postprob.thetasWindow_$WINDOWSIZE.$WINDOWSTEP.gz

```
Values in this output file are the sum of the per-site estimates for the whole window.


Window R representation


Download data:

```{bash}

scp mlucena@genomics-b.ebd.csic.es:/home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/window_analysis/*PG /Users/marialucenaperez/Owncloud/publico/WG_diversity/ANGSD/sfs/window_analysis/

```


```{r}

# Representación de las ventanas en manhattan plots. 

library(dplyr)
library(qqman)
library(ggplot2)
library ("GGally")

# wd <- "/Users/marialucenaperez/Owncloud/publico/WG_diversity/ANGSD/sfs/window_analysis/" 
wd <- "/home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/window_analysis/"

finsThetas = list.files(path = wd, pattern="*.pestPG$")

for (i in 1:length(finsThetas))
{
  datThetas <- read.csv (paste(wd,finsThetas[i],sep=""), header = T, sep = '\t')  
  name_thetas <- unlist(strsplit(finsThetas[i], "[.]"))
  datThetas$CHR <- as.numeric(gsub('^.{6}', '', datThetas$Chr))
  datThetas$Nsites <- as.numeric(datThetas$nSites)
  datThetas$SNPS_window = paste(datThetas$Chr, datThetas$WinCenter, sep='_')
  datThetas[,-c(1,2)][datThetas[, -c(1,2)] < 0] <- 0
  # Manhattan plot Theta Waterson
  max_thetas <- max (datThetas$tW)
  min_thetas <- min(datThetas$tW)
  setEPS()
  postscript(file=paste(wd,name_thetas[1], '_watterson_manhattan.eps', sep=''))
  manhattan(datThetas, chr = "CHR", bp = "WinCenter", p = "tW", snp = "SNPS_window", ylim = c(0 , max_thetas+10), ylab = "tW", xlab = "Scaffold", logp=F, genomewideline = F, suggestiveline = F, main = paste ("Manhattan plot of tW ",name_thetas[1], sep =''))
  dev.off() 
  # Correlation between diversity indixes
  setEPS()
  postscript(file=paste(wd,name_thetas[1], '_diversity_correlation.eps', sep=''), horizontal = FALSE, onefile = FALSE, paper = "special")
  print(ggpairs(datThetas, columns=4:8, title=paste("Theta correlations ", name_thetas[1], sep='')))
  dev.off()
  # Correlation between neutrality indixes
  setEPS()
  postscript(file=paste(wd,name_thetas[1], '_neutrality_correlation.eps', sep=''), horizontal = FALSE, onefile = FALSE, paper = "special")
  print(ggpairs(datThetas, columns=9:13, title=paste("Neutrality indexes correlations ", name_thetas[1], sep='')))
  dev.off()
}


```


```{bash}
scp mlucena@genomics-b.ebd.csic.es:/home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/window_analysis/*eps /Users/marialucenaperez/Desktop

```

# ----------------------------------

# ATLAS on contemporary data



```{bash}

mkdir mkdir /home/mlucena/ATLAS_contemporary_data
cd /home/mlucena/ATLAS_contemporary_data


# Cuando he probado a Validate samtools me dice que Value was put into PairInfoMap more than once.

java -jar /opt/picard-tools/picard.jar FixMateInformation I=c_lp_sm_0325_recal_round-1.bam O=/home/mlucena/ATLAS_contemporary_data/test_sorted_rg_solved.bam VALIDATION_STRINGENCY=LENIENT


java -jar /opt/picard-tools/picard.jar ValidateSamFile I=test_sorted_rg_solved.bam MAX_OPEN_TEMP_FILES=1000  IGNORE_WARNINGS=true MODE=VERBOSE

samtools index test_sorted_rg_solved.bam

REF="/home/GRUPOS/grupolince/reference_genomes/lynx_pardinus_genome/lp23_without_repetitive_transposable_low_complexity.fa"
bam_to_test=test_sorted_rg_solved.bam


/home/mlucena/atlas/atlas task=mergeReads bam=$bam_to_test fasta=$REF chr=lp23.s00001 verbose
#WARNING: read HWI-ST858:204:C5RR4ACXX:5:1311:13192:36371 was filtered out because it was longer than the insert size
#ERROR: One read of 'HWI-ST0857:233:C5T3DACXX:2:2201:5181:65649' is reverse mate, but forward one has not been read!


# Vuelvo a correr validate samtools

java -jar /opt/picard-tools/picard.jar ValidateSamFile I=test_sorted_rg_solved.bam MAX_OPEN_TEMP_FILES=1000  IGNORE_WARNINGS=true MODE=VERBOSE
# No errors found

# Preubo a correr estimateTheta:
/home/mlucena/atlas/atlas task=estimateTheta bam=$bam_to_test fasta=$REF chr=lp23.s00001 verbose
# WARNING: The following alignment is longer than its insert size: HWI-ST858:204:C5RR4ACXX:5:1311:15377:69596
# WARNING: The following alignment is longer than its insert size: HWI-ST858:204:C5RR4ACXX:5:1311:15377:69596

# aun así me devuelve output.
# Ya tengo el archivo con los valores theta.


# ¿qué pasaría si estimo el PMD?
REF="/home/GRUPOS/grupolince/reference_genomes/lynx_pardinus_genome/lp23_without_repetitive_transposable_low_complexity.fa"
/home/mlucena/atlas/atlas task=estimatePMD bam=test_sorted_rg_solved.bam fasta=$REF  verbose 
# ERROR: position in TPMDTable add function is < 0!
# El mismo error que ccon las antiguas mapeadas con BWA-mem






```


## R representation

Lo represento

```{r}

wd <- "/Users/marialucenaperez/Dropbox/PhD/ancient_historical/ATLAS/contemporary_data/"

thetas_file <- read.table(paste (wd, "test_sorted_rg_solved_theta_estimates.txt", sep =""), header = T)


c="lp23.s00001"
colors=c("#000000",rainbow(tot-2))
    plot('', type="n", xlim=c(0, 2.5e+8), ylim=c(1e-5,0.025),log='y',xlab="Chromosome position",main=paste("Chr=",c,sep=''), yaxt='n') #ylab=expression("Estimated "*theta)
    labelsY1=parse(text=paste(c(1,1,1,1),"%*%","10^",c(-5,-4,-3,-2), sep=""))
    axis(2, at=c(10^-5, 10^-4, 10^-3, 10^-2), labels=labelsY1, las=2)

    for (i in (2:tot)){
            a0<-read.delim(paste(args[i], "_theta_estimates.txt", sep=''))
            a0<-na.omit(a0)
            a0.1<-subset(a0,Chr==c|Chr==paste("chr",c,sep=''))
            lines(a0.1$start,a0.1$theta_MLE,type='l',col=colors[i-1])
            abline(h=median(subset(a0.1,theta_MLE>0)$theta_MLE),lty=2,col=colors[i-1])
            base=basename(args[i])
            legendvec[i-1]=strsplit(base,split="[_.]+")[[1]][1]
    }







eval(parse(text=args[thetas_file]))





# El script que suben ellos:
args <- commandArgs(TRUE)
tot=length(args)
legendvec=vector(length=tot-1)
chr=eval(parse(text=args[1]))

pdf(paste(args[2], "_theta_plot.pdf", sep=""),height=50,width=15)
par(mfrow=c(22,1))
for(c in chr){
    colors=c("#000000",rainbow(tot-2))
    plot('', type="n", xlim=c(0, 2.5e+8), ylim=c(1e-5,0.025),log='y',xlab="Chromosome position",main=paste("Chr=",c,sep=''), yaxt='n') #ylab=expression("Estimated "*theta)
    labelsY1=parse(text=paste(c(1,1,1,1),"%*%","10^",c(-5,-4,-3,-2), sep=""))
    axis(2, at=c(10^-5, 10^-4, 10^-3, 10^-2), labels=labelsY1, las=2)

    for (i in (2:tot)){
            a0<-read.delim(paste(args[i], "_theta_estimates.txt", sep=''))
            a0<-na.omit(a0)
            a0.1<-subset(a0,Chr==c|Chr==paste("chr",c,sep=''))
            lines(a0.1$start,a0.1$theta_MLE,type='l',col=colors[i-1])
            abline(h=median(subset(a0.1,theta_MLE>0)$theta_MLE),lty=2,col=colors[i-1])
            base=basename(args[i])
            legendvec[i-1]=strsplit(base,split="[_.]+")[[1]][1]
    }

    legend("bottomleft",legend=legendvec,col=colors,lty=rep(1,tot-1),lwd=rep(1.5,tot-1),horiz=T)
}
dev.off()



```


# ----------------------------------

# Otra info util


Coincide para sierra morena

3UTR
5UTR
CDS
CDS,lncRNA
feature
intergenic
intron
intron,intron_lncRNA
intron_lncRNA
intron,ncRNA
lncRNA
lncRNA,ncRNA
ncRNA
promoter_gene_1000
promoter_gene_1000,promoter_gene_250,promoter_gene_500
promoter_gene_250
promoter_gene_500
promoter_lncRNA_1000
promoter_lncRNA_1000,promoter_lncRNA_250,promoter_lncRNA_500
promoter_lncRNA_250
promoter_lncRNA_500
UCNE



____


Omnogovi (in Mongolian) is equivalent South Gobi (or S.Gobi)

Khentii−Aimag is better as Khentiy_Mong

Tov (also Aimag/region) is equivalent Central region (Centr_Mong)



_____


Si queremos descargar tablas en base a un nombre y si no está que nos de error: tryCatch

Procesamiento de la tabla con region annotation. Esto sólo se hace una vez!
Comprobamos que el archivo tiene información y añadimos un porcentaje con las bases que tiene que son teloméricas / centroméricas.

```{r}

library(dplyr)

wd <- "/Users/marialucenaperez/Documents/WG_lynx_diversity_per_unit/"

# Tel2m
files_tel2m <- list.files(path = wd, pattern = "region.tel2m")

# Esto saca el porcentaje de bases anotadas como de una región determinada para cada población
for (file_tel2m in files_tel2m){
dataframe <- tryCatch({
        if (file.size(paste(wd, file_tel2m, sep="")) > 0)
          {
    read.table(file=paste(wd, file_tel2m, sep=""), header=T) %>% 
            mutate (tel2m_percentage=tel2m_bases/length)
          }
}, error =function(err) {
            # error handler picks up where error was generated
            print(paste("Read.table didn't work!:  ",err))
        })
# Hasta aquí bien, ahora guardo estos archivos en la carpeta de interes.
write.table(dataframe, paste(wd, file_tel2m, sep=""), row.names = F, quote = F, sep = '\t')
}


# Tel10m
files_tel10m <- list.files(path = wd, pattern = "region.tel10m")

# Esto saca el porcentaje de bases anotadas como de una región determinada para cada población
for (file_tel10m in files_tel10m){
dataframe <- tryCatch({
        if (file.size(paste(wd, file_tel10m, sep="")) > 0)
          {
    read.table(file=paste(wd, file_tel10m, sep=""), header=T) %>% 
            mutate (tel10m_percentage=tel10m_bases/length)
          }
}, error =function(err) {
            # error handler picks up where error was generated
            print(paste("Read.table didn't work!:  ",err))
        })
# Hasta aquí bien, ahora guardo estos archivos en la carpeta de interes.
write.table(dataframe, paste(wd, file_tel10m, sep=""), row.names = F, quote = F, sep = '\t')
}

# Centr

files_centr <- list.files(path = wd, pattern = "region.centr")

# Esto saca el porcentaje de bases anotadas como de una región determinada para cada población
for (file_centr in files_centr){
dataframe <- tryCatch({
        if (file.size(paste(wd, file_centr, sep="")) > 0)
          {
    read.table(file=paste(wd, file_centr, sep=""), header=T) %>% 
            mutate (centr_percentage=centr_bases/length)
          }
}, error =function(err) {
            # error handler picks up where error was generated
            print(paste("Read.table didn't work!:  ",err))
        })
# Hasta aquí bien, ahora guardo estos archivos en la carpeta de interes.
write.table(dataframe, paste(wd, file_centr, sep=""), row.names = F, quote = F, sep = '\t')
}

```




