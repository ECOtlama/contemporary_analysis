---
title: "pipeline_contemporary_sfs"
output: html_document
---


# Generating SAF,SFS & thetas per site.

## Introduction and populations:

I am going to run SAF in my target populations.

Populations:

c_ll_no 
c_ll_po 
c_ll_ki 

c_lp_do 
c_lp_sm 

I checked that all bam files were indexed --> OK

## Global variables:


```{r, engine=bash, eval=FALSE}

RUTA=/home/mlucena/ANGSD_analysis 

cd $RUTA/whole_genome_analysis/sfs

#To launch one by one

# POP="c_lp_do_n012"  # <--CHANGE POP HERE
POP="c_ll_ki_n008"
screen -S "$POP"_sfs_wg
# screen -S "$POP"_thetas
#POP="all pops"
#POP="c_lp_do_n012"  # <--CHANGE POP HERE

#script "$POP"_sfs_wg.log
# script "$POP"_thetas.log

#POP="c_lp_sm_n012"  # <--CHANGE POP HERE
POP="c_ll_ki_n008"  # <--CHANGE POP HERE

# POPS=(c_lp_sm_n012 c_ll_ki_n008)   # <--CHANGE POP HERE

THREADS=15                     # no. of computer cores used by bwa and samtools. 20 = OK, >20 = ask people first!

RUTA=/home/mlucena/ANGSD_analysis/depth_calculus
ANGSD="/opt/angsd/angsd"
NGSTOOLS="/opt/angsd/angsd/misc"
REF="/home/GRUPOS/grupolince/reference_genomes/lynx_pardinus_genome/lp23_without_repetitive_transposable_low_complexity.fa"

###  Ojo para correr esto en el futuro: hay que tener en cuenta que este no tiene filtrado los sitios repetitivos y low complexity y que si nos interesa quitarlos habría que recortarlo luego, o usar una referencia libre de estos sitios. 
ANC="/home/GRUPOS/grupolince/reference_genomes/lynx_rufus_genome/c_lr_zz_0001_recal1.fa"
###  

FILTER1=" -uniqueOnly 1 -remove_bads 1 -only_proper_pairs 1 -baq 1 -C 50 "
FILTER2=" -minMapQ 30 -minQ 20 -doCounts 1 "
RUTA=/home/mlucena/ANGSD_analysis


# 6/3/2018 --> Lo he lanzado en un loop que acaba en SAF posprob. 

#for POP in  ${POPS[@]}
#do

read POP mean sd mean_truncated sd_truncated maxDepth minDepth maxDepth_truncated minDepth_truncated < $RUTA/depth_calculus/"${POP}"_mean_sd_depthGlobal_lynx_per_pop_mean_folds_0.95.csv

N_IND=$(echo ${POP: -3} )
MIN_IND=$(expr $N_IND / 2)

# Sanity checks:

echo $POP
echo $N_IND
echo $MIN_IND
echo $maxDepth_truncated
echo $minDepth_truncated

```

## Unfolded SAF 

```{r, engine=bash, eval=FALSE}


##########################
#  SAF (likelihood):     
##########################

echo "-------$POP----------SAF (likelihood)-----------------------------------------"

$ANGSD/angsd -P $THREADS -b $RUTA/whole_genome_analysis/"$POP".bamlist -ref $REF -anc $ANC \
-out "$POP".unfolded-lr \
$FILTER1 \
$FILTER2 \
-GL 1 -doSaf 1  \
-minInd  $MIN_IND -setMaxDepth $maxDepth_truncated -setMinDepth $minDepth_truncated


```

## SFS 

```{r, engine=bash, eval=FALSE}

##########################
#  SFS                   #(I dont require the -rf file as the saf already only contains the -rf sites
##########################
echo "-------$POP----------SFS------------------------------------------------------"

$NGSTOOLS/realSFS "$POP".unfolded-lr.saf.idx  -P $THREADS > "$POP".unfolded-lr.sfs


```

##  SAF (postprob) 

```{r, engine=bash, eval=FALSE}


echo "-------$POP----------SAF (postprob)-----------------------------------------"

$ANGSD/angsd -P $THREADS -b $RUTA/whole_genome_analysis/"$POP".bamlist -ref $REF -anc $ANC \
-out "$POP".unfolded-lr.postprob \
$FILTER1 \
$FILTER2 \
-GL 1 -doSaf 1  \
-minInd  $MIN_IND -setMaxDepth $maxDepth_truncated -setMinDepth $minDepth_truncated \
-doThetas 1 -pest "$POP".unfolded-lr.sfs 

done

```


##  Make bed theta        

La población de sierra morena se volvió a lanzar desde aquí en abril/mayo de 2018 porque me di cuenta de que los archivos que había creado se habína corrompido por el camino. Antes de esto (archivos saf, sfs, etc) sí están bien. 

```{r, engine=bash, eval=FALSE}

echo "-------$POP----------Make bed theta -----------------------------------------"

$NGSTOOLS/thetaStat print $POP.unfolded-lr.postprob.thetas.idx > $POP.printed.stats

# Transform the coordinates to use 0Based bedtools

awk 'NR>1 {print  $1" "$2-1" "$2}' $POP.printed.stats > $POP.0basedcoordinates.borrar # --> 1, 2, 3
# log transformation of watterson and pairwise:
awk 'NR>1 {print  "e("$3")"}' $POP.printed.stats | bc -l > "$POP".watterson.borrar # --> 4
awk 'NR>1 {print  "e("$4")"}' $POP.printed.stats | bc -l > "$POP".pairwise.borrar # --> 5
                                                                                  # --> PAIRWISE - WATTERSON
awk 'NR>1 {print  "e("$5")"}' $POP.printed.stats | bc -l > "$POP".thetaFL.borrar # --> 6
awk 'NR>1 {print  "e("$6")"}' $POP.printed.stats | bc -l > "$POP".thetaH.borrar # --> 7
awk 'NR>1 {print  "e("$7")"}' $POP.printed.stats | bc -l > "$POP".thetaL.borrar # --> 8

# Create a header for the "$POP".transformedThetas file

echo "scaffold position1 position2 watterson pairwise pairwise-watterson thetaFL thetaH thetaL" > "$POP".transformedThetas

# ¿Tengo las mismas posiciones?

wc -l $POP*borrar



paste $POP.0basedcoordinates.borrar "$POP".watterson.borrar "$POP".pairwise.borrar "$POP".thetaFL.borrar "$POP".thetaH.borrar "$POP".thetaL.borrar | \
awk -v OFS='\t'  '{print $1, $2, $3, $4, $5, $5-$4, $6, $7, $8}' >> "$POP".transformedThetas

# Ojo a día 02/03/2020 estoy corrigiendo que antes ponía $5-4, y he puesto $5-$4. Este resultado a afectado al menos a los siguiente archivos:
# 1. c_ll_ki_n008.transformedThetas
# 2. c_lp_sm_n012.transformedThetas

# que son los dos archivos submuestreados. 

rm "$POP".*.borrar
mv $POP.printed.stats /backup/mlucena/intermediate_files_ANGSD/whole_genome_analysis/
scp *.transformedThetas /backup/mlucena/intermediate_files_ANGSD/whole_genome_analysis/

# done

```



# ----------------------------------

## SFS graphical representation


First we copy the SFS files to our local folder.

```{bash}
scp mlucena@genomics-b.ebd.csic.es://home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/*sfs /Users/marialucenaperez/Owncloud/publico/WG_diversity/ANGSD/sfs

# I need to change name as R does not like "-lr.sfs"

cd /Users/marialucenaperez/Owncloud/publico/WG_diversity/ANGSD/sfs/
for i in *sfs
do 
#echo $i
echo $i "--->" ${i/-lr.sfs/_lr.sfs}
mv $i ${i/-lr.sfs/_lr.sfs}
done


```

Now we run the script. 

```{r}
library(dplyr)
library(magrittr)

wd<-"/Users/marialucenaperez/Owncloud/publico/WG_diversity/ANGSD/sfs/"

my_files_unfolded = list.files(path = wd, pattern="*sfs$")    
for (i in 1:length(my_files_unfolded)) 
{ assign(my_files_unfolded[i], (scan(paste(wd,my_files_unfolded[i], sep = ""), sep = " ", dec = ".")) %>% .[!is.na(.)])}

norm <- function(x) x/sum(x)

for (i in 1:length (my_files_unfolded))
{
  temp1=(eval(parse(text=my_files_unfolded[i])))
  temp2=norm(temp1[-1])
  temp3=norm(temp2[-c(length(temp2))])
  barplot(temp3,xlab="Chromosomes",
          names=1:length(temp3),ylab="Proportions",main=my_files_unfolded[i],col='grey')
  dev.print(device=postscript, paste("/Users/marialucenaperez/Owncloud/publico/WG_diversity/ANGSD/sfs/",my_files_unfolded[i],".eps", sep =""), onefile=FALSE, horizontal=FALSE)
  dev.off()
  rm(temp1)
  rm(temp2)
  rm(temp3)
}


```

# ------------------------------------------------
# 18/05/2018 PROBLEMA NO Y DO

Quería hacer un filtrado y me di cuenta de un problema que solucioné por el camino. En el script real esta parte se puede obviar e irías directamente al siguiente apartado. 

## Global diversity per feaure

Este archivo generado hasta ahora contiene todos los sitios, incluyendo los repetitivos. Hasta ahora he corrido todo usando este archivo pero el 17/05/2018 me di cuenta de que incluía las repeticiones, así que voy a repetir el proceso hasta aquí dejando fuera estas low complexity and repetitive regions. 

Para ello primero filtro el archivo

### Filtering repetitive regions 

Mi archivo de interés es el archivo Theta. 

```{bash}

mv c_ll_no_n008_separated_by_scaffold/*transformedThetas .
mv c_ll_ki_n013_separated_by_scaffold/*transformedThetas .
mv c_lp_do_n012_separated_by_scaffold/*transformedThetas .
mv c_ll_po_n008_separated_by_scaffold/*transformedThetas .
mv c_lp_sm_n019_separated_by_scaffold/*transformedThetas .

head -n1 c_ll_ki_n013.transformedThetas > header_thetas_file
for theta_file in *transformedThetas
do
echo $theta_file
bedtools subtract -a <(tail -n +2 $theta_file) -b /home/mlucena/grupolince/reference_genomes/lynx_pardinus_genome/repeats_and_lowcomp_no_redundant.bed | cat header_thetas_file - > ${theta_file/transformedThetas/transformedThetas_filtered}
done


```

Esto me devuelve un mensaje de error para dos poblaciones Noruega y Doñana.

## Sanity check -- > Problema detectado

Lee hasta el final!

Nos hemos dado cuenta al hacer el filtrado que bedtools cantaba porque decía que había archivos que tenían 8 columnas en vez de 9. Hemos ido a /backup/mlucena/intermediate_files_ANGSD/whole_genome_analysis y nos hemos dado cuenta que la columna que falta es watterson. Esto es grave porque es la primera columna, así que pasaría desapercibido.

¿Cómo lo he sabido?

```{bash}
cd /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs

awk 'NF == 8' < c_ll_no_n008.transformedThetas | head

# Me sale a partir de lp23.s37389. Así que busco ese scaffold en printed.stats para ver si está bien.  

cd

grep lp23.s37390  c_ll_no_n008.printed.stats | head > kk

# Efectivamente tiene todas las columnas, así que vamos a comprobar cual es la que luego falta comparando con el c_ll_no_n008.transformedThetas para ese scaffold. 

# Lo hago así:

awk '{print  "e("$3")"}' kk  | bc -l  # ¡AUSENTE!
awk '{print  "e("$4")"}' kk  | bc -l  # --> Presente
awk '{print  "e("$5")"}' kk  | bc -l  # --> Presente
awk '{print  "e("$6")"}' kk  | bc -l  # --> Presente
awk '{print  "e("$7")"}' kk  | bc -l  # --> Presente

```

Dada la gravedad del descubrimiento voy a comprobar si hay más archivos que les falten columnas: 

```{bash}

awk 'NF !=9' < c_ll_po_n008.transformedThetas | head # --> Perfecto!
awk 'NF !=9' < c_ll_no_n008.transformedThetas | head # PROBLEMA
awk 'NF !=9'< c_lp_do_n012.transformedThetas | head # PROBLEMA
awk 'NF !=9' < c_lp_sm_n019.transformedThetas | head # --> Perfecto!
awk 'NF !=9' c_ll_ki_n013.transformedThetas | head # --> Perfecto!

```

¿Qué poblaciones tienen este problema?

Efectivamente las que dieron el mensaje de aviso:

Noruega
Doñana

¿A partir de qué scaffold?
```{bash}
awk 'NF == 8' < c_ll_no_n008.transformedThetas | head

# lp23.s37389	1704	1705	.00000210987258195238	.00001016749185751371	8.05762e-06	.00000022186028500950	.00000116586646570892

# A partir del scaffold lp23.s37389 para noruega.


 awk 'NF == 8' < c_lp_do_n012.transformedThetas | head

# lp23.s37350	1364	1365	.00000006491013903094	.00000059407990881920	5.2917e-07	.00000000363581697066	.00000003427293024659	

# A partir del scaffold lp23.s37350 para doñana. 

# Para ambos:
# Cuando hago el tail me doy cuenta de que es hasta el final 


# Empiezo con doñana.

# Qué linea es esa?

cd /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs
grep -nr lp23.s37350 c_lp_do_n012.transformedThetas | head

# 1369623421:lp23.s37350	84	85	.00000160262764124604	.00000100177412987377	-6.00854e-07.00000266362654746090	.00000011972073675458	.00000056074756833874

cd /backup/mlucena/intermediate_files_ANGSD/whole_genome_analysis
grep -nr lp23.s37350 c_lp_do_n012.printed.stats | head

# 1369623421:lp23.s37350	85	-13.257177	-13.813738	-12.835822	-15.938104	-14.393995

# Para Doñana, ¡coinciden! El problema es a partir de la linea 1369623421


# ¿Y en Noruega?

cd /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs
grep -nr lp23.s37389 c_ll_no_n008.transformedThetas | head

# 1368906858:lp23.s37389	57	58	.00000280538155921492	.00000887284561212872	6.06746e-06	.00002134658039437027	.00000149690337555105	.00000518486890558351

cd /backup/mlucena/intermediate_files_ANGSD/whole_genome_analysis
grep -nr lp23.s37389 c_ll_no_n008.printed.stats | head

# 1368906858:lp23.s37389	58	-11.239233	-11.632515	-10.754619-13.412112	-12.169766

# Para Noruega, ¡coincide! El problema es a partir de la linea 1368906858

```

Corro desde el scaffold que va mal para cada una de las poblaciones. 

### Norway: Thetas file

```{bash}

# Población: c_ll_no_n008

#Corro todo para lo que sea mayor que 1368906857, por lo que debe empezar en el 1368906858 

# Me voy a backup que es donde tengo mis archivos:

cd /backup/mlucena/intermediate_files_ANGSD/whole_genome_analysis

# Transform the coordinates to use 0Based bedtools

awk 'NR>1368906857 {print  $1" "$2-1" "$2}' c_ll_no_n008.printed.stats > c_ll_no_n008.0basedcoordinates.borrar # --> 1, 2, 3
# log transformation of watterson and pairwise:
awk 'NR>1368906857 {print  "e("$3")"}' c_ll_no_n008.printed.stats | bc -l > c_ll_no_n008.watterson.borrar # --> 4
awk 'NR>1368906857 {print  "e("$4")"}' c_ll_no_n008.printed.stats | bc -l > c_ll_no_n008.pairwise.borrar # --> 5
                                                                                  # --> PAIRWISE - WATTERSON
awk 'NR>1368906857 {print  "e("$5")"}' c_ll_no_n008.printed.stats | bc -l > c_ll_no_n008.thetaFL.borrar # --> 6
awk 'NR>1368906857 {print  "e("$6")"}' c_ll_no_n008.printed.stats | bc -l > c_ll_no_n008.thetaH.borrar # --> 7
awk 'NR>1368906857 {print  "e("$7")"}' c_ll_no_n008.printed.stats | bc -l > c_ll_no_n008.thetaL.borrar # --> 8

# Create a header for the "$POP".transformedThetas file

# Sanity check:

wc -l *borrar
# 283248 c_ll_no_n008.0basedcoordinates.borrar
# 283248 c_ll_no_n008.pairwise.borrar
# 283248 c_ll_no_n008.thetaFL.borrar
# 283248 c_ll_no_n008.thetaH.borrar
# 283248 c_ll_no_n008.thetaL.borrar
# 283248 c_ll_no_n008.watterson.borrar

# ¡Bien!

# Ahora hago elimino a partir de esa linea en mi archivo de Thetas:

# Sanity:

cd /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs
wc -l c_ll_no_n008.transformedThetas
# 1369190105
awk 'NR>1368906857 {print $0}' c_ll_no_n008.transformedThetas | wc -l 
# 283248
awk 'NR<=1368906857 { print }' c_ll_no_n008.transformedThetas | wc -l 
# 1368906857

# 1368906857 + 283248 =  1369190105 --> ¡Bien!

# Ahora podría pegar los resultantes de *.borrar, eliminar las filas NR<=1368906857 en mi archivo Theta y pegarles estas. 

# Creo mi archivo con theta nuevo:

cd /backup/mlucena/intermediate_files_ANGSD/whole_genome_analysis

awk 'NR<=1368906857 { print }' c_ll_no_n008.transformedThetas > c_ll_no_n008.transformedThetas_subset

wc -l c_ll_no_n008.transformedThetas_subset
# 1368906857 c_ll_no_n008.transformedThetas_subset

# Ahora junto estas columnas y las añado a nuestro archivo subset. 

paste c_ll_no_n008.0basedcoordinates.borrar c_ll_no_n008.watterson.borrar c_ll_no_n008.pairwise.borrar c_ll_no_n008.thetaFL.borrar c_ll_no_n008.thetaH.borrar c_ll_no_n008.thetaL.borrar | awk -v OFS='\t'  '{print $1, $2, $3, $4, $5, $5-$4, $6, $7, $8}' >> c_ll_no_n008.transformedThetas_subset

# ¿Y ahora cuanto mide?

wc -l c_ll_no_n008.transformedThetas_subset
# 1369190105 c_ll_no_n008.transformedThetas_subset
#Perfecto.

awk 'NF != 9' < c_ll_no_n008.transformedThetas_subset
# Nada! Perfecto!

mv c_ll_no_n008.transformedThetas_subset c_ll_no_n008.transformedThetas

```


### Doñana: Thetas file

```{bash}

# Población: c_lp_do_n012

#Corro todo para lo que sea mayor que 1369623420, por lo que debe empezar en el 1369623421 

# Me voy a backup que es donde tengo mis archivos:

cd /backup/mlucena/intermediate_files_ANGSD/whole_genome_analysis

# Transform the coordinates to use 0Based bedtools

awk 'NR>1369623420 {print  $1" "$2-1" "$2}' c_lp_do_n012.printed.stats > c_lp_do_n012.0basedcoordinates.borrar # --> 1, 2, 3
# log transformation of watterson and pairwise:
awk 'NR>1369623420 {print  "e("$3")"}' c_lp_do_n012.printed.stats | bc -l > c_lp_do_n012.watterson.borrar # --> 4
awk 'NR>1369623420 {print  "e("$4")"}' c_lp_do_n012.printed.stats | bc -l > c_lp_do_n012.pairwise.borrar # --> 5
                                                                                  # --> PAIRWISE - WATTERSON
awk 'NR>1369623420 {print  "e("$5")"}' c_lp_do_n012.printed.stats | bc -l > c_lp_do_n012.thetaFL.borrar # --> 6
awk 'NR>1369623420 {print  "e("$6")"}' c_lp_do_n012.printed.stats | bc -l > c_lp_do_n012.thetaH.borrar # --> 7
awk 'NR>1369623420 {print  "e("$7")"}' c_lp_do_n012.printed.stats | bc -l > c_lp_do_n012.thetaL.borrar # --> 8

# Create a header for the "$POP".transformedThetas file

# Sanity check:

wc -l c_lp*borrar
#  282955 c_lp_do_n012.0basedcoordinates.borrar
#  282955 c_lp_do_n012.pairwise.borrar
#  282955 c_lp_do_n012.thetaFL.borrar
#  282955 c_lp_do_n012.thetaH.borrar
#  282955 c_lp_do_n012.thetaL.borrar
#  282955 c_lp_do_n012.watterson.borrar

# ¡Bien!

# Ahora hago elimino a partir de esa linea en mi archivo de Thetas:

# Sanity:

cd /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs
wc -l c_lp_do_n012.transformedThetas
# 1369906375

awk 'NR>1369623420 {print $0}' c_lp_do_n012.transformedThetas | wc -l 
# 282955

awk 'NR<=1369623420 { print }' c_lp_do_n012.transformedThetas | wc -l 
# 1369623420

# 1369623420 + 282955 = 1369906375 --> ¡Bien!

# Ahora podría pegar los resultantes de *.borrar, eliminar las filas NR<=1369623420 en mi archivo Theta y pegarles estas. 

# Creo mi archivo con theta nuevo:

cd /backup/mlucena/intermediate_files_ANGSD/whole_genome_analysis

awk 'NR<=1369623420 { print }' c_lp_do_n012.transformedThetas > c_lp_do_n012.transformedThetas_subset

# Ahora junto estas columnas y las añado a nuestro archivo subset. 

paste c_lp_do_n012.0basedcoordinates.borrar c_lp_do_n012.watterson.borrar c_lp_do_n012.pairwise.borrar c_lp_do_n012.thetaFL.borrar c_lp_do_n012.thetaH.borrar c_lp_do_n012.thetaL.borrar | awk -v OFS='\t'  '{print $1, $2, $3, $4, $5, $5-$4, $6, $7, $8}' >> c_lp_do_n012.transformedThetas_subset

# 03/03/2020 ojo! he corregido $5-$4! porque ponia $5-4. El problema está explicado más adelante. 

wc -l c_lp_do_n012.transformedThetas_subset
# 1369906375 c_lp_do_n012.transformedThetas_subset
# Perfecto!

awk 'NF != 9' < c_lp_do_n012.transformedThetas_subset
# Nada, perfecto!

mv c_lp_do_n012.transformedThetas_subset c_lp_do_n012.transformedThetas

```

Como ha ido bien en principio, elimino los archivos *borrar.

```{bash}
rm *borrar
```

Elimino todos los filtrados, porque durante este tiempo que hemos solucionado este problema hemos decidido filtrar también los que tienen baja mappability (script mappability).

```{bash}
cd /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs
rm *transformedThetas_filtered
```

Copiar a mi ruta ANGSD/wholegenome/sfs

```{bash}
cd /backup/mlucena/intermediate_files_ANGSD/whole_genome_analysis/
scp c_lp_do_n012.transformedThetas /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs
scp c_ll_no_n008.transformedThetas /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs
```

Hago sanity checks en mi carpeta donde los he movido, por segunda vez.

```{bash}

cd /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs

awk 'NF != 9' < c_lp_do_n012.transformedThetas | head # Ya están comprobados cuando los generé.
awk 'NF != 9' < c_ll_no_n008.transformedThetas | head # Ya están comprobados cuando los generé.

awk 'NF != 9' < c_lp_sm_n019.transformedThetas | head # --> Bien!
awk 'NF != 9' < c_ll_ki_n013.transformedThetas | head # --> Bien!
awk 'NF != 9' < c_ll_po_n008.transformedThetas | head # --> Bien!


tail c_lp_do_n012.transformedThetas
tail c_ll_no_n008.transformedThetas

tail c_lp_sm_n019.transformedThetas # --> Buena pinta
tail c_ll_ki_n013.transformedThetas # --> Buena pinta
tail c_ll_po_n008.transformedThetas # --> Buena pinta

 
```


## 03/03/2020 PROBLEMA DETECTADO.


En algún momento en el que generaba los archivos para calcular la diferencia de Pairwise-watterson hacíamos $5-$4, sin embargo se ve que en algun momento lo copie mal y escribí $5-4. 
Esta diferencia se usa para calcular despues la estandar deviation por la que dividir mi valor de tajima. Este fallo afecto a los siguientes datasets:

- c_ll_ki_n008 --> afectado entero
- c_lp_sm_n012 --> afectado entero

- c_ll_no_n008 --> afectado a partir del scaffold lp23.s37389
- c_lp_do_n012 --> afectado a partir del scaffold lp23.s37350
 
Es decir, están afectados los archivos submuestreados que se hiceron despues que los demás y la corrección de los archivos de doñana y noruega que tuvieron un fallo. 
El impacto de este problema es el siguiente:

1) ¿Para que se usa la columna de la diferencia?
- La columna de la diferencia se usa para calcular la estandar deviation de la misma. 
Tajima se calcula como la  media de pairwise - la media de watterson /standard deviation de la diferencia de cada una de las bases. 

Este problema lo que provocaría sería que en vez de estar usando la standard deviation de la diferencia ahora estaría usando la standard deviation de pi, porque realmente yo estoy haciendo pi - 4, y por tanto la estandar deviation será la de pi. 

2) ¿A cuantas unidades afecta?
Con respecto a los submuestreados a nada por ahora porque no estoy usando estos datos más que para diversidad por ventanas, y esto no está afectado.

¿Y para las poblaciones de noruega y doñana?

----NORWAY----

Hemos calculado a cuantas unidades finales afecta. Para ello, hemos cogido el archivo final que usamos para todos los calculos, que es data_diversity_filtered_ave_corrected.
Este archivo está en el script 13.Diversity_calculation_contemporary. 
Sobre este hemos seleccionado las unidades afectadas, para ver cuantas son.

Lo que hemos visto es que hay 133 unidades afectadas en noruega, de las cuales tenemos valor de tajima para 48, porque en los otros casos o bien watterson o bien pairwise era cero y por tanto no lo hemos calculado. De estas 48 unidades, 2 son CDS, 2 son intrones y lo demás son intergenico. 

Las unidades TOTALES (con valor para tajima): 144002
Unidades afectadas= 48

Un 0,03% de las unidades. 

En numero de bases: TOTAL=1045044731
BASES AFECTADAS: 37849

Es aun menor y tenemos que el 0.003621759 de las bases están afectadas. 

----DOÑANA----

En el caso de Doñana hay 144 unidades afectadas, de las cuales para 36 tenemos datos de tajima. 2 CDS y lo demás intergenico. 

Las unidades TOTALES (con valor para tajima): 87133
Unidades afectadas= 48

Un 0.055% de las unidades. 

En numero de bases: TOTAL=883318777
BASES AFECTADAS: 25925
Un 2.934954e-05 de las bases afectadas.

3) ¿Son los valores de las unidades afectadas distinguibles de las no afectadas?

La realidad es que no. Si los ordenas por valor, los valores "erroneos" aparecen intercalados entre los buenos, y esto se debe a que posiblemente, el valor de standard deviation de pi sea muy parecido al valor de la estandar deviation de la diferencia. 

Vamos a calcular también la media para los valores erroneos para compararla con la media normal:

Norway:

Weighted mean tajima erronea = 0.004512704
Weighted mean tajima buena   = 0.006937857
Incluyendo ambos valores =     0.006937769

Doñana:
Weighted mean tajima erronea =  0.01087424
Weighted mean tajima buena   =  0.005145623
Incluyendo ambos valores =      0.005145791

El script usado para comprobar la afectación ha sido el siguiente. Despues de cargar los datos para analizar en el script 13.Diversity_calculation_contemporary.

```{r}

substrRight <- function(x, n){
  substr(x, nchar(x)-n+1, nchar(x))
}

kk_Norway <- data_diversity_filtered_ave_corrected %>% filter (Populations=="Norway") %>% mutate (scaffold_number=substrRight(scaffold, 5)) %>% select (scaffold, scaffold_number, feature, informative_sites,tajimaD) %>% filter (scaffold_number>37000) %>% mutate (tajima_status=ifelse (scaffold_number>37388, "WRONG_TAJIMA", "GREAT_TAJIMA") ) 
kk_Norway %>% filter (scaffold_number>37388) %>% filter (tajimaD!="NA") %>% nrow()
data_diversity_filtered_ave_corrected %>% filter (Populations=="Norway") %>% filter (tajimaD!="NA") %>% nrow()
data_diversity_filtered_ave_corrected %>% filter (Populations=="Norway") %>% filter (tajimaD!="NA") %>% summarise(sum_informative_sites=sum(informative_sites))
kk_Norway %>% filter (scaffold_number>37388) %>% filter (tajimaD!="NA") %>% summarise(sum_informative_sites=sum(informative_sites))

kk_Norway %>% filter (scaffold_number>37388) %>% filter (tajimaD!="NA") %>% summarise(media_tajima=weighted.mean(tajimaD, informative_sites))
data_diversity_filtered_ave_corrected %>% filter (Populations=="Norway") %>% mutate (scaffold_number=substrRight(scaffold, 5)) %>% filter (scaffold_number<37388) %>% filter (tajimaD!="NA") %>% summarise(media_tajima=weighted.mean(tajimaD, informative_sites))
data_diversity_filtered_ave_corrected %>% filter (Populations=="Norway") %>% filter (tajimaD!="NA") %>% summarise(media_tajima=weighted.mean(tajimaD, informative_sites))

                                                                                      
                                                                                      
kk_Donana <- data_diversity_filtered_ave_corrected %>% filter (Populations=="Donana") %>% mutate (scaffold_number=substrRight(scaffold, 5)) %>% select (scaffold, scaffold_number, feature, informative_sites, tajimaD, watterson_zero, pairwise_zero) %>% filter (scaffold_number>37000) %>% mutate (tajima_status=ifelse (scaffold_number>37350, "WRONG_TAJIMA", "GREAT_TAJIMA") ) 
kk_Donana %>% filter (scaffold_number>37350) %>% filter (tajimaD!="NA") %>% nrow()
data_diversity_filtered_ave_corrected %>% filter (Populations=="Donana") %>% filter (tajimaD!="NA") %>% nrow()
data_diversity_filtered_ave_corrected %>% filter (Populations=="Donana") %>% filter (tajimaD!="NA") %>% summarise(sum_informative_sites=sum(informative_sites))
kk_Donana %>% filter (scaffold_number>37350) %>% filter (tajimaD!="NA") %>% summarise(sum_informative_sites=sum(informative_sites))

kk_Donana %>% filter (scaffold_number>37350) %>% filter (tajimaD!="NA") %>% summarise(media_tajima=weighted.mean(tajimaD, informative_sites))
data_diversity_filtered_ave_corrected %>% filter (Populations=="Donana") %>% mutate (scaffold_number=substrRight(scaffold, 5)) %>% filter (scaffold_number<37350) %>% filter (tajimaD!="NA") %>% summarise(media_tajima=weighted.mean(tajimaD, informative_sites))
data_diversity_filtered_ave_corrected %>% filter (Populations=="Donana") %>% filter (tajimaD!="NA") %>% summarise(media_tajima=weighted.mean(tajimaD, informative_sites))


```

Por tanto, resumen:

> Los valores no están afectados, y por tanto no merece la pena correr todo.
> Voy a poner un warning, en los archivos afectados. 

# ---------------------------------------------

# Filtering & separate by scaffold.

Normalmente ahora pasaríamos a separar las carpetas de theta por scaffold. Como vamos a usar unos archivos filtrados (sin regiones de baja mappability ni de low complexity o repetitivo), voy a crearlos y despues hacemos las carpetas para esos archivos filtrados. 

### Filtering repetitive regions

Mi archivo de interés es el archivo Theta. 

```{bash}

cd /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs
head -n1 c_ll_ki_n013.transformedThetas > header_thetas_file
for theta_file in *transformedThetas
do
echo $theta_file
bedtools subtract -a <(tail -n +2 $theta_file) -b /home/mlucena/grupolince/reference_genomes/lynx_pardinus_genome/repeats_and_lowcomp_no_redundant_mappability.k75_lessthan90.bed | cat header_thetas_file - > ${theta_file/transformedThetas/transformedThetas_filtered}
done

# ¿Cuanto ocupaban sin filtrar y filtrados?

# Not filtered

# 1369113172 c_ll_ki_n013.transformedThetas
# 1369400768 c_ll_po_n008.transformedThetas
# 1383676766 c_lp_sm_n019.transformedThetas
# 1369906375 c_lp_do_n012.transformedThetas
# 1369190105 c_ll_no_n008.transformedThetas

# Filtered

# 1284340665 c_ll_ki_n013.transformedThetas_filtered --> 1369113172 - 1284340665 = 84772507 que he filtrado. 
# 1277662393 c_ll_po_n008.transformedThetas_filtered --> 1369400768 - 1277662393 = 91738375 que he filtrado.
# 1289013196 c_lp_sm_n019.transformedThetas_filtered --> 1383676766 - 1289013196 = 94663570 que he filtrado. 
# 1282704524 c_lp_do_n012.transformedThetas_filtered --> 1369906375 - 1282704524 = 87201851 que he filtrado.
# 1284367841 c_ll_no_n008.transformedThetas_filtered --> 1369190105 - 1284367841 = 84822264 que he filtrado. 

# Ahora vamos a eliminar el archivo thetas de aquí y lo dejamos sólo en backup




#¡Lanzar ahora esto, pero cuidad con la población!
#rm *transformedThetas

```

## Separate by scaffold

Ahora creo las carpetas de theta per scaffold para todas las poblaciones de nuevo partiendo de los archivos filtrados. 

```{bash}

cd /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/

#NEW_FOLDER=("c_ll_no_n008_separated_by_scaffold" "c_lp_do_n012_separated_by_scaffold" "c_lp_sm_n019_separated_by_scaffold" "c_ll_ki_n013_separated_by_scaffold" "c_ll_po_n008_separated_by_scaffold")

NEW_FOLDER=("c_lp_sm_n012_separated_by_scaffold")


for FOLDER in "${NEW_FOLDER[@]}"
do
echo "---------------------------------------------------$FOLDER---------------------------------------------------"
mkdir $FOLDER;
done
 
 
POPS=("c_lp_sm_n012" )

# POPS=("c_ll_no_n008" "c_lp_do_n012")

for POP in "${POPS[@]}"
do
echo $POP
cd /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/
mv "$POP".transformedThetas_filtered "$POP"_separated_by_scaffold/
cd  /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/"$POP"_separated_by_scaffold
# Create multiple files base on one column: transformedThetas:
awk '{print >> $1; close($1)}' "$POP".transformedThetas_filtered
rm scaffold # esto borra la primera linea que forma un archivo llamado scaffold.
# Rename the files: transformedThetas

for file in lp23*
do
echo $file
mv $file ${file/lp23/"$POP".transformedThetas_lp23}
done
done

# Copiamos el archivo filtered a backup

POPS=("c_lp_sm_n019" "c_ll_ki_n013" "c_ll_po_n008" "c_ll_no_n008" "c_lp_do_n012")

cd /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs

for POP in "${POPS[@]}"
do
echo $POP
cd "$POP"_separated_by_scaffold/
scp *.transformedThetas_filtered /backup/mlucena/intermediate_files_ANGSD/whole_genome_analysis
cd /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs
done


```

# ---------------------------------------------

# Diversity calculus

He modificado este script para que lo haga en bloques de 10, por lo que tadaría 10 veces menos. La estrategia consiste en establecer 10 bloques con empezando por un scaffold hasta otro y despues iterar sobre este bloque. Como los 10 bloques se lanzan en un bucle al poner & al final se lanzan los 10 a la vez. Además no necesitamos definir los scaffold a priori como hacíamos antes porque vamos a iterar por todos. 

c_ll_ki_n013 # --> Lanzado
c_ll_po_n008 # --> Lanzado
c_lp_sm_n019 # --> Lanzado
c_ll_no_n008 # --> Lanzado
c_lp_do_n012 # --> Lanzado
c_ll_ki_n008 # --> Lanzado
c_lp_sm_n012 # --> Lanzado


```{bash}

# Run pop by pop:

POP=c_lp_sm_n012 # <--- Change pop here!
screen -S "$POP"_scaffold_per_unit
POP=c_lp_sm_n012 # <--- Change pop here!
script log_screen_"$POP"_scaffold_per_unit.log
POP=c_lp_sm_n012 # <--- Change pop here!

cd /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs

## Variables
SCAFFOLDS_FOLDER=/GRUPOS/grupolince/Lyp_annotation_Apr14_final/LYPA23C.CDS.GENE_promoters.GENE_introns.UTRs.ncRNA.lncRNA.lncRNA_introns.lncRNA_promoters.UCNE.intergenic.nr.gff3.PerScaffold/
DIVERSITY_PER_UNIT_FOLDER=/home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/diversity_per_unit/
SFS_FOLDER=/home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/


echo "---------------------------------------------------$POP---------------------------------------------------"

# Create headers for the outfile
echo -e "scaffold\tstart_cero_based\tend\tlength\tNAs\tinformative_sites\tfeature\tstrandness\tframe\tid_gene\tid\twatterson_ave\twatterson_sd\tpairwise_ave\tpairwise_sd\ttajimaD\tpop\tspecie\tepoch" > $DIVERSITY_PER_UNIT_FOLDER"$POP".per.unit.averages.tsv

for iteration in {1..10}
do

rm $DIVERSITY_PER_UNIT_FOLDER"$POP".per.unit.averages_iteration"$iteration".tsv

SCAFFOLD_START=$( echo '1 + 4170 * ('$iteration' - 1)' | bc)
SCAFFOLD_END=$( echo '4170 * '$iteration | bc )

for SCAFFOLD_NUMBER in $(seq -s " " -f %05g $SCAFFOLD_START $SCAFFOLD_END) # esta manera de hacer seq te va a sacar siempre 5 cifras.
do

SCAFFOLD=$(echo "lp23.s"$SCAFFOLD_NUMBER)

#echo "---------------------$SCAFFOLD---------------------"

#For each unit

cd "$SFS_FOLDER""$POP"_separated_by_scaffold

while read LOCATION METHOD FEATURE START_ONEBASED END POINT STRANDNESS FRAME IDRAW;

do

#echo "--------$SCAFFOLD:$FEATURE--------"

if [ "$METHOD" = "PipeR" ] 
then
ID_GENE=$(echo $IDRAW | awk -F "_" '{ split ($0, a, "ID="); split (a[2],b,"_"); print b[1]"_"b[2] }')
else
ID_GENE=$(echo $IDRAW | awk '{ split($0,a,"ID="); split (a[2],b,";"); split(b[1],c,"T"); print c[1] }') 
fi

if [ "$FEATURE" = "CDS" ]
then
ID=$(echo $IDRAW | awk -F "_" '{split ($0,a,"Target="); split(a[2],b,";"); print b[1]}' |  awk '{print $1"_"$2"_"$3}' )
else
ID=$(echo $IDRAW | awk '{ split($0,a,"ID="); split (a[2],b,";"); print b[1] }')
fi

START_CEROBASED=($(echo $START_ONEBASED-1 | bc))
LENGTH=($(echo $END-$START_CEROBASED | bc)) 
SPECIE=$(echo $POP | cut -d"_" -f 2)
EPOCH=$(echo $POP | cut -d"_" -f 1)


cat "$POP".transformedThetas_"$SCAFFOLD" | bedtools intersect -a stdin -b <(echo -e "$LOCATION\t$START_CEROBASED\t$END") > "$POP".iter.transformedThetas.iteration"$iteration".borrar
# He comprobado que en el archivo de Thetas hay por lo menos 4 posiciones que empieza en 0. por tanto asumo que este archivo es 0-based.


WATTERSON_AVERAGE_PER_UNIT=$(cut -f 4 "$POP".iter.transformedThetas.iteration"$iteration".borrar | awk -v OFS='\t' '{for(i=1;i<=NF;i++) {sum[i] += $i }} END {for (i=1;i<=NF;i++) {printf ("%.10e\n", sum[i]/NR )}}' | sed 's/[eE]+\{0,1\}/*10^/g')
WATTERSON_SD_PER_UNIT=$(cut -f 4 "$POP".iter.transformedThetas.iteration"$iteration".borrar | awk -v OFS='\t' '{for(i=1;i<=NF;i++) {sum[i] += $i; sumsq[i] += ($i)^2}} END {for (i=1;i<=NF;i++) {printf ("%.10e\n", sqrt((sumsq[i]-sum[i]^2/NR)/NR))}}' | sed 's/[eE]+\{0,1\}/*10^/g')

PAIRWISE_AVERAGE_PER_UNIT=$(cut -f 5 "$POP".iter.transformedThetas.iteration"$iteration".borrar | awk -v OFS='\t' '{for(i=1;i<=NF;i++) {sum[i] += $i }} END {for (i=1;i<=NF;i++) {printf ("%.10e\n", sum[i]/NR )}}' | sed 's/[eE]+\{0,1\}/*10^/g') 
PAIRWISE_SD_PER_UNIT=$(cut -f 5  "$POP".iter.transformedThetas.iteration"$iteration".borrar | awk -v OFS='\t' '{for(i=1;i<=NF;i++) {sum[i] += $i; sumsq[i] += ($i)^2}} END {for (i=1;i<=NF;i++) {printf ("%.10e\n", sqrt((sumsq[i]-sum[i]^2/NR)/NR))}}' | sed 's/[eE]+\{0,1\}/*10^/g') 

DIFFERENCE_PAIRWISE_WATTERSON_SD_PER_UNIT=$(cut -f 6  "$POP".iter.transformedThetas.iteration"$iteration".borrar   |  awk -v OFS='\t' '{for(i=1;i<=NF;i++) {sum[i] += $i; sumsq[i] += ($i)^2}} END {for (i=1;i<=NF;i++) {printf ("%.10e\n", sqrt((sumsq[i]-sum[i]^2/NR)/NR ))}}' | sed 's/[eE]+\{0,1\}/*10^/g') 

TAJIMAS_D_PER_UNIT=$(echo "(($PAIRWISE_AVERAGE_PER_UNIT) - ($WATTERSON_AVERAGE_PER_UNIT))/($DIFFERENCE_PAIRWISE_WATTERSON_SD_PER_UNIT)" | bc -l | awk '{printf ("%.10e\n",$1)}' |  sed 's/[eE]+\{0,1\}/*10^/g' )

INFORMATIVESITES=$(wc -l "$POP".iter.transformedThetas.iteration"$iteration".borrar | cut -d" " -f1)

NAs=($(echo $LENGTH - $INFORMATIVESITES | bc)) 

#Before printing check all variables are full, if empty use NA
if [ -z ${LOCATION} ]; then  LOCATION=NA;  fi
if [ -z ${START_CEROBASED} ]; then  START_CEROBASED=NA;  fi
if [ -z ${END} ]; then  END=NA;  fi
if [ -z ${LENGTH} ]; then  LENGTH=NA;  fi
if [ -z ${NAs} ]; then  NAs=NA;  fi
if [ -z ${INFORMATIVESITES} ]; then  INFORMATIVESITES=NA;  fi
if [ -z ${FEATURE} ]; then  FEATURE=NA;  fi
if [ -z ${STRANDNESS} ]; then  STRANDNESS=NA;  fi
if [ -z ${FRAME} ]; then  FRAME=NA;  fi
if [ -z ${ID_GENE} ]; then  ID_GENE=NA;  fi
if [ -z ${ID} ]; then  ID=NA;  fi
if [ -z ${WATTERSON_AVERAGE_PER_UNIT} ]; then  WATTERSON_AVERAGE_PER_UNIT=NA;  fi
if [ -z ${WATTERSON_SD_PER_UNIT} ]; then  WATTERSON_SD_PER_UNIT=NA;  fi
if [ -z ${PAIRWISE_AVERAGE_PER_UNIT} ]; then  PAIRWISE_AVERAGE_PER_UNIT=NA;  fi
if [ -z ${PAIRWISE_SD_PER_UNIT} ]; then  PAIRWISE_SD_PER_UNIT=NA;  fi
if [ -z ${TAJIMAS_D_PER_UNIT} ]; then  TAJIMAS_D_PER_UNIT=NA;  fi
if [ -z ${POP} ]; then  POP=NA;  fi
if [ -z ${SPECIE} ]; then  SPECIE=NA;  fi
if [ -z ${EPOCH} ]; then  EPOCH=NA;  fi


#Paste averages, and standatd deviations

paste \
<(echo $LOCATION ) \
<(echo $START_CEROBASED ) \
<(echo $END ) \
<(echo $LENGTH ) \
<(echo $NAs) \
<(echo $INFORMATIVESITES) \
<(echo $FEATURE ) \
<(echo $STRANDNESS ) \
<(echo $FRAME ) \
<(echo $ID_GENE ) \
<(echo $ID) \
<(echo $WATTERSON_AVERAGE_PER_UNIT) \
<(echo $WATTERSON_SD_PER_UNIT) \
<(echo $PAIRWISE_AVERAGE_PER_UNIT) \
<(echo $PAIRWISE_SD_PER_UNIT) \
<(echo $TAJIMAS_D_PER_UNIT) \
<(echo $POP) \
<(echo $SPECIE) \
<(echo $EPOCH) |\
sed 's/ /\t/g'| sed 's/\t\+/\t/g'  >>  $DIVERSITY_PER_UNIT_FOLDER"$POP".per.unit.averages_iteration"$iteration".tsv

#Reset all (but POP,SPECIE or EPOCH (those should be the same during the whole iteration and if you remove POP the loop won't work)) variables before next iteration

unset LOCATION
unset START_CEROBASED
unset END
unset LENGTH
unset NAs
unset INFORMATIVESITES
unset FEATURE
unset STRANDNESS
unset FRAME
unset ID_GENE
unset ID
unset WATTERSON_AVERAGE_PER_UNIT
unset WATTERSON_SD_PER_UNIT
unset PAIRWISE_AVERAGE_PER_UNIT
unset PAIRWISE_SD_PER_UNIT
unset TAJIMAS_D_PER_UNIT

done < <(cat "$SCAFFOLDS_FOLDER"LYPA23C.CDS.GENE_promoters.GENE_introns.UTRs.ncRNA.lncRNA.lncRNA_introns.lncRNA_promoters.UCNE.intergenic.nr.gff3_"$SCAFFOLD" ) 

done &

done

# rm "$POP".iter.transformedThetas*borrar

```


Concateno con la tabla que tiene el header

c_ll_ki_n013
c_ll_no_n008
c_ll_po_n008
c_lp_do_n012
c_lp_sm_n019

## Merge different iterations

```{bash}

mkdir iterations_to_rm_when_everything_is_correct
mv *_iteration*.tsv  iterations_to_rm_when_everything_is_correct 
cd iterations_to_rm_when_everything_is_correct

echo -e "scaffold\tstart_cero_based\tend\tlength\tNAs\tinformative_sites\tfeature\tstrandness\tframe\tid_gene\tid\twatterson_ave\twatterson_sd\tpairwise_ave\tpairwise_sd\ttajimaD\tpop\tspecie\tepoch" > header.annotation

POPS=($(ls *tsv | cut -d"." -f 1 | sort | uniq))
rm *raw.tsv
for POP in "${POPS[@]}"
do
echo $POP
cat "$POP".per.unit.averages_iteration{1..10}.tsv | cat header.annotation - >> "$POP".per.unit.averages_raw.tsv
done

```

## Sanity check

```{bash}
POPS=($(ls *tsv | cut -d"." -f 1 | sort | uniq))
for POP in "${POPS[@]}"
do
echo $POP
wc -l $POP*_iteration*tsv >> "${POP}".wcperiteration.list
wc -l $POP.per.unit.averages_raw.tsv >>  "${POP}".wctotal.list
done

```

Esto me está creando una tabla que tiene una entrada por cada una de las unidades de la notación aunque eso no significa que tengan ningún resultado. No tiene sentido arrastrar toda esta información todo el rato, así que la filtro.

```{bash}
cd iterations_to_rm_when_everything_is_correct

for FILE_DIVERSITY_EACH_ITERATION in *per.unit.averages_raw.tsv
do
echo $FILE_DIVERSITY_EACH_ITERATION
awk '($12!="NA") || ($13!="NA") || ($14!="NA") || ($15!="NA") || ($16!="NA")' $FILE_DIVERSITY_EACH_ITERATION > ${FILE_DIVERSITY_EACH_ITERATION/_raw.tsv/.tsv}
done

# Sanity
wc -l *_filtered.tsv >>  wcfiltered.list

# Perfecto! Mas o menos todos son iguales.

mv *.per.unit.averages.tsv ..
rm *raw.tsv
rm *list



```


# ---------------------------------------------

# Anotación de las unidades: cromosoma y regiones; tel y centr. 

Ahora vamos a anotar esas unidades según pertenezcan o no a distintos cromosomas, y según sean teloméricas, centromérica o no. 


#### Cromosomas 

Para ello hemos usado el archivo que tienen en común las coordenadas de gato y de lince

```{bash}
head /GRUPOS/grupolince/copia_fabascal/MAPPINGS/lynx2cat_wTiger.sorted.bed
```
  
Creo una carpeta donde voy a guardar los archivos:
```{bash}
mkdir /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/chromosome_annotation
```

Los archivos de los que parto estén en:
```{bash}
/home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/diversity_per_unit/
```

Empiezo a unirlos. 

El primer Bed te devolvería un archivo con una línea por cada posición de sintenia tal que así:

lp23.s00001	0	15011	15011	11950	3061	intergenic	-	.	intergenic_region_1	intergenic_region_1	2.4096547746*10^-06	1.3458559263*10^-05	1.1416658651*10^-06	6.2801913098*10^-06	-1.7606196687*10^-01	c_ll_no_n008	ll	c	lp23.s00001	4071	4072	TCL=TTT:chrA3:15104518-15104519:NO	1

Como no queremos pasar por ahí ni guardar tanto, vamos a intentar usar awk directamente para quedarnos con lo que nos interese. 

```{bash}

screen -S chromosome_annotation

cd /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/chromosome_annotation
echo -e "scaffold\tstart_cero_based\tend\tlength\tNAs\tinformative_sites\tfeature\tstrandness\tframe\tid_gene\tid\twatterson_ave\twatterson_sd\tpairwise_ave\tpairwise_sd\ttajimaD\tpop\tspecie\tepoch\tchr\tcat_position_start\tcat_position_end" > header.chr.annotation

cd /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/diversity_per_unit/
for FILE in *.per.unit.averages.tsv
do
echo $FILE
cd /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/chromosome_annotation
intersectBed -sorted -wo -a <(tail -n +2 /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/diversity_per_unit/$FILE) -b /GRUPOS/grupolince/copia_fabascal/MAPPINGS/lynx2cat_wTiger.sorted.bed | awk -v OFS='\t' '{split ($23, a, ":"); split(a[3],b, "-"); print $1,$2,$3,$4,$5,$6,$7,$8,$9,$10,$11,$12,$13,$14,$15,$16,$17,$18,$19,a[2],b[1],b[2] }' > intermediate

mv intermediate intermediate_folder
cd intermediate_folder
awk '{print>$1}' intermediate 

for SCAFFOLD_FILE in lp23* 
do
echo $SCAFFOLD_FILE
bedtools groupby -i $SCAFFOLD_FILE -g 1-19 -c 20,21,22 -o distinct,min,max >> intermediate2
done
cat ../header.chr.annotation intermediate2 > ../${FILE/.per.unit.averages.tsv/.per.unit.averages.chr.tsv}
cd /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/chromosome_annotation
rm intermediate_folder/*
done

```

##### Sanity checks

Las unidades que están anotadas a más de una cromosoma, aparecen separadas por coma.
Así que lo que voy a hacer es buscar esas lineas y ver si coinciden con la que salían anotadas dobles anteriormente.


Una vez tenga las unidades tengo que filtrarlas en todos los archivos para que se pueda hacer un full join. 
Lista antigua:

cat list_of_repeated_features* | sort -k5,5 | uniq | wc -l  
1490
wc -l list_of_repeated_features_ki 
1484 list_of_repeated_features_ki
wc -l list_of_repeated_features_sm
1481 list_of_repeated_features_sm
wc -l list_of_repeated_features_do
1479 list_of_repeated_features_do
wc -l list_of_repeated_features_po
1488 list_of_repeated_features_po
wc -l list_of_repeated_features_no
1488 list_of_repeated_features_no

```{bash}
wc -l c_ll_ki_n013.per.unit.averages.chr.tsv
# 429404 c_ll_ki_n013.per.unit.averages.chr.tsv
grep "," c_ll_ki_n013.per.unit.averages.chr.tsv | wc -l # --> Unidades asignadas a mas de un chr. 
# 1484

# Hago la lista de unidades en más de un chr (se hace de un modo ligeramente distinto a como se hacía antes)
grep "," c_ll_ki_n013.per.unit.averages.chr.tsv  | awk -v OFS='\t' '{print $1,$2,$3,$7,$11}' | sort | sed -e 's/^[ \t]*//' > list_of_repeated_features_ki
grep "," c_ll_no_n008.per.unit.averages.chr.tsv  | awk -v OFS='\t' '{print $1,$2,$3,$7,$11}' | sort | sed -e 's/^[ \t]*//' > list_of_repeated_features_no
grep "," c_ll_po_n008.per.unit.averages.chr.tsv  | awk -v OFS='\t' '{print $1,$2,$3,$7,$11}' | sort | sed -e 's/^[ \t]*//' > list_of_repeated_features_po
grep "," c_lp_sm_n019.per.unit.averages.chr.tsv  | awk -v OFS='\t' '{print $1,$2,$3,$7,$11}' | sort | sed -e 's/^[ \t]*//' > list_of_repeated_features_sm
grep "," c_lp_do_n012.per.unit.averages.chr.tsv  | awk -v OFS='\t' '{print $1,$2,$3,$7,$11}' | sort | sed -e 's/^[ \t]*//' > list_of_repeated_features_do

#  wc -l list*
#  1479 list_of_repeated_features_do
#  1484 list_of_repeated_features_ki
#  1488 list_of_repeated_features_no
#  1488 list_of_repeated_features_po
#  1481 list_of_repeated_features_sm

cat list_of_repeated_features* | sort -k5,5 | uniq > list_of_repeated_features_all_pops

#  wc -l  list_of_repeated_features_all_pops
#  1490 list_of_repeated_features_all_pops

awk -v OFS='\t' '{print $2"_"$3"_"$4"_"$5"_"$6}' list_of_repeated_features_all_pops > list_of_repeated_features_all_pops_merged


###

for FILE in *per.unit.averages.chr.tsv 
do
echo $FILE
awk -v OFS='\t' '{print $1"_"$2"_"$3"_"$7"_"$11, $0}' $FILE | 
grep -f list_of_repeated_features_all_pops_merged - | awk -v OFS='\t' '{$21="na"; print}' > ${FILE/.per.unit.averages.chr.tsv/.per.unit.averages.chr_repeated_feature.tsv.borrar}
awk -v OFS='\t' '{print $1"_"$2"_"$3"_"$7"_"$11, $0}' $FILE | 
grep -v -f list_of_repeated_features_all_pops_merged - > ${FILE/.per.unit.averages.chr.tsv/.per.unit.averages.chr_NOT_repeated_feature.tsv.borrar}
cat <(tail -n +2 ${FILE/.per.unit.averages.chr.tsv/.per.unit.averages.chr_NOT_repeated_feature.tsv.borrar}) <( cat ${FILE/.per.unit.averages.chr.tsv/.per.unit.averages.chr_repeated_feature.tsv.borrar}) | cut -f2-  | sort -k1,1 -k2,2n -k3,3n | cat header.chr.annotation - | awk '!a[$0]++' > ${FILE/.per.unit.averages.chr.tsv/.per.unit.averages.chr_filtered.tsv}
done


# Sanity check!

awk -v OFS='\t' '{print $1,$2,$3,$7,$11}' c_ll_ki_n013.per.unit.averages.chr_filtered.tsv | sort | uniq -c | grep -v "1 " | sort | sed -e 's/^[ \t]*//' | cut -d' ' -f2  | awk '{print $4}' | sort  | uniq -c

awk -v OFS='\t' '{print $1,$2,$3,$7,$11}' c_ll_ki_n008.per.unit.averages.chr_filtered.tsv | sort | uniq -c | grep -v "1 " | sort | sed -e 's/^[ \t]*//' | cut -d' ' -f2  | awk '{print $4}' | sort  | uniq -c

awk -v OFS='\t' '{print $1,$2,$3,$7,$11}' c_ll_po_n008.per.unit.averages.chr_filtered.tsv | sort | uniq -c | grep -v "1 " | sort | sed -e 's/^[ \t]*//' | cut -d' ' -f2  | awk '{print $4}' | sort  | uniq -c

awk -v OFS='\t' '{print $1,$2,$3,$7,$11}' c_ll_no_n008.per.unit.averages.chr_filtered.tsv | sort | uniq -c | grep -v "1 " | sort | sed -e 's/^[ \t]*//' | cut -d' ' -f2  | awk '{print $4}' | sort  | uniq -c

awk -v OFS='\t' '{print $1,$2,$3,$7,$11}' c_lp_do_n012.per.unit.averages.chr_filtered.tsv | sort | uniq -c | grep -v "1 " | sort | sed -e 's/^[ \t]*//' | cut -d' ' -f2  | awk '{print $4}' | sort  | uniq -c

awk -v OFS='\t' '{print $1,$2,$3,$7,$11}' c_lp_sm_n019.per.unit.averages.chr_filtered.tsv | sort | uniq -c | grep -v "1 " | sort | sed -e 's/^[ \t]*//' | cut -d' ' -f2  | awk '{print $4}' | sort  | uniq -c

awk -v OFS='\t' '{print $1,$2,$3,$7,$11}' c_lp_sm_n012.per.unit.averages.chr_filtered.tsv | sort | uniq -c | grep -v "1 " | sort | sed -e 's/^[ \t]*//' | cut -d' ' -f2  | awk '{print $4}' | sort  | uniq -c

# No hay nada! perfecto!

# Para comprobar que las unidades que estoy incluyendo no han cambiado al incluir las posiciones y eliminar las unidades repetidas (no veo por qué deberían pero quiero asegurarme), me voy a la carpeta donde tengo las anotaciones antiguas y compruebo que no hay diferencias con las nuevas corriendo este script. 

diff <( awk '{print $1, $2, $3}' c_lp_sm_n019.per.unit.averages.chr_filtered.tsv) <( awk '{print $1, $2, $3}' ../c_lp_sm_n019.per.unit.averages.chr_filtered.tsv) | head

```


###### Sanity checks (old)

DESDE AQUI hasta cuando ponga --> FIN <-- solo lo corria cuando la anotación de chromosomas era distinta. 
Ahora con la nueva notación el sanity check es mas facil, y ya está hecho antes, y se podría pasar al siguiente paso directamente. 
```{bash}

cd /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/chromosome_annotation

# Calculo qué unidades aparecen más de una vez:

awk '{print $7"_"$11}' c_lp_do_n012.per.unit.averages.chr.tsv | sort | uniq -c | grep -v "1 " | wc -l
# Hay 1477 

# ¿Qué hay en esta lista?

 awk '{print $7"_"$11}' c_lp_do_n012.per.unit.averages.chr.tsv | sort | uniq -c | grep -v "1 " | sed -e 's/^[ \t]*//' | cut -d' ' -f2  | cut -d"_" -f 1 | sort  | uniq -c

      9 3UTR
     14 5UTR
     12 CDS
   1024 intergenic
    406 intron
      2 lncRNA
      1 ncRNA
      9 promoter

# Esta lista incluye varias features distintas.

# Dentro de estas features repetidas tengo otras casuisticas:

# Ejemplo: Un intron repetido tres veces:

# grep LYPA23C000420T1 c_lp_do_n012.per.unit.averages.chr.tsv | grep intron_62

# lp23.s05342	577057	603192	26135	15052	11083	intron	+	0	LYPA23C000420	LYPA23C000420T1_intron_62	1.9406089032*10^-04	7.1920669656*10^-03	2.7265147013*10^-04	1.0183730201*10^-02	2.5254946264*10^-02	c_lp_do_n012	lp	cchrB2
# lp23.s05342	577057	603192	26135	15052	11083	intron	+	0	LYPA23C000420	LYPA23C000420T1_intron_62	1.9406089032*10^-04	7.1920669656*10^-03	2.7265147013*10^-04	1.0183730201*10^-02	2.5254946264*10^-02	c_lp_do_n012	lp	cchrX
# lp23.s05342	577057	603192	26135	15052	11083	intron	+	0	LYPA23C000420	LYPA23C000420T1_intron_62	1.9406089032*10^-04	7.1920669656*10^-03	2.7265147013*10^-04	1.0183730201*10^-02	2.5254946264*10^-02	c_lp_do_n012	lp	cchrB1

# Este intrón aparece asignado a tres cromosomas distintos. 

# Compruebo que efectivamente se asigna a tres cromosomas en el archivo de notación

grep lp23.s05342 /GRUPOS/grupolince/copia_fabascal/MAPPINGS/lynx2cat_wTiger.sorted.bed  | grep -v chrB2

# Efectivamente hay más de un cromosoma en ese scaffold. 

# Vamos a ver que número de features presentan este problema. 

# Creo una lista PARA DOÑANA de aquellas features que son identicas también en las posiciones (lo que excluiría a los casos de las UCR que hemos hablado antes). Para ello imprimo también la posición de comienzo y la de fin.

awk -v OFS='\t' '{print $1,$2,$3,$7,$11}' c_lp_do_n012.per.unit.averages.chr.tsv | sort | uniq -c | grep -v "1 " | sort | sed -e 's/^[ \t]*//'  > list_of_repeated_features_do

# Compruebo a qué unidades pertenecen. 

 cut -d' ' -f2 list_of_repeated_features_do | awk '{print $4}' | sort  | uniq -c

      9 3UTR
     14 5UTR
     12 CDS
   1024 intergenic
    382 intron
     24 intron_lncRNA
      2 lncRNA
      1 ncRNA
      1 promoter_gene_1000
      3 promoter_lncRNA_1000
      2 promoter_lncRNA_250
      3 promoter_lncRNA_500
      
      
# Vemos que todos los numeros coinciden con las unidades repetidas que habían salido antes (promotor e intrón son la suma). Por tanto, parece que todas estas regiones repetidas están asignadas a mas de un cromosoma. Ahora vamos a ver cuantas bases son eso.

# Compruebo que en las otras poblaciones son las mismas.

cd /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/chromosome_annotation

awk -v OFS='\t' '{print $1,$2,$3,$7,$11}' c_ll_ki_n013.per.unit.averages.chr.tsv | sort | uniq -c | grep -v "1 " | sort | sed -e 's/^[ \t]*//' > list_of_repeated_features_ki

awk -v OFS='\t' '{print $1,$2,$3,$7,$11}' c_ll_no_n008.per.unit.averages.chr.tsv | sort | uniq -c | grep -v "1 " | sort | sed -e 's/^[ \t]*//' > list_of_repeated_features_no

awk -v OFS='\t' '{print $1,$2,$3,$7,$11}' c_ll_po_n008.per.unit.averages.chr.tsv | sort | uniq -c | grep -v "1 " | sort | sed -e 's/^[ \t]*//' > list_of_repeated_features_po

awk -v OFS='\t' '{print $1,$2,$3,$7,$11}' c_lp_sm_n019.per.unit.averages.chr.tsv | sort | uniq -c | grep -v "1 " | sort | sed -e 's/^[ \t]*//' > list_of_repeated_features_sm

awk -v OFS='\t' '{print $1,$2,$3,$7,$11}' c_lp_sm_n012.per.unit.averages.chr.tsv | sort | uniq -c | grep -v "1 " | sort | sed -e 's/^[ \t]*//' > list_of_repeated_features_sm_n012

awk -v OFS='\t' '{print $1,$2,$3,$7,$11}' c_ll_ki_n008.per.unit.averages.chr.tsv | sort | uniq -c | grep -v "1 " | sort | sed -e 's/^[ \t]*//' > list_of_repeated_features_ki_n008




cut -d' ' -f2 list_of_repeated_features_po | awk '{print $4}' | sort  | uniq -c

     9 3UTR
     14 5UTR
     12 CDS
   1026 intergenic
    383 intron
     30 intron_lncRNA
      2 lncRNA
      1 ncRNA
      1 promoter_gene_1000
      3 promoter_lncRNA_1000
      2 promoter_lncRNA_250
      3 promoter_lncRNA_500

cut -d' ' -f2 list_of_repeated_features_ki | awk '{print $4}' | sort  | uniq -c

      9 3UTR
     14 5UTR
     12 CDS
   1023 intergenic
    382 intron
     30 intron_lncRNA
      2 lncRNA
      1 ncRNA
      1 promoter_gene_1000
      3 promoter_lncRNA_1000
      2 promoter_lncRNA_250
      3 promoter_lncRNA_500
      
cut -d' ' -f2 list_of_repeated_features_no | awk '{print $4}' | sort  | uniq -c

    9 3UTR
     14 5UTR
     12 CDS
   1025 intergenic
    384 intron
     30 intron_lncRNA
      2 lncRNA
      1 ncRNA
      1 promoter_gene_1000
      3 promoter_lncRNA_1000
      2 promoter_lncRNA_250
      3 promoter_lncRNA_500
      
      
cut -d' ' -f2 list_of_repeated_features_sm | awk '{print $4}' | sort  | uniq -c

      9 3UTR
     14 5UTR
     12 CDS
   1025 intergenic
    382 intron
     24 intron_lncRNA
      2 lncRNA
      1 ncRNA
      1 promoter_gene_1000
      3 promoter_lncRNA_1000
      2 promoter_lncRNA_250
      3 promoter_lncRNA_500
      

# Los números no coindicen, ¿en qúe se diferencian?


diff list_of_repeated_features_do list_of_repeated_features_po
diff list_of_repeated_features_do list_of_repeated_features_ki
diff list_of_repeated_features_do list_of_repeated_features_no
diff list_of_repeated_features_do list_of_repeated_features_sm


# No son iguales, aunque las diferencias son muy pequeñas.
# He comprobado que al menos en un caso, la feature que se reporta para Norway y no para Doñana no está disponible proque esas posiciones de diversidad no están.

# Ejemplo, intergenic_region_29405 no aparece en Doñana pero sí en Sierra Morena. 

# Lo busco.


grep  intergenic_region_29405  list_of_repeated_features_sm
# 2 lp23.s16211   3567    16982   intergenic      intergenic_region_29405

      
grep lp23.s16211 c_lp_do_n012.transformedThetas_filtered  
# Nada!



# La opción sería quitar todas las unidades que aparecen repetidas al menos en una población, visto las unidades que perdemos, el número de bases no sería muy alto. Anteriormente con aproximadamente las mismas unidades (algunos intergénicos más porque ahora hemos filtrado mejor) teníamos que:


# ¿Cuantas bases perdemos?

mkdir features_repeted
cd features_repeted/

# Primero calculamos cuantas bases totales tenemos:

# Este archivo que genero tiene todas las unidades, no solo las repetidas.

awk -v OFS='\t' '{print $1,$2,$3,$7,$11}' ../c_lp_sm_n019.per.unit.averages.chr.tsv |  awk '{print >> $4; close($4)}'
rm feature 

# ¿Cuantas bases por cada unidad?

for file in * 
do
echo $file "$(awk '{sum+=$3-$2}END{print sum}' $file )"
done >> bases_total.txt

mv bases_total.txt ../

# Ahora elimino estos archivos y hago la misma cuenta para aquellas que están repetidas.

# Ten cuidado de estar en la carpeta que debes!!!
cd features_repeted/
# rm *

# ¿Cuantas repetidas?

awk -v OFS='\t' '{print $1,$2,$3,$7,$11}' ../c_lp_sm_n019.per.unit.averages.chr.tsv | sort | uniq -c | grep -v "1 " | sort | sed -e 's/^[ \t]*//' | cut -d' ' -f2-6 | awk '{print >> $4; close($4)}'

rm feature 

# ¿Cuantas bases por cada unidad?

for file in * 
do
echo $file "$(awk '{sum+=$3-$2}END{print sum}' $file )"
done >> bases_repeted.txt

mv bases_repeted.txt ../

# rm *

# ¿Qué porcentaje supone del total?

cd ..

join -1 1 -2 1 bases_total.txt bases_repeted.txt | awk -v OFS='\t' '{print $1,$2,$3,$3/$2*100}' | cat <(echo -e "feature\ttotal_bases\tbases_assinged_to_2_or_more_chr\tpercentage") - > percentage_of_bases_assigned_to_2_chr_or_more_sm.txt

rm bases_repeted.txt
rm bases_total.txt 
rm -r features_repeted/
rm list_of_repeated_features_po

cat percentage_of_bases_assigned_to_2_chr_or_more_sm.txt

<!-- feature total_bases     bases_assinged_to_2_or_more_chr percentage -->
<!-- 3UTR    10888821        31596   0.290169 -->
<!-- 5UTR    39173937        747134  1.90722 -->
<!-- CDS     29521748        12656   0.0428701 -->
<!-- intergenic      1575985801      108644867       6.89377 -->
<!-- intron  781408454       12974162        1.66036 -->
<!-- intron_lncRNA   36490600        775481  2.12515 -->
<!-- lncRNA  3932428 1503    0.0382207 -->
<!-- ncRNA   319359  37      0.0115857 -->
<!-- promoter_gene_1000      6885450 1001    0.0145379 -->
<!-- promoter_lncRNA_1000    2905061 3003    0.103371 -->
<!-- promoter_lncRNA_250     660912  502     0.0759556 -->
<!-- promoter_lncRNA_500     1391917 1503    0.107981 -->


# Esto es un ejemplo,pero todas las poblaciones son más o menos igual, así que no se perderían tantas bases. 

```

Creo la lista de unidades que tengo que filtrar.

Sanity checks

```{bash}
cat list_of_repeated_features* | sort -k5,5 | uniq | wc -l  
1490
wc -l list_of_repeated_features_ki 
1484 list_of_repeated_features_ki
wc -l list_of_repeated_features_sm
1481 list_of_repeated_features_sm
wc -l list_of_repeated_features_do
1479 list_of_repeated_features_do
wc -l list_of_repeated_features_po
1488 list_of_repeated_features_po
wc -l list_of_repeated_features_no
1488 list_of_repeated_features_no
wc -l list_of_repeated_features_sm_n012
1481 list_of_repeated_features_sm_n012
wc -l  list_of_repeated_features_ki_n008
1485 list_of_repeated_features_ki_n008

```

Creo la lista

```{bash}
cat list_of_repeated_features* | sort -k5,5 | uniq > list_of_repeated_features_all_pops
```

Sanity checks

```{bash}

awk -v OFS='\t' '{print $2"_"$3"_"$4"_"$5"_"$6}' list_of_repeated_features_all_pops > list_of_repeated_features_all_pops_merged

wc -l *per.unit.averages.chr.tsv 
#   431096 c_ll_ki_n008.per.unit.averages.chr.tsv
#   431034 c_ll_ki_n013.per.unit.averages.chr.tsv
#   431217 c_ll_no_n008.per.unit.averages.chr.tsv
#   430907 c_ll_po_n008.per.unit.averages.chr.tsv
#   430255 c_lp_do_n012.per.unit.averages.chr.tsv
#   430829 c_lp_sm_n012.per.unit.averages.chr.tsv
#   430853 c_lp_sm_n019.per.unit.averages.chr.tsv

awk -v OFS='\t' '{print $1"_"$2"_"$3"_"$7"_"$11, $0}' c_lp_sm_n019.per.unit.averages.chr.tsv | grep -f list_of_repeated_features_all_pops_merged - | awk '{print $1}' | sort | uniq -c | sort | sed -e 's/^[ \t]*//' | awk '{print $1"_"$2}' > lista_de_unidades_repetidas_que_encuentra

head lista_de_unidades_repetidas_que_encuentra

wc -l lista_de_unidades_repetidas_que_encuentra
1478 lista_de_unidades_repetidas_que_encuentra

awk -v OFS='\t' '{print $1"_"$2"_"$3"_"$4"_"$5"_"$6}' list_of_repeated_features_sm > lista_de_unidades_repetidas_que_hay

head lista_de_unidades_repetidas_que_hay

 wc -l lista_de_unidades_repetidas_que_hay
1478 lista_de_unidades_repetidas_que_hay

grep -f -v lista_de_unidades_repetidas_que_encuentra lista_de_unidades_repetidas_que_hay
# Nada! genial!

awk -v OFS='\t' '{print $1"_"$2"_"$3"_"$7"_"$11, $0}' c_lp_sm_n019.per.unit.averages.chr.tsv | 
grep -f list_of_repeated_features_all_pops_merged - | wc -l 
# 3102

awk '{sum+=$1}END{print sum}' list_of_repeated_features_sm
# 3102
# Perfecto!

```

Modifico mis archivos para eliminar la información de cromosoma cuando la unidad esté en dos o más cromosomas. 


```{bash}

###

for FILE in *per.unit.averages.chr.tsv 

do

echo $FILE

awk -v OFS='\t' '{print $1"_"$2"_"$3"_"$7"_"$11, $0}' $FILE | 
grep -f list_of_repeated_features_all_pops_merged - | awk -v OFS='\t' '{$21="na"; print}' > ${FILE/.per.unit.averages.chr.tsv/.per.unit.averages.chr_repeated_feature.tsv.borrar}

awk -v OFS='\t' '{print $1"_"$2"_"$3"_"$7"_"$11, $0}' $FILE | 
grep -v -f list_of_repeated_features_all_pops_merged - > ${FILE/.per.unit.averages.chr.tsv/.per.unit.averages.chr_NOT_repeated_feature.tsv.borrar}

cat <(tail -n +2 ${FILE/.per.unit.averages.chr.tsv/.per.unit.averages.chr_NOT_repeated_feature.tsv.borrar}) <( cat ${FILE/.per.unit.averages.chr.tsv/.per.unit.averages.chr_repeated_feature.tsv.borrar}) | cut -f2-  | sort -k1,1 -k2,2n -k3,3n | cat header.chr.annotation - | awk '!a[$0]++' > ${FILE/.per.unit.averages.chr.tsv/.per.unit.averages.chr_filtered.tsv}

done


# Sanity check!

awk -v OFS='\t' '{print $1,$2,$3,$7,$11}' c_ll_ki_n013.per.unit.averages.chr_filtered.tsv | sort | uniq -c | grep -v "1 " | sort | sed -e 's/^[ \t]*//' | cut -d' ' -f2  | awk '{print $4}' | sort  | uniq -c

awk -v OFS='\t' '{print $1,$2,$3,$7,$11}' c_ll_ki_n008.per.unit.averages.chr_filtered.tsv | sort | uniq -c | grep -v "1 " | sort | sed -e 's/^[ \t]*//' | cut -d' ' -f2  | awk '{print $4}' | sort  | uniq -c

awk -v OFS='\t' '{print $1,$2,$3,$7,$11}' c_ll_po_n008.per.unit.averages.chr_filtered.tsv | sort | uniq -c | grep -v "1 " | sort | sed -e 's/^[ \t]*//' | cut -d' ' -f2  | awk '{print $4}' | sort  | uniq -c

awk -v OFS='\t' '{print $1,$2,$3,$7,$11}' c_ll_no_n008.per.unit.averages.chr_filtered.tsv | sort | uniq -c | grep -v "1 " | sort | sed -e 's/^[ \t]*//' | cut -d' ' -f2  | awk '{print $4}' | sort  | uniq -c

awk -v OFS='\t' '{print $1,$2,$3,$7,$11}' c_lp_do_n012.per.unit.averages.chr_filtered.tsv | sort | uniq -c | grep -v "1 " | sort | sed -e 's/^[ \t]*//' | cut -d' ' -f2  | awk '{print $4}' | sort  | uniq -c

awk -v OFS='\t' '{print $1,$2,$3,$7,$11}' c_lp_sm_n019.per.unit.averages.chr_filtered.tsv | sort | uniq -c | grep -v "1 " | sort | sed -e 's/^[ \t]*//' | cut -d' ' -f2  | awk '{print $4}' | sort  | uniq -c

awk -v OFS='\t' '{print $1,$2,$3,$7,$11}' c_lp_sm_n012.per.unit.averages.chr_filtered.tsv | sort | uniq -c | grep -v "1 " | sort | sed -e 's/^[ \t]*//' | cut -d' ' -f2  | awk '{print $4}' | sort  | uniq -c


# Perfecto!

```


#### Telomeros2m

El archivo de interés, es: 
```{bash}
/GRUPOS/grupolince/telomers_centromers_definition/tel2m_regions_based_on_synteny_1000bp.bed
```

Creo una carpeta donde voy a guardar los archivos:
```{bash}
mkdir /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/region_annotation
```

Los archivos de los que parto estén en:
```{bash}
/home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/diversity_per_unit/
```

Empiezo a unirlos. 

```{bash}
cd /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/diversity_per_unit/

files_per_pop=($(ls *.per.unit.averages.tsv))

cd /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/region_annotation

echo -e "scaffold\tstart_cero_based\tend\tlength\tNAs\tinformative_sites\tfeature\tstrandness\tframe\tid_gene\tid\twatterson_ave\twatterson_sd\tpairwise_ave\tpairwise_sd\ttajimaD\tpop\tspecie\tepoch\ttel2m_bases\ttel2m_percentage" > header.region.tel2m.tsv

for file_pop in ${files_per_pop[@]}
do
echo $file_pop
intersectBed -sorted -wo -a <(tail -n +2 /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/diversity_per_unit/$file_pop) -b <(awk -v OFS='\t' '{print $1,$2, $3}' /GRUPOS/grupolince/telomers_centromers_definition/tel2m_regions_based_on_synteny_1000bp.bed) | awk -v OFS='\t' '{print $1,$2,$3,$4,$5,$6,$7,$8,$9,$10,$11,$12,$13,$14,$15,$16,$17,$18,$19,$23 }' | awk '!a[$0]++' | bedtools groupby -i stdin -g 1-19 -c 20 -o sum | awk -v OFS='\t'  '{percentage=$20/$4; printf "%s\t%f\n",$0,percentage}' | cat header.region.tel2m.tsv - > ${file_pop/.per.unit.averages.tsv/.per.unit.averages.region.tel2m.tsv}
done

```

Esto me devuelve algo como así:
lp23.s00006	14749	29606	14857	8272	6585	intergenic	-	.	intergenic_region_445	intergenic_region_445	6.8371515163*10^-04	1.2551611729*10^-02	1.1155938889*10^-03	2.1427002344*10^-02	4.4628203002*10^-02	c_lp_sm_n019	lp	c	ChrB2 485

donde el último número representa las bases que solapan. 

#### Telomeros10m

El archivo de interés, es: 
```{bash}
/GRUPOS/grupolince/telomers_centromers_definition/tel0-10m_regions_based_on_synteny_1000bp.bed
```

Voy a guardar los archivos en la carpeta anteriormente creada:
```{bash}
/home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/region_annotation
```

Los archivos de los que parto estén en:
```{bash}
/home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/diversity_per_unit/
```

Empiezo a unirlos. 

```{bash}
cd /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/diversity_per_unit/

files_per_pop=($(ls *.per.unit.averages.tsv))

cd /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/region_annotation

echo -e "scaffold\tstart_cero_based\tend\tlength\tNAs\tinformative_sites\tfeature\tstrandness\tframe\tid_gene\tid\twatterson_ave\twatterson_sd\tpairwise_ave\tpairwise_sd\ttajimaD\tpop\tspecie\tepoch\ttel10m_bases\ttel10m_percentage" > header.region.tel10m.tsv

for file_pop in ${files_per_pop[@]}
do
echo $file_pop
intersectBed -sorted -wo -a <(tail -n +2 /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/diversity_per_unit/$file_pop) -b <(awk -v OFS='\t' '{print $1,$2, $3}' /GRUPOS/grupolince/telomers_centromers_definition/tel0-10m_regions_based_on_synteny_1000bp.bed) | awk -v OFS='\t' '{print $1,$2,$3,$4,$5,$6,$7,$8,$9,$10,$11,$12,$13,$14,$15,$16,$17,$18,$19,$23 }' | awk '!a[$0]++' |  bedtools groupby -i stdin -g 1-19 -c 20 -o sum | awk -v OFS='\t'  '{percentage=$20/$4; printf "%s\t%f\n",$0,percentage}' | cat header.region.tel10m.tsv - > ${file_pop/.per.unit.averages.tsv/.per.unit.averages.region.tel10m.tsv}
done

```


Y por último los centrómeros:

#### Centrómeros

El archivo de interés, es: 
```{bash}
/GRUPOS/grupolince/telomers_centromers_definition/centr_regions_based_on_synteny_1000bp.bed
```

Voy a guardar los archivos en la carpeta anteriormente creada:
```{bash}
/home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/region_annotation
```

Los archivos de los que parto estén en:
```{bash}
/home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/diversity_per_unit/
```

Empiezo a unirlos. 

```{bash}
cd /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/diversity_per_unit/

files_per_pop=($(ls *.per.unit.averages.tsv))

cd /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/region_annotation

echo -e "scaffold\tstart_cero_based\tend\tlength\tNAs\tinformative_sites\tfeature\tstrandness\tframe\tid_gene\tid\twatterson_ave\twatterson_sd\tpairwise_ave\tpairwise_sd\ttajimaD\tpop\tspecie\tepoch\tcentr_bases\tcentr_percentage" > header.region.centr.tsv

for file_pop in ${files_per_pop[@]}
do
echo $file_pop
# En el awk que le aplico a la region clasificada como cent, lo que estoy haciendo es eliminar el cromsoma. Así aunque esté asignado a distintos cromosomas, yo sí m voy a creer que es región centromérica. 
intersectBed -sorted -wo -a <(tail -n +2 /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/diversity_per_unit/$file_pop) -b <(awk -v OFS='\t' '{print $1,$2, $3}' /GRUPOS/grupolince/telomers_centromers_definition/centr_regions_based_on_synteny_1000bp.bed) | awk -v OFS='\t' '{print $1,$2,$3,$4,$5,$6,$7,$8,$9,$10,$11,$12,$13,$14,$15,$16,$17,$18,$19,$23,$24 }' | awk '!a[$0]++' |  bedtools groupby -i stdin -g 1-19 -c 20 -o sum | awk -v OFS='\t'  '{percentage=$20/$4; printf "%s\t%f\n",$0,percentage}' | cat header.region.centr.tsv - > ${file_pop/.per.unit.averages.tsv/.per.unit.averages.region.centr.tsv}
done


# Con el bedtools groupby agrupo a regiones que estaban partidas. 
```

Ahora borro lo que no necesito

```{bash}
# rm header.region.*

```

Sanity checks

```{bash}
awk -v OFS='\t' '{print $1,$2,$3,$7,$11}' c_lp_sm_n019.per.unit.averages.region.centr_filtered.tsv | sort | uniq -c | grep -v "1 " | head

# Nada perfecto! Y además había comprobado que antes salían dos regiones independientes, pero ahora con el groupby se ha arreglado y se ha sumado bien las columnas. 


awk -v OFS='\t' '{print $1,$2,$3,$7,$11}' c_ll_ki_n013.per.unit.averages.region.centr_filtered.tsv | sort | uniq -c | grep -v "1 " | head

# Nada perfecto!

```


# ------------------------------------------

# Descargar tablas 

Primero me bajo las tablas.


```{bash}

scp mlucena@genomics-b.ebd.csic.es:/home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/diversity_per_unit/*tsv /Users/marialucenaperez/Documents/WG_lynx_diversity_per_unit/raw_tables/
scp mlucena@genomics-b.ebd.csic.es:/home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/region_annotation/*.tsv /Users/marialucenaperez/Documents/WG_lynx_diversity_per_unit/raw_tables/
scp mlucena@genomics-b.ebd.csic.es:/home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/chromosome_annotation/*.per.unit.averages.chr_filtered.tsv /Users/marialucenaperez/Documents/WG_lynx_diversity_per_unit/raw_tables/

```

Tambien tengo descargada la tabla de recombinación. 


# ------------------------------------------

# OLD (before May 2018)

Correcciones diversity per feature!!

```{bash}

# 8/9/2017: Haciendo un plot me he dado cuenta que hay algunas filas duplicadas en el archivo original .gff3, corresponden a UTR. 
# En la carpeta /home/mlucena/grupolince/notation, están especificadas cuales son. He corregido el gff3, pero como no voy a volver a correr mis archivos para sacar los números, voy a corregir todos los originales para tenerlo todo bien desde el principio. 

# Check:

# sort c_ll_ki_n013.per.unit.averages.tsv | uniq -c | sort -nr | head

# 2 lp23.s36682     338697  338929  232     -1      3UTR    +       .       LYPA23C000008   LYPA23C000008   1.4564973179*10^-06     4.5837356691*10^-06     4.4811971320*10^-07     1.3890498033*10^-06     -3.1562808160*1
# 2 lp23.s36682     328523  328962  439     75      5UTR    -       .       LYPA23C000003   LYPA23C000003   5.0803158132*10^-06     1.2267826914*10^-05     1.8178262696*10^-06     4.0698820431*10^-06     -3.9712504595*1
# 2 lp23.s36682     279677  279811  134     -1      3UTR    +       .       LYPA23C000015   LYPA23C000015   1.0437203878*10^-06     4.1265547119*10^-06     3.1206706874*10^-07     1.2333416354*10^-06     -2.5288512495*1
# 2 lp23.s36682     100823  101610  787     82      3UTR    +       .       LYPA23C000004   LYPA23C000004   3.7669060166*10^-04     9.8624719772*10^-03     3.9965744098*10^-04     1.0556168131*10^-02     3.3103961265*10
# 2 lp23.s26402     92633   95884   3251    166     3UTR    +       .       LYPA23C000017   LYPA23C000017   9.8348133084*10^-06     3.1577310510*10^-04     4.3617798050*10^-06     1.5841654621*10^-04     -3.4633370151*1
# 2 lp23.s10719     96754   96794   40      -1      5UTR    -       .       LYPA23C000041   LYPA23C000041   3.0378406270*10^-06     1.0043475395*10^-06     1.0387353248*10^-06     3.9400597979*10^-07     -3.2716817179*1
# 2 lp23.s10719     156260  156378  118     96      5UTR    -       .       LYPA23C000045   LYPA23C000045   1.3834272997*10^-05     2.2261745529*10^-06     6.3570507930*10^-06     1.2906641862*10^-06     -7.9734277877*1
# 1 scaffold        start   end     length  NAs     feature strandness      frame   id_gene id      watterson_ave   watterson_sd    pairwise_ave    pairwise_sd     tajimaD informative_sites       pop     specie
# 1 lp23.s41700     1       200     199     199     intergenic      -       .       intergenic_region_65795 intergenic_region_65795         0       c_ll_ki_n013    c_ll
# 1 lp23.s41699     1       200     199     199     intergenic      -       .       intergenic_region_65794 intergenic_region_65794         0       c_ll_ki_n013    c_ll


# 491637 c_ll_ki_n013.per.unit.averages.tsv

# Efectivamente aquí también está duplicados, así que corro el loop para corregirlo:

for i in  *.per.unit.averages.tsv
do
echo $i
POP=$(echo ${i} | cut -d "." -f 1 )
SPECIE=$(echo $POP | cut -d"_" -f 1-2)
awk '!seen[$0]++' $i > ${i/.tsv/.corrected.tsv} # remove duplicated rows.
mv ${i/.tsv/.corrected.tsv} $i
done



# Create a unique file with all the information

awk 'FNR==1 && NR!=1{next;}{print}' c_lp*averages.tsv > c_lp_do_n012-c_lp_sm_n019.per.unit.averages.per_specie.tsv
awk 'FNR==1 && NR!=1{next;}{print}' c_ll*averages.tsv > c_ll_ki_n013-c_ll_no_n008-c_ll_po_n008.per.unit.averages.per_specie.tsv


# Compruebo que ha funcionado:

# sort c_ll_ki_n013.per.unit.averages.tsv | uniq -c | sort -nr | head
# 1 scaffold	start	end	length	NAs	feature	strandness	frame	id_gene	id	watterson_ave	watterson_sd	pairwise_ave	pairwise_sd	tajimaD	informative_sites	pop	specie
# 1 lp23.s41700	1	200	
      
# Perfecto!!
      
```


OJO!!

Calcular luego en R medidas pareadas!!! cada unidad con su equivalente en las tres poblaciones!


```{bash}
nota


wc -l c_ll_ki_n013-c_ll_no_n008-c_ll_po_n008.per.unit.averages.per_specie.tsv
1474888 c_ll_ki_n013-c_ll_no_n008-c_ll_po_n008.per.unit.averages.per_specie.tsv


awk '{if ($18 == "") print $0}' c_ll_ki_n013-c_ll_no_n008-c_ll_po_n008.per.unit.averages.per_specie.tsv  | wc -l
134260

# Tengo 134260 lineas que no tienen todos los campos, eso supone un 9,1% de total. 
# ¿A cuantas les faltan todos los campos y por tanto no ha perjudicados los cálculos? (A todas las que tengas informative sites ($11), igual a cero)

awk '{if ($18 == "") print $0}' c_ll_ki_n013-c_ll_no_n008-c_ll_po_n008.per.unit.averages.per_specie.tsv  | awk '{if ($11=="0") print $0}' | wc -l
133589

# Por tanto, de las 134260, a 133589 le faltan todos los datos, por tanto sólo hay 671 lineas con algunos valores sí y otros no. 

133589-134260=671

awk '{if ($18 == "") print $0}' c_ll_ki_n013-c_ll_no_n008-c_ll_po_n008.per.unit.averages.per_specie.tsv  | awk '{if ($11!="0") print $0}' | wc -l
671



# De estos, parece que hay muchos con sitios informativos=1, por lo que asumo que no puede calcular las tajimaD. 

awk '{if ($18 == "") print $0}' c_ll_ki_n013-c_ll_no_n008-c_ll_po_n008.per.unit.averages.per_specie.tsv  | awk '{if ($11!="0") print $0}' | head
lp23.s00005	1472211	1472211	0	-1	CDS	-	0	LYPA23C006624	LYPA23C006624P4_1_1	9.1722861064*10^-06	0.0000000000*10^00	3.7772361236*10^-06	0.0000000000*10^00		1	c_ll_ki_n013	c_ll
lp23.s00006	42798	43028	230	229	intron	+	0	LYPA23C020681	LYPA23C020681T1_intron_13	7.9066451013*10^-06	0.0000000000*10^00	3.1403854839*10^-06	0.0000000000*10^00		1	c_ll_ki_n013	c_ll
lp23.s00010	215823	215849	26	25	lncRNA	+

# Concretamente encontramos,

 awk '{if ($18 == "") print $0}' c_ll_ki_n013-c_ll_no_n008-c_ll_po_n008.per.unit.averages.per_specie.tsv  | awk '{if ($11!="0") print $0}' | awk '{if ($15="1") print $0}' | wc -l
671

# Todo cuadra. 

# 671 supoone un 0.045 del total, así que yo creo que no influye o no mucho en los resultados que tenog. En todo caso, como en algunos casos la coumna de informative site 01 está donde watterson debería estar, ahí si pero en el resto no. 


```




Este script nos devuelve una gráfica por cada población. Hay un segundo script que devuelve una gráfica para lynx lynx y otra para lynx pardinus. 

```{r}

library ("ggplot2")
library ("plyr")
library ("RColorBrewer")
library ("scales")
library ("magrittr")

# wd <- "/Users/marialucenaperez/Owncloud/publico/WG_diversity/ANGSD/sfs/diversity_per_unit"
wd <- "/home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/diversity_per_unit"
# wd <- "/Users/marialucenaperez/Desktop/Diversity_per_unit"

finsdiversity = list.files(path = wd,pattern="*per.unit.averages.tsv$")
INFORMATIVE_SITES=1
PERCENTAGE_COVERED=0.5

for (i in 1:length(finsdiversity))
{
  infile <- finsdiversity[i]
  name_diversity <- unlist(strsplit(finsdiversity[i], "[.]"))

#### Para que almacene varias dataframe con el nombre correcto
#  dataframename <- paste("data_diversity", name_diversity[1], sep ='_')
#  assign(dataframename, read.csv (paste(wd,infile, sep="/"), header = T, sep = '\t',stringsAsFactors = FALSE,  row.names=NULL, na.strings = "", dec=".", colClasses=c("watterson_ave"="character")))

dataframe <- read.csv (paste(wd,infile, sep="/"), header = T, sep = '\t',stringsAsFactors = FALSE,  row.names=NULL, na.strings = "", dec=".")

dataframe = dataframe[-1,]
dataframe$watterson_ave <- as.numeric(gsub("\\*10\\^","e",dataframe$watterson_ave))
dataframe$pairwise_ave <- as.numeric(gsub("\\*10\\^","e",dataframe$pairwise_ave))
dataframe$tajimaD <- as.numeric(gsub("\\*10\\^","e",dataframe$tajimaD))
#dataframe  <- dataframe[complete.cases(dataframe),] ## --> No se que pasa que la columna de specie sale NA

########################################################
# Filtering
dataframe <- dataframe %>% 
  mutate(percentage_covered=ifelse(NAs>0, (as.numeric(length)-as.numeric(NAs))/as.numeric(length), 1)) %>% dplyr::filter(percentage_covered>PERCENTAGE_COVERED) %>% dplyr::filter(informative_sites>INFORMATIVE_SITES)
name_diversity[1] <- paste(name_diversity[1],".percentagecovered",PERCENTAGE_COVERED,"_mininformativesites",INFORMATIVE_SITES, sep="")
########################################################

df.pm <- ddply(dataframe, "feature", summarise, weightedmean=weighted.mean(pairwise_ave, informative_sites))

df.wm <- ddply(dataframe, "feature", summarise, weightedmean=weighted.mean(watterson_ave, informative_sites))


# pdf(file = paste(wd,"/Diversity_by_feature_pairwise",name_diversity[1],".pdf", sep = ""))
ggplot(data = dataframe, aes(x=feature, y = pairwise_ave)) + 
  geom_boxplot()+
  geom_point(data=df.pm,aes(x=feature, y = weightedmean),shape = 23, 
             size = 3, inherit.aes=FALSE) +
  # scale_y_continuous(limits = c(-20,-3), expand =c(0,0), labels = comma) +
  facet_grid(.~feature, scales="free") +
  scale_y_continuous(trans = 'log10') +
  scale_x_discrete("Genomic feature") +
  xlab(label = "Genomic feature") + #x title
  ylab(label = "Pairwise average") + # y title
  theme_bw() +  #theme selection for background and lines
  # scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  ggtitle(paste ("Diversity by feature",name_diversity[1], sep = " " )) +
  theme(axis.line.x = element_line(color="black", size = 0.5),
        axis.line.y = element_line(color="black", size = 0.5),
        axis.text.x = element_text(angle = 60, vjust = 0.8, hjust = 1),
        axis.text.y = element_text(vjust = 0.2, hjust = 0.2),
        axis.title.y = element_text(margin=margin(r=0.3, unit="cm")),
        axis.title.x = element_text(margin=margin(t=0.5, unit="cm")),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.border = element_blank(),
        panel.background = element_blank(),
        strip.background = element_blank(),
        strip.text.x = element_blank())
ggsave (file = paste(wd,"/",name_diversity[1],".diversity_feature_pairwise.pdf", sep = ""), device = "pdf")
# dev.off()



# pdf(file = paste(wd,"/Diversity_by_feature_watterson",name_diversity[1],".pdf", sep = ""))

ggplot(data = dataframe, aes(x=feature, y = watterson_ave)) + 
  geom_boxplot()+
  geom_point(data=df.wm,aes(x=feature, y = weightedmean),shape = 23, 
             size = 3, inherit.aes=FALSE) +
  # scale_y_continuous(limits = c(-20,-3), expand =c(0,0), labels = comma) +
  facet_grid(.~feature, scales="free") +
  scale_y_continuous(trans = 'log10') +
  scale_x_discrete("Genomic feature") +
  xlab(label = "Genomic feature") + #x title
  ylab(label = "Watterson average") + # y title
  theme_bw() +  #theme selection for background and lines
  # scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  ggtitle(paste ("Diversity by feature",name_diversity[1], sep = " " )) +
  theme(axis.line.x = element_line(color="black", size = 0.5),
        axis.line.y = element_line(color="black", size = 0.5),
        axis.text.x = element_text(angle = 60, vjust = 0.8, hjust = 1),
        axis.text.y = element_text(vjust = 0.2, hjust = 0.2),
        axis.title.y = element_text(margin=margin(r=0.3, unit="cm")),
        axis.title.x = element_text(margin=margin(t=0.5, unit="cm")),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.border = element_blank(),
        panel.background = element_blank(),
        strip.background = element_blank(),
        strip.text.x = element_blank())

ggsave(filename = paste(wd,"/", name_diversity[1],".diversity_feature_watterson.pdf", sep = ""), device = "pdf")


ggplot(data = dataframe, aes(x=feature, y = tajimaD)) + 
  geom_boxplot()+
  # scale_y_continuous(limits = c(-20,-3), expand =c(0,0), labels = comma) +
  facet_grid(.~feature, scales="free") +
  #  scale_y_continuous(trans = 'log10') +
  scale_x_discrete("Genomic feature") +
  xlab(label = "Genomic feature") + #x title
  ylab(label = "Watterson average") + # y title
  theme_bw() +  #theme selection for background and lines
  # scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  ggtitle(paste ("Diversity by feature",name_diversity[1], sep = " " )) +
  theme(axis.line.x = element_line(color="black", size = 0.5),
        axis.line.y = element_line(color="black", size = 0.5),
        axis.text.x = element_text(angle = 60, vjust = 0.8, hjust = 1),
        axis.text.y = element_text(vjust = 0.2, hjust = 0.2),
        axis.title.y = element_text(margin=margin(r=0.3, unit="cm")),
        axis.title.x = element_text(margin=margin(t=0.5, unit="cm")),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.border = element_blank(),
        panel.background = element_blank(),
        strip.background = element_blank(),
        strip.text.x = element_blank())
ggsave(filename = paste(wd,"/",name_diversity[1],".percentagecovered",PERCENTAGE_COVERED,"_mininformativesites",INFORMATIVE_SITES,".diversity_feature_tajimaD.pdf", sep = ""), device = "pdf")


# Exploratory plots

jpeg(paste(wd,"/",name_diversity[1],".plot1.jpg", sep=""))
plot (dataframe$informative_sites, dataframe$tajimaD)
dev.off()

jpeg(paste(wd,"/",name_diversity[1],".plot2.jpg", sep=""))
plot (dataframe$pairwise_ave, dataframe$watterson_ave)
dev.off()

jpeg(paste(wd,"/",name_diversity[1],".plot3.jpg", sep=""))
plot (dataframe$pairwise_ave, dataframe$tajimaD)
dev.off()

jpeg(paste(wd,"/",name_diversity[1],"plot4.jpg", sep=""))
plot (dataframe$watterson_ave, dataframe$tajimaD)
dev.off()

jpeg(paste(wd,"/",name_diversity[1],"plot5.jpg", sep=""))
plot (dataframe$informative_sites, dataframe$pairwise_ave)
dev.off()

jpeg(paste(wd,"/",name_diversity[1],"plot6.jpg", sep=""))
plot (dataframe$informative_sites, dataframe$watterson_ave)
dev.off()


}


# Cosas útiles outliers.shape=NA.

```


Comparación por pares unidad


```{r}

library ("ggplot2")
library ("plyr")
library ("RColorBrewer")
library ("scales")
library ("magrittr")
library(dplyr)

# wd <- "/Users/marialucenaperez/Owncloud/publico/WG_diversity/ANGSD/sfs/diversity_per_unit"
wd <- "/Users/marialucenaperez/Owncloud/publico/WG_diversity/ANGSD/sfs/diversity_per_unit"
# wd <- "/Users/marialucenaperez/Desktop/Diversity_per_region"

finsdiversity = list.files(path = wd,pattern="*.per.unit.averages.tsv$")

for (i in 1:length(finsdiversity))
{
  infile <- finsdiversity[i]
  name_diversity <- unlist(strsplit(finsdiversity[i], "[.]"))
dataframe <- read.csv (paste(wd,infile, sep="/"), header = T, sep = '\t',stringsAsFactors = FALSE,  row.names=NULL, na.strings = "", dec=".")

dataframe = dataframe[-1,]
dataframe$watterson_ave <- as.numeric(gsub("\\*10\\^","e",dataframe$watterson_ave))
dataframe$pairwise_ave <- as.numeric(gsub("\\*10\\^","e",dataframe$pairwise_ave))
dataframe$tajimaD <- as.numeric(gsub("\\*10\\^","e",dataframe$tajimaD))

#### Para que almacene varias dataframe con el nombre correcto
  dataframename <- paste("data_diversity", name_diversity[1], sep ='_')
  assign(dataframename, dataframe)
  
  

}


pair_comparison <- full_join (data_diversity_c_ll_ki_n013, data_diversity_c_ll_no_n008, by=c("row.names"="row.names","scaffold"="scaffold","start"="start", "end"="end",  "feature"="feature", "strandness"="strandness", "frame"="frame", "id_gene"="id_gene", "id"="id")  )

colourCount = length(unique(pair_comparison$feature))
getPalette = colorRampPalette(brewer.pal(9, "Paired"))


ggplot (filter(pair_comparison), aes(x= watterson_ave.x, y=watterson_ave.y , fill=feature, colour=feature)) +
  geom_point(alpha=1/100) +
  geom_smooth(method = "lm", se = FALSE) +
  geom_abline(intercept = 0) +
  scale_fill_manual(values = getPalette(colourCount))+
  scale_colour_manual(values = getPalette(colourCount)) +
   theme(legend.position="bottom") +
  guides(fill=guide_legend(nrow=2))

```











OLD (before 18/05/2018)

Global diversity per feature

I made a gff3 file with all the characteristics that we want to evaluate. 
Now that I have the file to run over the calculus per unit. To do it in a efficient way I will split this file into as many files as scaffolds. I will do the same with my thetas file. 
That way I will just have to search over the file that contains the scaffold, and bedtools will be faster.  

ESTE ES EL ARCHIVO (2018/04/03): 

/GRUPOS/grupolince/Lyp_annotation_Apr14_final/LYPA23C.CDS.GENE_promoters.GENE_introns.UTRs.ncRNA.lncRNA.lncRNA_introns.lncRNA_promoters.intergenic.UCR.nr.gff3

First I separate transformedThetas per scaffold. 

```{bash}

cd /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/diversity_per_unit

NEW_FOLDER=("c_ll_ki_separated_by_scaffold" "c_ll_po_separated_by_scaffold" "c_ll_no_separated_by_scaffold" "c_lp_do_separated_by_scaffold" "c_lp_sm_separated_by_scaffold")

for FOLDER in "${NEW_FOLDER[@]}"
do
echo "---------------------------------------------------$FOLDER---------------------------------------------------"
mkdir $FOLDER;
done
 
 
POPS=("c_ll_no_n008" "c_ll_po_n008"  "c_lp_do_n012" "c_lp_sm_n019" "c_ll_ki_n013")

########################


for POP in "${POPS[@]}"
do
echo $POP
cd /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/
mv "$POP".transformedThetas "$POP"_separated_by_scaffold/
cd  /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/"$POP"_separated_by_scaffold
# Create multiple files base on one column: transformedThetas:
awk '{print >> $1; close($1)}' "$POP".transformedThetas
rm scaffold # esto borra la primera linea que forma un archivo llamado scaffold.
# Rename the files: transformedThetas

for file in lp23*
do
echo $file
mv $file ${file/lp23/"$POP".transformedThetas_lp23}
done
done

```


Diversity per feature.

14/05/2018 --> Este script es válido, el problema es que en su momento tiraba de un archivo de notaciónq que no estaba bien 100% (ya si coge el archivo adecuado) y por eso tuve que hacer todas las modificaciones que explico abajo, pero funciona bien. 

De hecho, para las poblaciones de kirov, noruega, polonia y doñana, los archivos de notación que aparecían en este script y con el que se lanzaron estas poblaciones eran otros. Estos archivos era:
el archivo original de notación + los UCR.

A partir de mayo, cuando nos dimos cuenta del fallo (UCR solapaban con intergenic) lo que hicimos fue reemplazar este archivo de notación, por otro en el que intergenic se definiera como todo lo que no es funcional, incluidas las UCR. Este proceso está explicado en el script de notación. En este script también he cambiado el archivo al que se llama (/GRUPOS/grupolince/Lyp_annotation_Apr14_final/LYPA23C.CDS.GENE_promoters.GENE_introns.UTRs.ncRNA.lncRNA.lncRNA_introns.lncRNA_promoters.UCR.intergenic.nr.gff3), porque lo he necesitado relanzar para sierra morena. 

Aun así quiero que quede constancia de que aunque sierra morena se ha lanzado usando este archivo, las otras pobalciones no, si no usando otro antiguo sobre el que despues tuvimos que eliminar las zonas intergénicas y volverlas a correr como explico más abajo. 



```{bash}

# Run pop by pop:

POP=c_lp_sm_n019 # <--- Change pop here!
screen -S "$POP"_scaffold_per_unit
POP=c_lp_sm_n019 # <--- Change pop here!
script log_screen_"$POP"_scaffold_per_unit.log
POP=c_lp_sm_n019 # <--- Change pop here!

# Run a loop: 

# screen -S Scaffold_per_unit
# script log_screen_Scaffold_per_unit
# POPS=("c_ll_ki_n013" "c_ll_no_n008" "c_ll_po_n008" "c_lp_do_n012" "c_lp_sm_n019")


## Variables
SCAFFOLDS_FOLDER=/GRUPOS/grupolince/Lyp_annotation_Apr14_final/LYPA23C.CDS.GENE_promoters.GENE_introns.UTRs.ncRNA.lncRNA.lncRNA_introns.lncRNA_promoters.UCR.intergenic.nr.gff3.PerScaffold/
cd $SCAFFOLDS_FOLDER
SCAFFOLDS=($(find /GRUPOS/grupolince/Lyp_annotation_Apr14_final/LYPA23C.CDS.GENE_promoters.GENE_introns.UTRs.ncRNA.lncRNA.lncRNA_introns.lncRNA_promoters.UCR.intergenic.nr.gff3.PerScaffold/ -name "LYPA23C*" -exec ls {} \; | awk '{ print substr( $0, length($0) - 10, length($0) ) }'  | LANG=en_EN sort | uniq ))
DIVERSITY_PER_UNIT_FOLDER=/home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/diversity_per_unit/
SFS_FOLDER=/home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/


# for POP in "${POPS[@]}"
# do

echo "---------------------------------------------------$POP---------------------------------------------------"

# Create headers for the outfile
echo -e "scaffold\tstart_cero_based\tend\tlength\tNAs\tinformative_sites\tfeature\tstrandness\tframe\tid_gene\tid\twatterson_ave\twatterson_sd\tpairwise_ave\tpairwise_sd\ttajimaD\tpop\tspecie\tepoch" > $DIVERSITY_PER_UNIT_FOLDER"$POP".per.unit.averages.tsv

for SCAFFOLD in "${SCAFFOLDS[@]}"
do
echo "---------------------$SCAFFOLD---------------------"

#For each unit

cd "$SFS_FOLDER""$POP"_separated_by_scaffold

while read LOCATION METHOD FEATURE START_ONEBASED END POINT STRANDNESS FRAME IDRAW;

do

echo "--------$SCAFFOLD:$FEATURE--------"

if [ "$METHOD" = "PipeR" ] 
then
ID_GENE=$(echo $IDRAW | awk -F "_" '{ split ($0, a, "ID="); split (a[2],b,"_"); print b[1]"_"b[2] }')
else
ID_GENE=$(echo $IDRAW | awk '{ split($0,a,"ID="); split (a[2],b,";"); split(b[1],c,"T"); print c[1] }') 
fi

if [ "$FEATURE" = "CDS" ]
then
ID=$(echo $IDRAW | awk -F "_" '{split ($0,a,"Target="); split(a[2],b,";"); print b[1]}' |  awk '{print $1"_"$2"_"$3}' )
else
ID=$(echo $IDRAW | awk '{ split($0,a,"ID="); split (a[2],b,";"); print b[1] }')
fi

START_CEROBASED=($(echo $START_ONEBASED-1 | bc))
LENGTH=($(echo $END-$START_CEROBASED | bc)) 
SPECIE=$(echo $POP | cut -d"_" -f 2)
EPOCH=$(echo $POP | cut -d"_" -f 1)


cat "$POP".transformedThetas_"$SCAFFOLD" | bedtools intersect -a stdin -b <(echo -e "$LOCATION\t$START_CEROBASED\t$END") > "$POP".iter.transformedThetas.borrar
# He comprobado que en el archivo de Thetas hay por lo menos 4 posiciones que empieza en 0. por tanto asumo que este archivo es 0-based.

WATTERSON_AVERAGE_PER_UNIT=$(cut -f 4 "$POP".iter.transformedThetas.borrar | awk -v OFS='\t' '{for(i=1;i<=NF;i++) {sum[i] += $i }} END {for (i=1;i<=NF;i++) {printf ("%.10e\n", sum[i]/NR )}}' | sed 's/[eE]+\{0,1\}/*10^/g')
WATTERSON_SD_PER_UNIT=$(cut -f 4 "$POP".iter.transformedThetas.borrar | awk -v OFS='\t' '{for(i=1;i<=NF;i++) {sum[i] += $i; sumsq[i] += ($i)^2}} END {for (i=1;i<=NF;i++) {printf ("%.10e\n", sqrt((sumsq[i]-sum[i]^2/NR)/NR))}}' | sed 's/[eE]+\{0,1\}/*10^/g')

PAIRWISE_AVERAGE_PER_UNIT=$(cut -f 5 "$POP".iter.transformedThetas.borrar | awk -v OFS='\t' '{for(i=1;i<=NF;i++) {sum[i] += $i }} END {for (i=1;i<=NF;i++) {printf ("%.10e\n", sum[i]/NR )}}' | sed 's/[eE]+\{0,1\}/*10^/g') 
PAIRWISE_SD_PER_UNIT=$(cut -f 5  "$POP".iter.transformedThetas.borrar | awk -v OFS='\t' '{for(i=1;i<=NF;i++) {sum[i] += $i; sumsq[i] += ($i)^2}} END {for (i=1;i<=NF;i++) {printf ("%.10e\n", sqrt((sumsq[i]-sum[i]^2/NR)/NR))}}' | sed 's/[eE]+\{0,1\}/*10^/g') 

DIFFERENCE_PAIRWISE_WATTERSON_SD_PER_UNIT=$(cut -f 6  "$POP".iter.transformedThetas.borrar   |  awk -v OFS='\t' '{for(i=1;i<=NF;i++) {sum[i] += $i; sumsq[i] += ($i)^2}} END {for (i=1;i<=NF;i++) {printf ("%.10e\n", sqrt((sumsq[i]-sum[i]^2/NR)/NR ))}}' | sed 's/[eE]+\{0,1\}/*10^/g') 

TAJIMAS_D_PER_UNIT=$(echo "(($PAIRWISE_AVERAGE_PER_UNIT) - ($WATTERSON_AVERAGE_PER_UNIT))/($DIFFERENCE_PAIRWISE_WATTERSON_SD_PER_UNIT)" | bc -l | awk '{printf ("%.10e\n",$1)}' |  sed 's/[eE]+\{0,1\}/*10^/g' )

INFORMATIVESITES=$(wc -l "$POP".iter.transformedThetas.borrar | cut -d" " -f1)

NAs=($(echo $LENGTH - $INFORMATIVESITES | bc)) 

#Before printing check all variables are full, if empty use NA
if [ -z ${LOCATION} ]; then  LOCATION=NA;  fi
if [ -z ${START_CEROBASED} ]; then  START_CEROBASED=NA;  fi
if [ -z ${END} ]; then  END=NA;  fi
if [ -z ${LENGTH} ]; then  LENGTH=NA;  fi
if [ -z ${NAs} ]; then  NAs=NA;  fi
if [ -z ${INFORMATIVESITES} ]; then  INFORMATIVESITES=NA;  fi
if [ -z ${FEATURE} ]; then  FEATURE=NA;  fi
if [ -z ${STRANDNESS} ]; then  STRANDNESS=NA;  fi
if [ -z ${FRAME} ]; then  FRAME=NA;  fi
if [ -z ${ID_GENE} ]; then  ID_GENE=NA;  fi
if [ -z ${ID} ]; then  ID=NA;  fi
if [ -z ${WATTERSON_AVERAGE_PER_UNIT} ]; then  WATTERSON_AVERAGE_PER_UNIT=NA;  fi
if [ -z ${WATTERSON_SD_PER_UNIT} ]; then  WATTERSON_SD_PER_UNIT=NA;  fi
if [ -z ${PAIRWISE_AVERAGE_PER_UNIT} ]; then  PAIRWISE_AVERAGE_PER_UNIT=NA;  fi
if [ -z ${PAIRWISE_SD_PER_UNIT} ]; then  PAIRWISE_SD_PER_UNIT=NA;  fi
if [ -z ${TAJIMAS_D_PER_UNIT} ]; then  TAJIMAS_D_PER_UNIT=NA;  fi
if [ -z ${POP} ]; then  POP=NA;  fi
if [ -z ${SPECIE} ]; then  SPECIE=NA;  fi
if [ -z ${EPOCH} ]; then  EPOCH=NA;  fi


#Paste averages, and standatd deviations

paste \
<(echo $LOCATION ) \
<(echo $START_CEROBASED ) \
<(echo $END ) \
<(echo $LENGTH ) \
<(echo $NAs) \
<(echo $INFORMATIVESITES) \
<(echo $FEATURE ) \
<(echo $STRANDNESS ) \
<(echo $FRAME ) \
<(echo $ID_GENE ) \
<(echo $ID) \
<(echo $WATTERSON_AVERAGE_PER_UNIT) \
<(echo $WATTERSON_SD_PER_UNIT) \
<(echo $PAIRWISE_AVERAGE_PER_UNIT) \
<(echo $PAIRWISE_SD_PER_UNIT) \
<(echo $TAJIMAS_D_PER_UNIT) \
<(echo $POP) \
<(echo $SPECIE) \
<(echo $EPOCH) |\
sed 's/ /\t/g'| sed 's/\t\+/\t/g'  >>  $DIVERSITY_PER_UNIT_FOLDER"$POP".per.unit.averages.tsv


#Reset all (but POP,SPECIE or EPOCH (those should be the same during the whole iteration and if you remove POP the loop won't work)) variables before next iteration

unset LOCATION
unset START_CEROBASED
unset END
unset LENGTH
unset NAs
unset INFORMATIVESITES
unset FEATURE
unset STRANDNESS
unset FRAME
unset ID_GENE
unset ID
unset WATTERSON_AVERAGE_PER_UNIT
unset WATTERSON_SD_PER_UNIT
unset PAIRWISE_AVERAGE_PER_UNIT
unset PAIRWISE_SD_PER_UNIT
unset TAJIMAS_D_PER_UNIT

done < <(cat "$SCAFFOLDS_FOLDER"LYPA23C.CDS.GENE_promoters.GENE_introns.UTRs.ncRNA.lncRNA.lncRNA_introns.lncRNA_promoters.UCR.intergenic.nr.gff3_"$SCAFFOLD" ) 

done 

rm "$POP".iter.transformedThetas.borrar

```

Copia de seguridad

Hace falta copiar también lo demás que lo había lanzado de nuevo, el printed.stats y el transformedThetas:

```{bash}
# Dentro del servidor B:
scp /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/diversity_per_unit/*per.unit.averages.tsv /backup/mlucena/intermediate_files_ANGSD/whole_genome_analysis/
```

-----------

10/05/2018
y repetido con nuevo archivo el día 17/05/2018

Este script lo corrí en su momento con el archivo: /GRUPOS/grupolince/Lyp_annotation_Apr14_final/LYPA23C.CDS.GENE_promoters.GENE_introns.UTRs.ncRNA.lncRNA.lncRNA_introns.lncRNA_promoters.UCR.intergenic.nr.gff3.PerScaffold/ que no es el definitivo y que ha sido sustituido por: LYPA23C.CDS.GENE_promoters.GENE_introns.UTRs.ncRNA.lncRNA.lncRNA_introns.lncRNA_promoters.UCNE.intergenic.nr.gff3.PerScaffold. Con este corro mi script de nuevo el día 17. 


OJO! Me he dado cuenta que al menos hay un UCR dentro de una región que se considera intergénico!
Modificación de mis archivos y vuelta a correr lo que está mal. 

Lo que ocurre es que UCR están dentro de intergénico, por tanto lo que hemos considerado intergénico estaría mal. 
La forma de proceder va a ser la siguiente:

1. Borrar intergenic y los UCR del archivo de diversidad.
2. Lanzar el cálculo de diversidad sobre el intergénico y los UCNE unicamente.
3. Pegar los nuevos cálculos a nuestro archivo original de diversidad.
4. Ordenar por scaffold y posición.


---> 1. Borrar intergenic del archivo de diversidad.

```{bash}

cd /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/diversity_per_unit

for DIVERSITY_FILE in c_lp_do*.per.unit.averages.tsv
do
echo $DIVERSITY_FILE
wc -l $DIVERSITY_FILE
grep -v intergenic $DIVERSITY_FILE | grep -v UCR  > ${DIVERSITY_FILE/per.unit.averages.tsv/per.unit.averages.no_intergenic_no_UCR.tsv}
wc -l ${DIVERSITY_FILE/per.unit.averages.tsv/per.unit.averages.no_intergenic_no_UCR.tsv}
done


# Sanity
wc -l 

c_ll_ki_n013.per.unit.averages.tsv
# 499065 c_ll_ki_n013.per.unit.averages.tsv
# 425829 c_ll_ki_n013.per.unit.averages.no_intergenic_no_UCR.tsv

c_ll_no_n008.per.unit.averages.tsv
# 499065 c_ll_no_n008.per.unit.averages.tsv
# 425829 c_ll_no_n008.per.unit.averages.no_intergenic_no_UCR.tsv

c_ll_po_n008.per.unit.averages.tsv
# 499065 c_ll_po_n008.per.unit.averages.tsv
# 425829 c_ll_po_n008.per.unit.averages.no_intergenic_no_UCR.tsv

c_lp_do_n012.per.unit.averages.tsv
# 499065 c_lp_do_n012.per.unit.averages.tsv
# 425829 c_lp_do_n012.per.unit.averages.no_intergenic_no_UCR.tsv

```


---> 2. Lanzar el cálculo de diversidad sobre el intergénico únicamente.

```{bash}

# Run pop by pop:

POP=c_lp_do_n012 # <--- Change pop here!
screen -S "$POP"_scaffold_per_unit
POP=c_lp_do_n012 # <--- Change pop here!
script log_screen_"$POP"_scaffold_per_unit.log
POP=c_lp_do_n012 # <--- Change pop here!


## Variables
SCAFFOLDS_FOLDER=/GRUPOS/grupolince/Lyp_annotation_Apr14_final/LYPA23C.CDS.GENE_promoters.GENE_introns.UTRs.ncRNA.lncRNA.lncRNA_introns.lncRNA_promoters.UCNE.intergenic.nr.gff3.PerScaffold/
cd $SCAFFOLDS_FOLDER
SCAFFOLDS=($(find /GRUPOS/grupolince/Lyp_annotation_Apr14_final/LYPA23C.CDS.GENE_promoters.GENE_introns.UTRs.ncRNA.lncRNA.lncRNA_introns.lncRNA_promoters.UCNE.intergenic.nr.gff3.PerScaffold/ -name "LYPA23C*" -exec ls {} \; | awk '{ print substr( $0, length($0) - 10, length($0) ) }'  | LANG=en_EN sort | uniq ))
DIVERSITY_PER_UNIT_FOLDER=/home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/diversity_per_unit/
SFS_FOLDER=/home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/



# for POP in "${POPS[@]}"
# do

echo "---------------------------------------------------$POP---------------------------------------------------"

# Create headers for the outfile
# echo -e "scaffold\tstart_cero_based\tend\tlength\tNAs\tinformative_sites\tfeature\tstrandness\tframe\tid_gene\tid\twatterson_ave\twatterson_sd\tpairwise_ave\tpairwise_sd\ttajimaD\tpop\tspecie\tepoch" > $DIVERSITY_PER_UNIT_FOLDER"$POP".per.unit.averages.tsv

for SCAFFOLD in "${SCAFFOLDS[@]}"
do
echo "---------------------$SCAFFOLD---------------------"

#For each unit

cd "$SFS_FOLDER""$POP"_separated_by_scaffold

while read LOCATION METHOD FEATURE START_ONEBASED END POINT STRANDNESS FRAME IDRAW;

do

echo "--------$SCAFFOLD:$FEATURE--------"

if [ "$METHOD" = "PipeR" ] 
then
ID_GENE=$(echo $IDRAW | awk -F "_" '{ split ($0, a, "ID="); split (a[2],b,"_"); print b[1]"_"b[2] }')
else
ID_GENE=$(echo $IDRAW | awk '{ split($0,a,"ID="); split (a[2],b,";"); split(b[1],c,"T"); print c[1] }') 
fi

if [ "$FEATURE" = "CDS" ]
then
ID=$(echo $IDRAW | awk -F "_" '{split ($0,a,"Target="); split(a[2],b,";"); print b[1]}' |  awk '{print $1"_"$2"_"$3}' )
else
ID=$(echo $IDRAW | awk '{ split($0,a,"ID="); split (a[2],b,";"); print b[1] }')
fi

START_CEROBASED=($(echo $START_ONEBASED-1 | bc))
LENGTH=($(echo $END-$START_CEROBASED | bc)) 
SPECIE=$(echo $POP | cut -d"_" -f 2)
EPOCH=$(echo $POP | cut -d"_" -f 1)


cat "$POP".transformedThetas_"$SCAFFOLD" | bedtools intersect -a stdin -b <(echo -e "$LOCATION\t$START_CEROBASED\t$END") > "$POP".iter.transformedThetas.borrar
# He comprobado que en el archivo de Thetas hay por lo menos 4 posiciones que empieza en 0. por tanto asumo que este archivo es 0-based.


WATTERSON_AVERAGE_PER_UNIT=$(cut -f 4 "$POP".iter.transformedThetas.borrar | awk -v OFS='\t' '{for(i=1;i<=NF;i++) {sum[i] += $i }} END {for (i=1;i<=NF;i++) {printf ("%.10e\n", sum[i]/NR )}}' | sed 's/[eE]+\{0,1\}/*10^/g')
WATTERSON_SD_PER_UNIT=$(cut -f 4 "$POP".iter.transformedThetas.borrar | awk -v OFS='\t' '{for(i=1;i<=NF;i++) {sum[i] += $i; sumsq[i] += ($i)^2}} END {for (i=1;i<=NF;i++) {printf ("%.10e\n", sqrt((sumsq[i]-sum[i]^2/NR)/NR))}}' | sed 's/[eE]+\{0,1\}/*10^/g')

PAIRWISE_AVERAGE_PER_UNIT=$(cut -f 5 "$POP".iter.transformedThetas.borrar | awk -v OFS='\t' '{for(i=1;i<=NF;i++) {sum[i] += $i }} END {for (i=1;i<=NF;i++) {printf ("%.10e\n", sum[i]/NR )}}' | sed 's/[eE]+\{0,1\}/*10^/g') 
PAIRWISE_SD_PER_UNIT=$(cut -f 5  "$POP".iter.transformedThetas.borrar | awk -v OFS='\t' '{for(i=1;i<=NF;i++) {sum[i] += $i; sumsq[i] += ($i)^2}} END {for (i=1;i<=NF;i++) {printf ("%.10e\n", sqrt((sumsq[i]-sum[i]^2/NR)/NR))}}' | sed 's/[eE]+\{0,1\}/*10^/g') 

DIFFERENCE_PAIRWISE_WATTERSON_SD_PER_UNIT=$(cut -f 6  "$POP".iter.transformedThetas.borrar   |  awk -v OFS='\t' '{for(i=1;i<=NF;i++) {sum[i] += $i; sumsq[i] += ($i)^2}} END {for (i=1;i<=NF;i++) {printf ("%.10e\n", sqrt((sumsq[i]-sum[i]^2/NR)/NR ))}}' | sed 's/[eE]+\{0,1\}/*10^/g') 

TAJIMAS_D_PER_UNIT=$(echo "(($PAIRWISE_AVERAGE_PER_UNIT) - ($WATTERSON_AVERAGE_PER_UNIT))/($DIFFERENCE_PAIRWISE_WATTERSON_SD_PER_UNIT)" | bc -l | awk '{printf ("%.10e\n",$1)}' |  sed 's/[eE]+\{0,1\}/*10^/g' )

INFORMATIVESITES=$(wc -l "$POP".iter.transformedThetas.borrar | cut -d" " -f1)

NAs=($(echo $LENGTH - $INFORMATIVESITES | bc)) 

#Before printing check all variables are full, if empty use NA
if [ -z ${LOCATION} ]; then  LOCATION=NA;  fi
if [ -z ${START_CEROBASED} ]; then  START_CEROBASED=NA;  fi
if [ -z ${END} ]; then  END=NA;  fi
if [ -z ${LENGTH} ]; then  LENGTH=NA;  fi
if [ -z ${NAs} ]; then  NAs=NA;  fi
if [ -z ${INFORMATIVESITES} ]; then  INFORMATIVESITES=NA;  fi
if [ -z ${FEATURE} ]; then  FEATURE=NA;  fi
if [ -z ${STRANDNESS} ]; then  STRANDNESS=NA;  fi
if [ -z ${FRAME} ]; then  FRAME=NA;  fi
if [ -z ${ID_GENE} ]; then  ID_GENE=NA;  fi
if [ -z ${ID} ]; then  ID=NA;  fi
if [ -z ${WATTERSON_AVERAGE_PER_UNIT} ]; then  WATTERSON_AVERAGE_PER_UNIT=NA;  fi
if [ -z ${WATTERSON_SD_PER_UNIT} ]; then  WATTERSON_SD_PER_UNIT=NA;  fi
if [ -z ${PAIRWISE_AVERAGE_PER_UNIT} ]; then  PAIRWISE_AVERAGE_PER_UNIT=NA;  fi
if [ -z ${PAIRWISE_SD_PER_UNIT} ]; then  PAIRWISE_SD_PER_UNIT=NA;  fi
if [ -z ${TAJIMAS_D_PER_UNIT} ]; then  TAJIMAS_D_PER_UNIT=NA;  fi
if [ -z ${POP} ]; then  POP=NA;  fi
if [ -z ${SPECIE} ]; then  SPECIE=NA;  fi
if [ -z ${EPOCH} ]; then  EPOCH=NA;  fi


#Paste averages, and standatd deviations

paste \
<(echo $LOCATION ) \
<(echo $START_CEROBASED ) \
<(echo $END ) \
<(echo $LENGTH ) \
<(echo $NAs) \
<(echo $INFORMATIVESITES) \
<(echo $FEATURE ) \
<(echo $STRANDNESS ) \
<(echo $FRAME ) \
<(echo $ID_GENE ) \
<(echo $ID) \
<(echo $WATTERSON_AVERAGE_PER_UNIT) \
<(echo $WATTERSON_SD_PER_UNIT) \
<(echo $PAIRWISE_AVERAGE_PER_UNIT) \
<(echo $PAIRWISE_SD_PER_UNIT) \
<(echo $TAJIMAS_D_PER_UNIT) \
<(echo $POP) \
<(echo $SPECIE) \
<(echo $EPOCH) |\
sed 's/ /\t/g'| sed 's/\t\+/\t/g'  >>  $DIVERSITY_PER_UNIT_FOLDER"$POP".per.unit.averages.no_intergenic_no_UCR.tsv

#Reset all (but POP,SPECIE or EPOCH (those should be the same during the whole iteration and if you remove POP the loop won't work)) variables before next iteration

unset LOCATION
unset START_CEROBASED
unset END
unset LENGTH
unset NAs
unset INFORMATIVESITES
unset FEATURE
unset STRANDNESS
unset FRAME
unset ID_GENE
unset ID
unset WATTERSON_AVERAGE_PER_UNIT
unset WATTERSON_SD_PER_UNIT
unset PAIRWISE_AVERAGE_PER_UNIT
unset PAIRWISE_SD_PER_UNIT
unset TAJIMAS_D_PER_UNIT

done < <(cat "$SCAFFOLDS_FOLDER"LYPA23C.CDS.GENE_promoters.GENE_introns.UTRs.ncRNA.lncRNA.lncRNA_introns.lncRNA_promoters.UCNE.intergenic.nr.gff3_"$SCAFFOLD" | grep 'intergenic\|UCNE' ) 

done &

rm "$POP".iter.transformedThetas.borrar

```

---> 4. Muevo los archivos recien creados al nombre inicial.


```{bash}

for diversity_file in *.no_intergenic.tsv
do
echo $diversity_file
mv $diversity_file ${diversity_file/.per.unit.averages.no_intergenic.tsv/.per.unit.averages.tsv}
done

```

---> 5. Ordeno


```{bash}

for file_diversity in *per.unit.averages.tsv
do
echo $file_diversity
echo ${file_diversity/.per.unit.averages.tsv/.per.unit.averages.sorted.tsv}
LANG=en_EN sort -k 1,1 -k 2,2n -k 3,3n $file_diversity  > ${file_diversity/.per.unit.averages.tsv/.per.unit.averages.sorted.tsv}
done


mv c_lp_do_n012.per.unit.averages.sorted.tsv c_lp_do_n012.per.unit.averages.tsv
mv c_ll_po_n008.per.unit.averages.sorted.tsv c_ll_po_n008.per.unit.averages.tsv
mv c_ll_ki_n013.per.unit.averages.sorted.tsv c_ll_ki_n013.per.unit.averages.tsv
mv c_ll_no_n008.per.unit.averages.sorted.tsv c_ll_no_n008.per.unit.averages.tsv
mv c_lp_sm_n019.per.unit.averages.sorted.tsv c_lp_sm_n019.per.unit.averages.tsv


# Hemos cometido un fallo, y es que hemos ordenado el archivo y tenía header, por lo que ahora lo tenemos que buscar, eliminar y ponerlo en donde corresponde. 

grep -v start_cero_based c_ll_ki_n013.per.unit.averages.tsv > c_ll_ki_n013.per.unit.averages.NO_HEADER.tsv
grep -v start_cero_based c_ll_no_n008.per.unit.averages.tsv > c_ll_no_n008.per.unit.averages.NO_HEADER.tsv
grep -v start_cero_based c_ll_po_n008.per.unit.averages.tsv > c_ll_po_n008.per.unit.averages.NO_HEADER.tsv
grep -v start_cero_based c_lp_do_n012.per.unit.averages.tsv > c_lp_do_n012.per.unit.averages.NO_HEADER.tsv


# HEADER:

echo -e "scaffold\tstart_cero_based\tend\tlength\tNAs\tinformative_sites\tfeature\tstrandness\tframe\tid_gene\tid\twatterson_ave\twatterson_sd\tpairwise_ave\tpairwise_sd\ttajimaD\tpop\tspecie\tepoch" > header_global

for file in *.NO_HEADER.tsv
do
echo $file
cat header_global $file > ${file/NO_HEADER./}
done

rm *HEADER*

```


---> 6. Copia de seguridad

Hace falta copiar también lo demás que lo había lanzado de nuevo, el printed.stats y el transformedThetas:

```{bash}
# Dentro del servidor B:
scp /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/diversity_per_unit/*per.unit.averages.tsv /backup/mlucena/intermediate_files_ANGSD/whole_genome_analysis/
```



-----------

Anotación de las unidades: cromosoma y regiones; tel y centr. 

Ahora vamos a anotar esas unidades según pertenezcan o no a distintos cromosomas, y según sean teloméricas, centromérica o no. 


Cromosomas 

Para ello hemos usado el archivo que tienen en común las coordenadas de gato y de lince

```{bash}
head /GRUPOS/grupolince/copia_fabascal/MAPPINGS/lynx2cat_wTiger.sorted.bed
```
  
Creo una carpeta donde voy a guardar los archivos:
```{bash}
mkdir /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/chromosome_annotation
```

Los archivos de los que parto estén en:
```{bash}
/home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/diversity_per_unit/
```

Empiezo a unirlos. 

El primer Bed te devolvería un archivo con una línea por cada posición de sintenia tal que así:

lp23.s00001	0	15011	15011	11950	3061	intergenic	-	.	intergenic_region_1	intergenic_region_1	2.4096547746*10^-06	1.3458559263*10^-05	1.1416658651*10^-06	6.2801913098*10^-06	-1.7606196687*10^-01	c_ll_no_n008	ll	c	lp23.s00001	4071	4072	TCL=TTT:chrA3:15104518-15104519:NO	1

Como no queremos pasar por ahí ni guardar tanto, vamos a intentar usar awk directamente para quedarnos con lo que nos interese. 

```{bash}

screen -S chromosome_annotation

cd /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/diversity_per_unit/

for FILE in *.per.unit.averages.tsv

do
echo $FILE
cd /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/chromosome_annotation
intersectBed -sorted -wo -a <(tail -n +2 /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/diversity_per_unit/$FILE) -b /GRUPOS/grupolince/copia_fabascal/MAPPINGS/lynx2cat_wTiger.sorted.bed | awk -v OFS='\t' '{split ($23, a, ":"); split(a[3],b, "-"); print $1,$2,$3,$4,$5,$6,$7,$8,$9,$10,$11,$12,$13,$14,$15,$16,$17,$18,$19,a[2] }' | awk '!a[$0]++' > ${FILE/.per.unit.averages.tsv/.per.unit.averages.chr.tsv}
done

echo -e "scaffold\tstart_cero_based\tend\tlength\tNAs\tinformative_sites\tfeature\tstrandness\tframe\tid_gene\tid\twatterson_ave\twatterson_sd\tpairwise_ave\tpairwise_sd\ttajimaD\tpop\tspecie\tepoch\tchr" > header.chr.annotation

for FILE in *per.unit.averages.chr.tsv
do
echo $FILE
cat header.chr.annotation $FILE > ${FILE/per.unit.averages.chr.tsv/per.unit.averages.chr.HEADER.tsv} 
mv ${FILE/per.unit.averages.chr.tsv/per.unit.averages.chr.HEADER.tsv} $FILE
done

# De esta manera estamos eliminando filas duplicadas. Si queremos comprobar cuantos cromosomas habría en cada unidad tenemos que postprocesar en R. El merge de abajo va a juntarnos todo auqnue sean distintas unidades si les separa una base, así que no nos interesa.
# bedtools merge -i stdin -c 11,20 -o distinct,distinct -delim ";" > ${FILE/per.unit.averages.tsv/per.unit.averages.chr.tsv}

```

Esto me devuelve algo como así:
lp23.s00001	0	15011	15011	11950	3061	intergenic	-	.	intergenic_region_1	intergenic_region_1	2.4096547746*10^-06	1.3458559263*10^-05	1.1416658651*10^-06	6.2801913098*10^-06	-1.7606196687*10^-01	c_ll_no_n008	ll	c	chrA3


Sanity checks

```{bash}

cd /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/chromosome_annotation

# Calculo qué unidades aparecen más de una vez:

awk '{print $7"_"$11}' c_lp_do_n012.per.unit.averages.chr.tsv | sort | uniq -c | grep -v "1 " | wc -l
# Hay 2534 

# ¿Qué hay en esta lista?

 awk '{print $7"_"$11}' c_lp_do_n012.per.unit.averages.chr.tsv | sort | uniq -c | grep -v "1 " | sed -e 's/^[ \t]*//' | cut -d' ' -f2  | cut -d"_" -f 1 | sort  | uniq -c


      9 3UTR
     14 5UTR
     12 CDS
   1033 intergenic
    414 intron
      2 lncRNA
      1 ncRNA
      9 promoter
   1040 UCR

# Esta lista incluye varias features distintas.

# Parece haber al menos dos casos claros: 

# 1. Por un lado, aquellos UCR que están separados en varios trozos porque no logré unirlo por la sintenia. Ejemplo: ZMF521_Dorian está tres veces

# grep ZNF521_Dorian ../c_lp_do_n012.per.unit.averages.chr.tsv
# lp23.s31342	333553	333644	91	0	91	UCR	-	.	ZNF521_Dorian	ZNF521_Dorian	4.8025515249*10^-07	3.4994486408*10^-06	1.7249711055*10^-08	8.4251477269*10^-09	-1.3232153436*10^-01	c_lp_do_n012	lp	c	chrD3
# lp23.s31342	333645	333762	117	0	117	UCR	-	.	ZNF521_Dorian	ZNF521_Dorian	2.0280707373*10^-07	1.7930201697*10^-07	6.1292390857*10^-08	2.8242579987*10^-08	-7.3264237026*10^-01	c_lp_do_n012	lp	c	chrD3
# lp23.s31342	333769	333873	104	0	104	UCR	-	.	ZNF521_Dorian	ZNF521_Dorian	1.4021552729*10^-07	6.4529679510*10^-08	1.0221886658*10^-07	7.6602016576*10^-07	-4.8985261517*10^-02	c_lp_do_n012	lp	c	chrD3

# ¿Cuantas ultraconserved regions hay en total?

grep UCR /GRUPOS/grupolince/Lyp_annotation_Apr14_final/LYPA23C.CDS.GENE_promoters.GENE_introns.UTRs.ncRNA.lncRNA.lncRNA_introns.lncRNA_promoters.UCR.intergenic.nr.gff3 | wc -l
5539 

# Si tenemos que 1040 aparecen repetidas, vemos que son un quinto aquellas que no tienen contigüidad. 
# Hay que tener cuidado con esto al hacer calculos por unidad! Pero esto no es tan preocupante porque sabíamos que ocurria.

# 2. Por otro lado, parece que dentro de estas features repetidas tengo otras casuisticas:

# Ejemplo: Un intron repetido tres veces:

# grep LYPA23C000420T1 c_lp_do_n012.per.unit.averages.chr.tsv | grep intron_62

# lp23.s05342	577057	603192	26135	15052	11083	intron	+	0	LYPA23C000420	LYPA23C000420T1_intron_62	1.9406089032*10^-04	7.1920669656*10^-03	2.7265147013*10^-04	1.0183730201*10^-02	2.5254946264*10^-02	c_lp_do_n012	lp	cchrB2
# lp23.s05342	577057	603192	26135	15052	11083	intron	+	0	LYPA23C000420	LYPA23C000420T1_intron_62	1.9406089032*10^-04	7.1920669656*10^-03	2.7265147013*10^-04	1.0183730201*10^-02	2.5254946264*10^-02	c_lp_do_n012	lp	cchrX
# lp23.s05342	577057	603192	26135	15052	11083	intron	+	0	LYPA23C000420	LYPA23C000420T1_intron_62	1.9406089032*10^-04	7.1920669656*10^-03	2.7265147013*10^-04	1.0183730201*10^-02	2.5254946264*10^-02	c_lp_do_n012	lp	cchrB1

# Este intrón aparece asignado a tres cromosomas distintos. 

# Compruebo que efectivamente se asigna a tres cromosomas en el archivo de notación

grep lp23.s05342 /GRUPOS/grupolince/copia_fabascal/MAPPINGS/lynx2cat_wTiger.sorted.bed  | grep -v chrB2

# Efectivamente hay más de un cromosoma en ese scaffold. 

# Vamos a ver que número de features presentan este problema. 

# Creo una lista PARA DOÑANA de aquellas features que son identicas también en las posiciones (lo que excluiría a los casos de las UCR que hemos hablado antes). Para ello imprimo también la posición de comienzo y la de fin.

awk -v OFS='\t' '{print $1,$2,$3,$7,$11}' c_lp_do_n012.per.unit.averages.chr.tsv | sort | uniq -c | grep -v "1 " | sort | sed -e 's/^[ \t]*//'  > list_of_repeated_features_do

# Compruebo a qué unidades pertenecen. 

 cut -d' ' -f2 list_of_repeated_features_do | awk '{print $4}' | sort  | uniq -c

      9 3UTR
     14 5UTR
     12 CDS
   1033 intergenic
    384 intron
     30 intron_lncRNA
      2 lncRNA
      1 ncRNA
      1 promoter_gene_1000
      3 promoter_lncRNA_1000
      2 promoter_lncRNA_250
      3 promoter_lncRNA_500
      
      
# Vemos que todos los numeros coinciden con las unidades repetidas que habían salido antes (ojo: promotores=9 (1+3+2+3); intrones=414(384+30)). Por tanto, parece que todas estas regiones repetidas están asignadas a mas de un cromosoma. Ahora vamos a ver cuantas bases son eso.

# Compruebo que en las otras poblaciones son las mismas.

cd /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/chromosome_annotation

awk -v OFS='\t' '{print $1,$2,$3,$7,$11}' c_ll_ki_n013.per.unit.averages.chr.tsv | sort | uniq -c | grep -v "1 " | sort | sed -e 's/^[ \t]*//' > list_of_repeated_features_ki

awk -v OFS='\t' '{print $1,$2,$3,$7,$11}' c_ll_no_n008.per.unit.averages.chr.tsv | sort | uniq -c | grep -v "1 " | sort | sed -e 's/^[ \t]*//' > list_of_repeated_features_no

awk -v OFS='\t' '{print $1,$2,$3,$7,$11}' c_ll_po_n008.per.unit.averages.chr.tsv | sort | uniq -c | grep -v "1 " | sort | sed -e 's/^[ \t]*//' > list_of_repeated_features_po

diff list_of_repeated_features_do list_of_repeated_features_po
diff list_of_repeated_features_do list_of_repeated_features_ki
diff list_of_repeated_features_do list_of_repeated_features_no

# Por suerte son iguales! 

# Así que para futuras comprobaciones puedo coger cualquiera. Borro 3 de ellas. 

rm list_of_repeated_features_no
rm list_of_repeated_features_ki
rm list_of_repeated_features_po

# ¿Cuantas bases perdemos?

mkdir features_repeted
cd features_repeted/

# Primero calculamos cuantas bases totales tenemos:

# Este archivo que genero tiene todas las unidades, no solo las repetidas.
awk -v OFS='\t' '{print $1,$2,$3,$7,$11}' ../c_lp_do_n012.per.unit.averages.chr.tsv |  awk '{print >> $4; close($4)}'

rm feature 

# ¿Cuantas bases por cada unidad?

for file in * 
do
echo $file "$(awk '{sum+=$3-$2}END{print sum}' $file )"
done >> bases_total.txt

mv bases_total.txt ../

# Ahora elimino estos archivos y hago la misma cuenta para aquellas que están repetidas.

# ¿Cuantas repetidas?

awk -v OFS='\t' '{print $1,$2,$3,$7,$11}' ../c_lp_do_n012.per.unit.averages.chr.tsv | sort | uniq -c | grep -v "1 " | sort | sed -e 's/^[ \t]*//' 

awk -v OFS='\t' '{print $1,$2,$3,$7,$11}' ../c_lp_do_n012.per.unit.averages.chr.tsv | sort | uniq -c | grep -v "1 " | sort | sed -e 's/^[ \t]*//' | cut -d' ' -f2-6 | awk '{print >> $4; close($4)}'

rm feature 

# ¿Cuantas bases por cada unidad?

for file in * 
do
echo $file "$(awk '{sum+=$3-$2}END{print sum}' $file )"
done >> bases_repeted.txt

mv bases_repeted.txt ../

rm *

# ¿Qué porcentaje supone del total?

join -1 1 -2 1 bases_total.txt bases_repeted.txt | awk -v OFS='\t' '{print $1,$2,$3,$3/$2*100}' | cat <(echo -e "feature\ttotal_bases\tbases_assinged_to_2_or_more_chr\tpercentage") - > percentage_of_bases_assigned_to_2_chr_or_more.txt


rm bases_repeted.txt
rm bases_total.txt 
rm -r features_repeted/
rm list_of_repeated_features_do 


```


Telomeros2m

El archivo de interés, es: 
```{bash}
/GRUPOS/grupolince/telomers_centromers_definition/tel2m_regions_based_on_synteny_1000bp.bed
```

Creo una carpeta donde voy a guardar los archivos:
```{bash}
mkdir /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/region_annotation
```

Los archivos de los que parto estén en:
```{bash}
/home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/diversity_per_unit/
```

Empiezo a unirlos. 

```{bash}
cd /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/diversity_per_unit/

files_per_pop=($(ls *.per.unit.averages.tsv))

cd /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/region_annotation

echo -e "scaffold\tstart_cero_based\tend\tlength\tNAs\tinformative_sites\tfeature\tstrandness\tframe\tid_gene\tid\twatterson_ave\twatterson_sd\tpairwise_ave\tpairwise_sd\ttajimaD\tpop\tspecie\tepoch\tchr\ttel2m_bases" > header.region.tel2m.tsv

for file_pop in ${files_per_pop[@]}
do
echo $file_pop
intersectBed -sorted -wo -a <(tail -n +2 /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/diversity_per_unit/$file_pop) -b /GRUPOS/grupolince/telomers_centromers_definition/tel2m_regions_based_on_synteny_1000bp.bed | awk -v OFS='\t' '{print $1,$2,$3,$4,$5,$6,$7,$8,$9,$10,$11,$12,$13,$14,$15,$16,$17,$18,$19,$23,$24 }' | awk '!a[$0]++' | cat header.region.tel2m.tsv - > ${file_pop/.per.unit.averages.tsv/.per.unit.averages.region.tel2m.tsv}

done
```

Esto me devuelve algo como así:
lp23.s00006	14749	29606	14857	8272	6585	intergenic	-	.	intergenic_region_445	intergenic_region_445	6.8371515163*10^-04	1.2551611729*10^-02	1.1155938889*10^-03	2.1427002344*10^-02	4.4628203002*10^-02	c_lp_sm_n019	lp	c	ChrB2 485

donde el último número representa las bases que solapan. 

Telomeros10m

El archivo de interés, es: 
```{bash}
/GRUPOS/grupolince/telomers_centromers_definition/tel0-10m_regions_based_on_synteny_1000bp.bed
```

Voy a guardar los archivos en la carpeta anteriormente creada:
```{bash}
/home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/region_annotation
```

Los archivos de los que parto estén en:
```{bash}
/home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/diversity_per_unit/
```

Empiezo a unirlos. 

```{bash}
cd /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/diversity_per_unit/

files_per_pop=($(ls *.per.unit.averages.tsv))

cd /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/region_annotation

echo -e "scaffold\tstart_cero_based\tend\tlength\tNAs\tinformative_sites\tfeature\tstrandness\tframe\tid_gene\tid\twatterson_ave\twatterson_sd\tpairwise_ave\tpairwise_sd\ttajimaD\tpop\tspecie\tepoch\tchr\ttel10m_bases" > header.region.tel10m.tsv


for file_pop in ${files_per_pop[@]}
do
echo $file_pop
intersectBed -sorted -wo -a <(tail -n +2 /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/diversity_per_unit/$file_pop) -b /GRUPOS/grupolince/telomers_centromers_definition/tel0-10m_regions_based_on_synteny_1000bp.bed | awk -v OFS='\t' '{print $1,$2,$3,$4,$5,$6,$7,$8,$9,$10,$11,$12,$13,$14,$15,$16,$17,$18,$19,$23,$24 }' | awk '!a[$0]++' | cat header.region.tel10m.tsv - > ${file_pop/.per.unit.averages.tsv/.per.unit.averages.region.tel10m.tsv}
done

```


Y por último los centrómeros:

Centrómeros:

El archivo de interés, es: 
```{bash}
/GRUPOS/grupolince/telomers_centromers_definition/centr_regions_based_on_synteny_1000bp.bed
```

Voy a guardar los archivos en la carpeta anteriormente creada:
```{bash}
/home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/region_annotation
```

Los archivos de los que parto estén en:
```{bash}
/home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/diversity_per_unit/
```

Empiezo a unirlos. 

```{bash}
cd /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/diversity_per_unit/

files_per_pop=($(ls *.per.unit.averages.tsv))

cd /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/region_annotation

echo -e "scaffold\tstart_cero_based\tend\tlength\tNAs\tinformative_sites\tfeature\tstrandness\tframe\tid_gene\tid\twatterson_ave\twatterson_sd\tpairwise_ave\tpairwise_sd\ttajimaD\tpop\tspecie\tepoch\tchr\tcentr_bases" > header.region.centr.tsv


for file_pop in ${files_per_pop[@]}
do
echo $file_pop
intersectBed -sorted -wo -a <(tail -n +2 /home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/diversity_per_unit/$file_pop) -b /GRUPOS/grupolince/telomers_centromers_definition/centr_regions_based_on_synteny_1000bp.bed | awk -v OFS='\t' '{print $1,$2,$3,$4,$5,$6,$7,$8,$9,$10,$11,$12,$13,$14,$15,$16,$17,$18,$19,$23,$24 }' | awk '!a[$0]++' | cat header.region.centr.tsv - > ${file_pop/.per.unit.averages.tsv/.per.unit.averages.region.centr.tsv}
done

```


Ahora borro lo que no necesito

```{bash}
rm header.region.*
```


------------------------------------------


Postprocesado en R

Ahora necesito hacer varias cositas con estos archivos que acabo de crear. En primer lugar tengo que colapsar aquellos que tienen tel o centr, ya que si este centr estaba definido en base a varias regiones pertenecientes a la misma unidad aparece varias veces. Ejemplo:

scaffold1 1 3000 genA

telómeros:

scaffold1 2 200
scaffold1 500 1000
scaffold1 1500  3000

Esto apareceria así:

scaffold1 1 3000 genA scaffold1 2 200 198
scaffold1 1 3000 genA scaffold1 500 1000  500
scaffold1 1 3000 genA scaffold1 1500  3000  1500

Siendo la tercera columna el número de posiciones que solapan.

Por tanto yo lo que querría obtener sería:
scaffold1 1 3000 genA 2198

Con una columna que recoja el número de posiciones que solapan, para que despues yo pueda hacer un porcentaje de cuantas bases están cubiertas. 


Primero me bajo las tablas.


```{bash}

scp mlucena@genomics-b.ebd.csic.es:/home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/diversity_per_unit/*tsv /Users/marialucenaperez/Documents/WG_lynx_diversity_per_unit
scp mlucena@genomics-b.ebd.csic.es:/home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/region_annotation/*tsv /Users/marialucenaperez/Documents/WG_lynx_diversity_per_unit
scp mlucena@genomics-b.ebd.csic.es:/home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/chromosome_annotation/*tsv /Users/marialucenaperez/Documents/WG_lynx_diversity_per_unit

```


Procesamiento de la tabla con region annotation. Esto sólo se hace una vez!
Comprobamos que el archivo tiene información y añadimos un porcentaje con las bases que tiene que son teloméricas / centroméricas.

```{r}

library(dplyr)

wd <- "/Users/marialucenaperez/Documents/WG_lynx_diversity_per_unit/"

# Tel2m
files_tel2m <- list.files(path = wd, pattern = "region.tel2m")

# Esto saca el porcentaje de bases anotadas como de una región determinada para cada población
for (file_tel2m in files_tel2m){
dataframe <- tryCatch({
        if (file.size(paste(wd, file_tel2m, sep="")) > 0)
          {
    read.table(file=paste(wd, file_tel2m, sep=""), header=T) %>% 
            mutate (tel2m_percentage=tel2m_bases/length)
          }
}, error =function(err) {
            # error handler picks up where error was generated
            print(paste("Read.table didn't work!:  ",err))
        })
# Hasta aquí bien, ahora guardo estos archivos en la carpeta de interes.
write.table(dataframe, paste(wd, file_tel2m, sep=""), row.names = F, quote = F, sep = '\t')
}


# Tel10m
files_tel10m <- list.files(path = wd, pattern = "region.tel10m")

# Esto saca el porcentaje de bases anotadas como de una región determinada para cada población
for (file_tel10m in files_tel10m){
dataframe <- tryCatch({
        if (file.size(paste(wd, file_tel10m, sep="")) > 0)
          {
    read.table(file=paste(wd, file_tel10m, sep=""), header=T) %>% 
            mutate (tel10m_percentage=tel10m_bases/length)
          }
}, error =function(err) {
            # error handler picks up where error was generated
            print(paste("Read.table didn't work!:  ",err))
        })
# Hasta aquí bien, ahora guardo estos archivos en la carpeta de interes.
write.table(dataframe, paste(wd, file_tel10m, sep=""), row.names = F, quote = F, sep = '\t')
}

# Centr

files_centr <- list.files(path = wd, pattern = "region.centr")

# Esto saca el porcentaje de bases anotadas como de una región determinada para cada población
for (file_centr in files_centr){
dataframe <- tryCatch({
        if (file.size(paste(wd, file_centr, sep="")) > 0)
          {
    read.table(file=paste(wd, file_centr, sep=""), header=T) %>% 
            mutate (centr_percentage=centr_bases/length)
          }
}, error =function(err) {
            # error handler picks up where error was generated
            print(paste("Read.table didn't work!:  ",err))
        })
# Hasta aquí bien, ahora guardo estos archivos en la carpeta de interes.
write.table(dataframe, paste(wd, file_centr, sep=""), row.names = F, quote = F, sep = '\t')
}


```


------------------------------------------

R analysis



Ahora tengo que descargarme todos los archivos de cada población y despues hacer un super-join para cada población para acabar haciendo una unica tabla con rbind con todas las poblaciones. 

Creación tablas

```{r}

# Ahora hago un join de la tabla principal de diversidad con las de telómeros, centrómeros y cromosoma. 

library(dplyr)
library(ggplot2)
library(tidyr)

wd <- "/Users/marialucenaperez/Documents/WG_lynx_diversity_per_unit/"

# poplist <- c("sm")
poplist <- c("do","po","no","ki")

for (pop in poplist)
{
  if (exists("dataset"))
  {rm (dataset)
  }
  
  files_for_given_pop <- list.files(path = wd, pattern = pop)
  
  for (file_for_given_pop in files_for_given_pop)
  {
    # if the merged dataset doesn't exist, create it
    if (!exists("dataset"))
    {
      dataset <- read.table(paste(wd,file_for_given_pop, sep=""), header=TRUE, sep="\t") 
    }
    
    # if the merged dataset does exist, append to it
    if (exists("dataset"))
    {
      temp_dataset <-read.table(paste(wd,file_for_given_pop, sep=""), header=TRUE, sep="\t")
      dataset<-full_join(dataset, temp_dataset, by = c("scaffold", "start_cero_based", "end", "length", "NAs", "informative_sites", "feature", "strandness", "frame", "id_gene", "id", "watterson_ave", "watterson_sd", "pairwise_ave", "pairwise_sd", "tajimaD", "pop", "specie", "epoch"))   
      rm(temp_dataset)
      assign(pop, dataset) # Con esto quiero ponerle el nombre de la población a la dataframe. 
    }
  }    
}


rm(dataset)

data_diversity <-rbind(do, ki, no, po) %>% 
    mutate( watterson_ave = as.numeric(gsub("\\*10\\^","e",watterson_ave)),
            watterson_sd = as.numeric(gsub("\\*10\\^","e",watterson_sd)),
            pairwise_ave  = as.numeric(gsub("\\*10\\^","e",pairwise_ave)),
            pairwise_sd  = as.numeric(gsub("\\*10\\^","e",pairwise_sd)),
            tajimaD = as.numeric(gsub("\\*10\\^","e",tajimaD)))


write.table (data_diversity, paste(wd, "global.per.unit.averages.chr.all.regions.tsv", sep=""), row.names = F, quote = F, sep = '\t')
write.table (ki, paste(wd, "c_ll_ki_n013.per.unit.averages.chr.all.regions.tsv", sep=""), row.names = F, quote = F, sep = '\t')
write.table (no, paste(wd, "c_ll_no_n008.per.unit.averages.chr.all.regions.tsv", sep=""), row.names = F, quote = F, sep = '\t')
write.table (po, paste(wd, "c_ll_po_n008.per.unit.averages.chr.all.regions.tsv", sep=""), row.names = F, quote = F, sep = '\t')
write.table (do, paste(wd, "c_ll_do_n012.per.unit.averages.chr.all.regions.tsv", sep=""), row.names = F, quote = F, sep = '\t')

```


Cargo las tablas


```{r}

data_diversity <- read.table(paste(wd, "global.per.unit.averages.chr.all.regions.tsv", sep=""), header=T)

# # # 
# Sanity check --> ok!
# lista_ki <- ki$id
# lista_no  <- no$id
# lista_po  <- po$id
# lista_do  <- do$id
# setdiff(lista_ki, lista_no)
# setdiff(lista_ki, lista_po)
# setdiff(lista_ki, lista_do)
# # # 


data_diversity_ki_no <- full_join(ki, no, by = c("scaffold", "start_cero_based", "end", "length", "feature", "strandness", "frame", "id","id_gene")) 

kk <- data_diversity_ki_no %>% mutate (comparison=ifelse(chr.x.x.x==chr.y.x, "equal", "different"))

lista_ki_no <- full_join(ki, no, by = c("scaffold", "start_cero_based", "end", "length", "feature", "strandness", "frame", "id","id_gene")) %>% select (feature, id) %>% unite (feature, id, sep="_", col="prueba") %>% select(prueba)

setdiff(lista_ki, lista_ki_no)
duplicated(lista_ki_no)
duplicated(lista_ki_no) | lista_ki_no(lista_ki_no[nrow(lista_ki_no):1, ])[nrow(lista_ki_no):1]

lista_ki_no[duplicated(lista_ki_no)]


which(data_diversity_ki_no$prueba == 675)

kk <- do %>% filter (id_gene=="intergenic_region_44")
```


Exploracion de los datos

```{r}




ggplot(data = data_diversity, aes(x=feature, y = pairwise_ave, fill=pop)) +
  geom_boxplot() +
  facet_grid(.~chr, scales="free") 





ggplot(data = data_diversity, aes(x=feature, y = pairwise_ave, fill=pop)) +
  geom_boxplot() +
  # geom_point(data=df.pm,aes(x=feature, y = weightedmean, fill=pop),shape = 23, position=position_dodge(width=0.75), size = 3, inherit.aes=FALSE) +
  facet_grid(.~feature, scales="free") +
  scale_y_continuous(trans = 'log10') +
  scale_x_discrete("Genomic region") +
  xlab(label = "Genomic region") + #x title
  ylab(label = "Pairwise average") + # y title
  theme_bw() +  #theme selection for background and lines
 # scale_x_continuous(expand = c(0, 1)) +# + scale_y_continuous(expand = c(0, 0)) +
  ggtitle(paste ("Diversity region",name_diversity[1], sep = " " )) +
  theme(axis.line.x = element_line(color="black", size = 0.5),
        axis.line.y = element_line(color="black", size = 0.5),
        axis.text.x = element_text(angle = 60, vjust = 0.8, hjust = 1),
        axis.text.y = element_text(vjust = 0.2, hjust = 0.2),
        axis.title.y = element_text(margin=margin(r=0.3, unit="cm")),
        axis.title.x = element_text(margin=margin(t=0.5, unit="cm")),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.border = element_blank(),
        panel.background = element_blank(),
        strip.background = element_blank(),
        strip.text.x = element_blank())+ 
        stat_summary(fun.y=mean, colour="darkred", geom="point", shape=18, size=3,show_guide = FALSE) + 
  geom_text(data = df.pm.gathered, aes(feature, percentage_do_sm, label=(percentage_do_sm %>% round(2))),angle = 45, size=2)

```




------------------------------------------

Cosas antiguas!

Correcciones diversity per feature!!

```{bash}

# 8/9/2017: Haciendo un plot me he dado cuenta que hay algunas filas duplicadas en el archivo original .gff3, corresponden a UTR. 
# En la carpeta /home/mlucena/grupolince/notation, están especificadas cuales son. He corregido el gff3, pero como no voy a volver a correr mis archivos para sacar los números, voy a corregir todos los originales para tenerlo todo bien desde el principio. 

# Check:

# sort c_ll_ki_n013.per.unit.averages.tsv | uniq -c | sort -nr | head

# 2 lp23.s36682     338697  338929  232     -1      3UTR    +       .       LYPA23C000008   LYPA23C000008   1.4564973179*10^-06     4.5837356691*10^-06     4.4811971320*10^-07     1.3890498033*10^-06     -3.1562808160*1
# 2 lp23.s36682     328523  328962  439     75      5UTR    -       .       LYPA23C000003   LYPA23C000003   5.0803158132*10^-06     1.2267826914*10^-05     1.8178262696*10^-06     4.0698820431*10^-06     -3.9712504595*1
# 2 lp23.s36682     279677  279811  134     -1      3UTR    +       .       LYPA23C000015   LYPA23C000015   1.0437203878*10^-06     4.1265547119*10^-06     3.1206706874*10^-07     1.2333416354*10^-06     -2.5288512495*1
# 2 lp23.s36682     100823  101610  787     82      3UTR    +       .       LYPA23C000004   LYPA23C000004   3.7669060166*10^-04     9.8624719772*10^-03     3.9965744098*10^-04     1.0556168131*10^-02     3.3103961265*10
# 2 lp23.s26402     92633   95884   3251    166     3UTR    +       .       LYPA23C000017   LYPA23C000017   9.8348133084*10^-06     3.1577310510*10^-04     4.3617798050*10^-06     1.5841654621*10^-04     -3.4633370151*1
# 2 lp23.s10719     96754   96794   40      -1      5UTR    -       .       LYPA23C000041   LYPA23C000041   3.0378406270*10^-06     1.0043475395*10^-06     1.0387353248*10^-06     3.9400597979*10^-07     -3.2716817179*1
# 2 lp23.s10719     156260  156378  118     96      5UTR    -       .       LYPA23C000045   LYPA23C000045   1.3834272997*10^-05     2.2261745529*10^-06     6.3570507930*10^-06     1.2906641862*10^-06     -7.9734277877*1
# 1 scaffold        start   end     length  NAs     feature strandness      frame   id_gene id      watterson_ave   watterson_sd    pairwise_ave    pairwise_sd     tajimaD informative_sites       pop     specie
# 1 lp23.s41700     1       200     199     199     intergenic      -       .       intergenic_region_65795 intergenic_region_65795         0       c_ll_ki_n013    c_ll
# 1 lp23.s41699     1       200     199     199     intergenic      -       .       intergenic_region_65794 intergenic_region_65794         0       c_ll_ki_n013    c_ll


# 491637 c_ll_ki_n013.per.unit.averages.tsv

# Efectivamente aquí también está duplicados, así que corro el loop para corregirlo:

for i in  *.per.unit.averages.tsv
do
echo $i
POP=$(echo ${i} | cut -d "." -f 1 )
SPECIE=$(echo $POP | cut -d"_" -f 1-2)
awk '!seen[$0]++' $i > ${i/.tsv/.corrected.tsv} # remove duplicated rows.
mv ${i/.tsv/.corrected.tsv} $i
done



# Create a unique file with all the information

awk 'FNR==1 && NR!=1{next;}{print}' c_lp*averages.tsv > c_lp_do_n012-c_lp_sm_n019.per.unit.averages.per_specie.tsv
awk 'FNR==1 && NR!=1{next;}{print}' c_ll*averages.tsv > c_ll_ki_n013-c_ll_no_n008-c_ll_po_n008.per.unit.averages.per_specie.tsv


# Compruebo que ha funcionado:

# sort c_ll_ki_n013.per.unit.averages.tsv | uniq -c | sort -nr | head
# 1 scaffold	start	end	length	NAs	feature	strandness	frame	id_gene	id	watterson_ave	watterson_sd	pairwise_ave	pairwise_sd	tajimaD	informative_sites	pop	specie
# 1 lp23.s41700	1	200	
      
# Perfecto!!
      
```

OJO!!

Calcular luego en R medidas pareadas!!! cada unidad con su equivalente en las tres poblaciones!


```{bash}
nota


wc -l c_ll_ki_n013-c_ll_no_n008-c_ll_po_n008.per.unit.averages.per_specie.tsv
1474888 c_ll_ki_n013-c_ll_no_n008-c_ll_po_n008.per.unit.averages.per_specie.tsv


awk '{if ($18 == "") print $0}' c_ll_ki_n013-c_ll_no_n008-c_ll_po_n008.per.unit.averages.per_specie.tsv  | wc -l
134260

# Tengo 134260 lineas que no tienen todos los campos, eso supone un 9,1% de total. 
# ¿A cuantas les faltan todos los campos y por tanto no ha perjudicados los cálculos? (A todas las que tengas informative sites ($11), igual a cero)

awk '{if ($18 == "") print $0}' c_ll_ki_n013-c_ll_no_n008-c_ll_po_n008.per.unit.averages.per_specie.tsv  | awk '{if ($11=="0") print $0}' | wc -l
133589

# Por tanto, de las 134260, a 133589 le faltan todos los datos, por tanto sólo hay 671 lineas con algunos valores sí y otros no. 

133589-134260=671

awk '{if ($18 == "") print $0}' c_ll_ki_n013-c_ll_no_n008-c_ll_po_n008.per.unit.averages.per_specie.tsv  | awk '{if ($11!="0") print $0}' | wc -l
671



# De estos, parece que hay muchos con sitios informativos=1, por lo que asumo que no puede calcular las tajimaD. 

awk '{if ($18 == "") print $0}' c_ll_ki_n013-c_ll_no_n008-c_ll_po_n008.per.unit.averages.per_specie.tsv  | awk '{if ($11!="0") print $0}' | head
lp23.s00005	1472211	1472211	0	-1	CDS	-	0	LYPA23C006624	LYPA23C006624P4_1_1	9.1722861064*10^-06	0.0000000000*10^00	3.7772361236*10^-06	0.0000000000*10^00		1	c_ll_ki_n013	c_ll
lp23.s00006	42798	43028	230	229	intron	+	0	LYPA23C020681	LYPA23C020681T1_intron_13	7.9066451013*10^-06	0.0000000000*10^00	3.1403854839*10^-06	0.0000000000*10^00		1	c_ll_ki_n013	c_ll
lp23.s00010	215823	215849	26	25	lncRNA	+

# Concretamente encontramos,

 awk '{if ($18 == "") print $0}' c_ll_ki_n013-c_ll_no_n008-c_ll_po_n008.per.unit.averages.per_specie.tsv  | awk '{if ($11!="0") print $0}' | awk '{if ($15="1") print $0}' | wc -l
671

# Todo cuadra. 

# 671 supoone un 0.045 del total, así que yo creo que no influye o no mucho en los resultados que tenog. En todo caso, como en algunos casos la coumna de informative site 01 está donde watterson debería estar, ahí si pero en el resto no. 


```


Representation per pop --> I rm this files, but I keep the script in case is useful. 

Este script nos devuelve una gráfica por cada población. Hay un segundo script que devuelve una gráfica para lynx lynx y otra para lynx pardinus. 

```{r}

library ("ggplot2")
library ("plyr")
library ("RColorBrewer")
library ("scales")
library ("magrittr")

# wd <- "/Users/marialucenaperez/Owncloud/publico/WG_diversity/ANGSD/sfs/diversity_per_unit"
wd <- "/home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/diversity_per_unit"
# wd <- "/Users/marialucenaperez/Desktop/Diversity_per_unit"

finsdiversity = list.files(path = wd,pattern="*per.unit.averages.tsv$")
INFORMATIVE_SITES=1
PERCENTAGE_COVERED=0.5

for (i in 1:length(finsdiversity))
{
  infile <- finsdiversity[i]
  name_diversity <- unlist(strsplit(finsdiversity[i], "[.]"))

#### Para que almacene varias dataframe con el nombre correcto
#  dataframename <- paste("data_diversity", name_diversity[1], sep ='_')
#  assign(dataframename, read.csv (paste(wd,infile, sep="/"), header = T, sep = '\t',stringsAsFactors = FALSE,  row.names=NULL, na.strings = "", dec=".", colClasses=c("watterson_ave"="character")))

dataframe <- read.csv (paste(wd,infile, sep="/"), header = T, sep = '\t',stringsAsFactors = FALSE,  row.names=NULL, na.strings = "", dec=".")

dataframe = dataframe[-1,]
dataframe$watterson_ave <- as.numeric(gsub("\\*10\\^","e",dataframe$watterson_ave))
dataframe$pairwise_ave <- as.numeric(gsub("\\*10\\^","e",dataframe$pairwise_ave))
dataframe$tajimaD <- as.numeric(gsub("\\*10\\^","e",dataframe$tajimaD))
#dataframe  <- dataframe[complete.cases(dataframe),] ## --> No se que pasa que la columna de specie sale NA

########################################################
# Filtering
dataframe <- dataframe %>% 
  mutate(percentage_covered=ifelse(NAs>0, (as.numeric(length)-as.numeric(NAs))/as.numeric(length), 1)) %>% dplyr::filter(percentage_covered>PERCENTAGE_COVERED) %>% dplyr::filter(informative_sites>INFORMATIVE_SITES)
name_diversity[1] <- paste(name_diversity[1],".percentagecovered",PERCENTAGE_COVERED,"_mininformativesites",INFORMATIVE_SITES, sep="")
########################################################

df.pm <- ddply(dataframe, "feature", summarise, weightedmean=weighted.mean(pairwise_ave, informative_sites))

df.wm <- ddply(dataframe, "feature", summarise, weightedmean=weighted.mean(watterson_ave, informative_sites))


# pdf(file = paste(wd,"/Diversity_by_feature_pairwise",name_diversity[1],".pdf", sep = ""))
ggplot(data = dataframe, aes(x=feature, y = pairwise_ave)) + 
  geom_boxplot()+
  geom_point(data=df.pm,aes(x=feature, y = weightedmean),shape = 23, 
             size = 3, inherit.aes=FALSE) +
  # scale_y_continuous(limits = c(-20,-3), expand =c(0,0), labels = comma) +
  facet_grid(.~feature, scales="free") +
  scale_y_continuous(trans = 'log10') +
  scale_x_discrete("Genomic feature") +
  xlab(label = "Genomic feature") + #x title
  ylab(label = "Pairwise average") + # y title
  theme_bw() +  #theme selection for background and lines
  # scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  ggtitle(paste ("Diversity by feature",name_diversity[1], sep = " " )) +
  theme(axis.line.x = element_line(color="black", size = 0.5),
        axis.line.y = element_line(color="black", size = 0.5),
        axis.text.x = element_text(angle = 60, vjust = 0.8, hjust = 1),
        axis.text.y = element_text(vjust = 0.2, hjust = 0.2),
        axis.title.y = element_text(margin=margin(r=0.3, unit="cm")),
        axis.title.x = element_text(margin=margin(t=0.5, unit="cm")),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.border = element_blank(),
        panel.background = element_blank(),
        strip.background = element_blank(),
        strip.text.x = element_blank())
ggsave (file = paste(wd,"/",name_diversity[1],".diversity_feature_pairwise.pdf", sep = ""), device = "pdf")
# dev.off()



# pdf(file = paste(wd,"/Diversity_by_feature_watterson",name_diversity[1],".pdf", sep = ""))

ggplot(data = dataframe, aes(x=feature, y = watterson_ave)) + 
  geom_boxplot()+
  geom_point(data=df.wm,aes(x=feature, y = weightedmean),shape = 23, 
             size = 3, inherit.aes=FALSE) +
  # scale_y_continuous(limits = c(-20,-3), expand =c(0,0), labels = comma) +
  facet_grid(.~feature, scales="free") +
  scale_y_continuous(trans = 'log10') +
  scale_x_discrete("Genomic feature") +
  xlab(label = "Genomic feature") + #x title
  ylab(label = "Watterson average") + # y title
  theme_bw() +  #theme selection for background and lines
  # scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  ggtitle(paste ("Diversity by feature",name_diversity[1], sep = " " )) +
  theme(axis.line.x = element_line(color="black", size = 0.5),
        axis.line.y = element_line(color="black", size = 0.5),
        axis.text.x = element_text(angle = 60, vjust = 0.8, hjust = 1),
        axis.text.y = element_text(vjust = 0.2, hjust = 0.2),
        axis.title.y = element_text(margin=margin(r=0.3, unit="cm")),
        axis.title.x = element_text(margin=margin(t=0.5, unit="cm")),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.border = element_blank(),
        panel.background = element_blank(),
        strip.background = element_blank(),
        strip.text.x = element_blank())

ggsave(filename = paste(wd,"/", name_diversity[1],".diversity_feature_watterson.pdf", sep = ""), device = "pdf")


ggplot(data = dataframe, aes(x=feature, y = tajimaD)) + 
  geom_boxplot()+
  # scale_y_continuous(limits = c(-20,-3), expand =c(0,0), labels = comma) +
  facet_grid(.~feature, scales="free") +
  #  scale_y_continuous(trans = 'log10') +
  scale_x_discrete("Genomic feature") +
  xlab(label = "Genomic feature") + #x title
  ylab(label = "Watterson average") + # y title
  theme_bw() +  #theme selection for background and lines
  # scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  ggtitle(paste ("Diversity by feature",name_diversity[1], sep = " " )) +
  theme(axis.line.x = element_line(color="black", size = 0.5),
        axis.line.y = element_line(color="black", size = 0.5),
        axis.text.x = element_text(angle = 60, vjust = 0.8, hjust = 1),
        axis.text.y = element_text(vjust = 0.2, hjust = 0.2),
        axis.title.y = element_text(margin=margin(r=0.3, unit="cm")),
        axis.title.x = element_text(margin=margin(t=0.5, unit="cm")),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.border = element_blank(),
        panel.background = element_blank(),
        strip.background = element_blank(),
        strip.text.x = element_blank())
ggsave(filename = paste(wd,"/",name_diversity[1],".percentagecovered",PERCENTAGE_COVERED,"_mininformativesites",INFORMATIVE_SITES,".diversity_feature_tajimaD.pdf", sep = ""), device = "pdf")


# Exploratory plots

jpeg(paste(wd,"/",name_diversity[1],".plot1.jpg", sep=""))
plot (dataframe$informative_sites, dataframe$tajimaD)
dev.off()

jpeg(paste(wd,"/",name_diversity[1],".plot2.jpg", sep=""))
plot (dataframe$pairwise_ave, dataframe$watterson_ave)
dev.off()

jpeg(paste(wd,"/",name_diversity[1],".plot3.jpg", sep=""))
plot (dataframe$pairwise_ave, dataframe$tajimaD)
dev.off()

jpeg(paste(wd,"/",name_diversity[1],"plot4.jpg", sep=""))
plot (dataframe$watterson_ave, dataframe$tajimaD)
dev.off()

jpeg(paste(wd,"/",name_diversity[1],"plot5.jpg", sep=""))
plot (dataframe$informative_sites, dataframe$pairwise_ave)
dev.off()

jpeg(paste(wd,"/",name_diversity[1],"plot6.jpg", sep=""))
plot (dataframe$informative_sites, dataframe$watterson_ave)
dev.off()


}


# Cosas útiles outliers.shape=NA.

```

Comparación por pares unidad


```{r}

library ("ggplot2")
library ("plyr")
library ("RColorBrewer")
library ("scales")
library ("magrittr")
library(dplyr)

# wd <- "/Users/marialucenaperez/Owncloud/publico/WG_diversity/ANGSD/sfs/diversity_per_unit"
wd <- "/Users/marialucenaperez/Owncloud/publico/WG_diversity/ANGSD/sfs/diversity_per_unit"
# wd <- "/Users/marialucenaperez/Desktop/Diversity_per_region"

finsdiversity = list.files(path = wd,pattern="*.per.unit.averages.tsv$")

for (i in 1:length(finsdiversity))
{
  infile <- finsdiversity[i]
  name_diversity <- unlist(strsplit(finsdiversity[i], "[.]"))
dataframe <- read.csv (paste(wd,infile, sep="/"), header = T, sep = '\t',stringsAsFactors = FALSE,  row.names=NULL, na.strings = "", dec=".")

dataframe = dataframe[-1,]
dataframe$watterson_ave <- as.numeric(gsub("\\*10\\^","e",dataframe$watterson_ave))
dataframe$pairwise_ave <- as.numeric(gsub("\\*10\\^","e",dataframe$pairwise_ave))
dataframe$tajimaD <- as.numeric(gsub("\\*10\\^","e",dataframe$tajimaD))

#### Para que almacene varias dataframe con el nombre correcto
  dataframename <- paste("data_diversity", name_diversity[1], sep ='_')
  assign(dataframename, dataframe)
  
  

}


pair_comparison <- full_join (data_diversity_c_ll_ki_n013, data_diversity_c_ll_no_n008, by=c("row.names"="row.names","scaffold"="scaffold","start"="start", "end"="end",  "feature"="feature", "strandness"="strandness", "frame"="frame", "id_gene"="id_gene", "id"="id")  )

colourCount = length(unique(pair_comparison$feature))
getPalette = colorRampPalette(brewer.pal(9, "Paired"))


ggplot (filter(pair_comparison), aes(x= watterson_ave.x, y=watterson_ave.y , fill=feature, colour=feature)) +
  geom_point(alpha=1/100) +
  geom_smooth(method = "lm", se = FALSE) +
  geom_abline(intercept = 0) +
  scale_fill_manual(values = getPalette(colourCount))+
  scale_colour_manual(values = getPalette(colourCount)) +
   theme(legend.position="bottom") +
  guides(fill=guide_legend(nrow=2))

```


Representation per specie

Esta gráfica representa por especie.

```{r}


# stat_summary(fun.y=mean,

library ("ggplot2")
library ("plyr")
library ("RColorBrewer")
library ("scales")
library ("magrittr")
library("tidyr")
library ("dplyr")
# wd <- "/Users/marialucenaperez/Owncloud/publico/WG_diversity/ANGSD/sfs/diversity_per_unit"
# wd <- "/home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/diversity_per_unit"
# wd <- "/Users/marialucenaperez/Desktop/Diversity_per_unit"

finsdiversity = list.files(path = wd,pattern="*per_specie.tsv$")
INFORMATIVE_SITES=1
PERCENTAGE_COVERED=0


for (i in 1:length(finsdiversity))
{
  infile <- finsdiversity[i]
  name_diversity <- unlist(strsplit(finsdiversity[i], "[.]"))
  dataframe <- read.csv (paste(wd,infile, sep="/"), header = T, sep = '\t',stringsAsFactors = FALSE,  row.names=NULL, na.strings = "", dec=".")
  dataframe = dataframe[-1,]
  dataframe$watterson_ave <- as.numeric(gsub("\\*10\\^","e",dataframe$watterson_ave))
  dataframe$pairwise_ave <- as.numeric(gsub("\\*10\\^","e",dataframe$pairwise_ave))
  dataframe$tajimaD <- as.numeric(gsub("\\*10\\^","e",dataframe$tajimaD))
  dataframe  <- dataframe[complete.cases(dataframe),]
  dataframe$pop <- as.factor(dataframe$pop)
  dataframe$pop <- factor(dataframe$pop, levels=c("c_ll_ki_n013", "c_ll_po_n008", "c_ll_no_n008", "c_lp_sm_n019", "c_lp_do_n012"))
  #### Para que almacene varias dataframe con el nombre correcto
  dataframename <- paste("data_diversity", name_diversity[1], sep ='_')
  assign(dataframename, dataframe)
  
  ########################################################
# Filtering
  
  dataframe <- dataframe %>% 
  mutate(percentage_covered=ifelse(NAs>0, (as.numeric(length)-as.numeric(NAs))/as.numeric(length), 1)) %>% dplyr::filter(percentage_covered>PERCENTAGE_COVERED) %>% dplyr::filter(informative_sites>INFORMATIVE_SITES)
  
}



### LYNX PARDINUS #### 

dataframe <- `data_diversity_c_lp_do_n012-c_lp_sm_n019`
name_diversity[1] <- "c_lp_do_n012-c_lp_sm_n019"
name_diversity[1] <- paste(name_diversity[1],".percentagecovered",PERCENTAGE_COVERED,"_mininformativesites",INFORMATIVE_SITES, sep="")


# Calculo weighted mean y lo guardo en un df.

df.pm <- ddply(dataframe, c("feature","specie","pop"), summarise, weightedmean=weighted.mean(pairwise_ave, informative_sites), mean=mean(pairwise_ave))

df.pm1 <- df.pm %>% select (feature,pop, weightedmean) %>% spread (., pop, weightedmean) %>% select (feature, wm_sm= c_lp_sm_n019,wm_do=c_lp_do_n012)
df.pm2 <- df.pm %>% select (feature,pop, mean) %>% spread (., pop, mean) %>% select (feature, m_sm= c_lp_sm_n019,m_do=c_lp_do_n012)

df.pm_guardar <- full_join(df.pm1, df.pm2)


df.wm <- ddply(dataframe, c("feature","specie","pop"), summarise, weightedmean=weighted.mean(watterson_ave, informative_sites), mean=mean(watterson_ave))

df.wm1 <- df.pm %>% select (feature,pop, weightedmean) %>% spread (., pop, weightedmean) %>% select (feature, wm_sm= c_lp_sm_n019,wm_do=c_lp_do_n012)
df.wm2 <- df.pm %>% select (feature,pop, mean) %>% spread (., pop, mean) %>% select (feature, m_sm= c_lp_sm_n019,m_do=c_lp_do_n012)

df.wm_guardar <- full_join(df.pm1, df.pm2)


# LYNX PARDINUS
# Hago spread para calcular el porcentage que representa do respecto a sm
df.pm_lost <- df.pm %>% select(.,-mean) %>% tidyr::spread(., pop, weightedmean) %>%
 mutate(., percentage_do_sm=(c_lp_do_n012 *100)/c_lp_sm_n019)
# Hago gathered para poder plotearlo.
df.pm.gathered <- tidyr::gather(df.pm_lost, "pop", "weightedmean", -specie, -feature, -percentage_do_sm)
# Hago spread para calcular el porcentage de perdida
df.wm_lost <-df.wm %>% select(.,-mean) %>% tidyr::spread(., pop, weightedmean) %>% mutate(., percentage_do_sm=(c_lp_do_n012 *100)/c_lp_sm_n019)
# Hago gathered para poder plotearlo.
df.wm.gathered <- tidyr::gather(df.wm_lost, "pop", "weightedmean", -specie, -feature, -percentage_do_sm)


# Guardo el porcentage de perdida. 
write.table(df.pm_guardar, paste(wd,"/", name_diversity[1],".pairwise_mean.csv", sep = ""), row.names = F, dec=",", sep=";")
write.table(df.wm_guardar, paste(wd,"/", name_diversity[1],".watterson_mean.csv", sep = ""), row.names = F, dec=",", sep=";")


ggplot(data = dataframe, aes(x=feature, y = pairwise_ave, fill=pop)) +
  geom_boxplot() +
  geom_point(data=df.pm,aes(x=feature, y = weightedmean, fill=pop),shape = 23, position=position_dodge(width=0.75), size = 3, inherit.aes=FALSE) +
  facet_grid(.~feature, scales="free") +
  scale_y_continuous(trans = 'log10') +
  scale_x_discrete("Genomic feature") +
  xlab(label = "Genomic feature") + #x title
  ylab(label = "Pairwise average") + # y title
  theme_bw() +  #theme selection for background and lines
  # scale_x_continuous(expand = c(0, 1)) +# + scale_y_continuous(expand = c(0, 0)) +
  ggtitle(paste ("Diversity feature",name_diversity[1], sep = " " )) +
  theme(axis.line.x = element_line(color="black", size = 0.5),
        axis.line.y = element_line(color="black", size = 0.5),
        axis.text.x = element_text(angle = 60, vjust = 0.8, hjust = 1),
        axis.text.y = element_text(vjust = 0.2, hjust = 0.2),
        axis.title.y = element_text(margin=margin(r=0.3, unit="cm")),
        axis.title.x = element_text(margin=margin(t=0.5, unit="cm")),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.border = element_blank(),
        panel.background = element_blank(),
        strip.background = element_blank(),
        strip.text.x = element_blank())+ 
        stat_summary(fun.y=mean, colour="darkred", geom="point", shape=18, size=3,show_guide = FALSE) + 
  geom_text(data = df.pm.gathered, aes(feature, percentage_do_sm, label=(percentage_do_sm %>% round(2))),angle = 45, size=2)

ggsave (file = paste(wd,"/",name_diversity[1],".diversity_feature_pairwise.eps", sep = ""), device = "eps")
# dev.off()


ggplot(data = dataframe, aes(x=feature, y = watterson_ave, fill =pop)) + 
  geom_boxplot(outlier.shape=NA)+
  geom_point(data=df.wm,aes(x=feature, y = weightedmean, fill=pop),shape = 23, position=position_dodge(width=0.75), size = 3, inherit.aes=FALSE) +
  # scale_y_continuous(limits = c(-20,-3), expand =c(0,0), labels = comma) +
  facet_grid(.~feature, scales="free") +
  scale_y_continuous(trans = 'log10') +
  scale_x_discrete("Genomic feature") +
  xlab(label = "Genomic feature") + #x title
  ylab(label = "Watterson average") + # y title
  theme_bw() +  #theme selection for background and lines
  # scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  ggtitle(paste ("Diversity by feature",name_diversity[1], sep = " " )) +
  theme(axis.line.x = element_line(color="black", size = 0.5),
        axis.line.y = element_line(color="black", size = 0.5),
        axis.text.x = element_text(angle = 60, vjust = 0.8, hjust = 1),
        axis.text.y = element_text(vjust = 0.2, hjust = 0.2),
        axis.title.y = element_text(margin=margin(r=0.3, unit="cm")),
        axis.title.x = element_text(margin=margin(t=0.5, unit="cm")),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.border = element_blank(),
        panel.background = element_blank(),
        strip.background = element_blank(),
        strip.text.x = element_blank()) + 
        stat_summary(fun.y=mean, colour="darkred", geom="point", shape=18, size=3,show_guide = FALSE) + 
  geom_text(data = df.wm.gathered, aes(feature, percentage_do_sm, label=(percentage_do_sm %>% round(2))),angle = 45, size=2)
ggsave(filename = paste(wd,"/",name_diversity[1],".diversity_feature_watterson.eps", sep = ""), device="eps")



### LYNX LYNX ####

dataframe <- `data_diversity_c_ll_ki_n013-c_ll_no_n008-c_ll_po_n008`
name_diversity[1] <- "c_ll_ki_n013-c_ll_no_n008-c_ll_po_n008"
name_diversity[1] <- paste(name_diversity[1],".percentagecovered",PERCENTAGE_COVERED,"_mininformativesites",INFORMATIVE_SITES, sep="")

# Calculo weighted mean y lo guardo en un df.

df.pm <- ddply(dataframe, c("feature","specie","pop"), summarise, weightedmean=weighted.mean(pairwise_ave, informative_sites), mean=mean(pairwise_ave))

df.pm1 <- df.pm %>% select (feature,pop, weightedmean) %>% spread (., pop, weightedmean) %>% select (feature, wm_ki= c_ll_ki_n013,wm_no=c_ll_no_n008,wm_po=c_ll_po_n008)
df.pm2 <- df.pm %>% select (feature,pop, mean) %>% spread (., pop, mean) %>% select (feature, m_ki= c_ll_ki_n013,m_no=c_ll_no_n008,m_po=c_ll_po_n008)

df.pm_guardar <- full_join(df.pm1, df.pm2)


df.wm <- ddply(dataframe, c("feature","specie","pop"), summarise, weightedmean=weighted.mean(watterson_ave, informative_sites), mean=mean(watterson_ave))

df.wm1 <- df.wm %>% select (feature,pop, weightedmean) %>% spread (., pop, weightedmean) %>% select (feature, wm_ki= c_ll_ki_n013,wm_no=c_ll_no_n008,wm_po=c_ll_po_n008)
df.wm2 <- df.wm %>% select (feature,pop, mean) %>% spread (., pop, mean) %>% select (feature, m_ki= c_ll_ki_n013,m_no=c_ll_no_n008,m_po=c_ll_po_n008)

df.wm_guardar <- full_join(df.pm1, df.pm2)


df.pm_lost <- df.pm %>% select(.,-mean) %>% tidyr::spread(., pop, weightedmean) %>% 
  mutate(., percentage_po_ki=(c_ll_po_n008 *100)/c_ll_ki_n013) %>% 
  mutate(., percentage_no_ki=(c_ll_no_n008*100)/c_ll_ki_n013)
df.pm.gathered <- tidyr::gather(df.pm_lost, "pop", "weightedmean", -specie, -feature,  -percentage_po_ki, -percentage_no_ki)

df.wm_lost <- df.wm %>% select(.,-mean) %>% tidyr::spread(., pop, weightedmean) %>% 
  mutate(., percentage_po_ki=(c_ll_po_n008 *100)/c_ll_ki_n013) %>% 
  mutate(., percentage_no_ki=(c_ll_no_n008*100)/c_ll_ki_n013)
df.wm.gathered <- tidyr::gather(df.wm_lost, "pop", "weightedmean", -specie, -feature,  -percentage_po_ki, -percentage_no_ki) 

# Guardo el porcentage de perdida. 
write.table(df.pm_guardar, paste(wd,"/", name_diversity[1],".pairwise_mean.csv", sep = ""), row.names = F, dec=",", sep=";")
write.table(df.wm_guardar, paste(wd,"/", name_diversity[1],".watterson_mean.csv", sep = ""), row.names = F, dec=",", sep=";")



ggplot(data = dataframe, aes(x=feature, y = pairwise_ave, fill=pop)) +
  geom_boxplot(outlier.shape=NA)+
  geom_point(data=df.pm,aes(x=feature, y = weightedmean, fill=pop),shape = 23, position=position_dodge(width=0.75), size = 3, inherit.aes=FALSE) +
  # scale_y_continuous(limits = c(-20,-3), expand =c(0,0), labels = comma) +
  facet_grid(.~feature, scales="free") +
  scale_y_continuous(trans = 'log10') +
  scale_x_discrete("Genomic feature") +
  xlab(label = "Genomic feature") + #x title
  ylab(label = "Pairwise average") + # y title
  theme_bw() +  #theme selection for background and lines
  #scale_x_continuous(expand = c(0, 1)) +# + scale_y_continuous(expand = c(0, 0)) +
  ggtitle(paste ("Diversity feature",name_diversity[1], sep = " " )) +
  theme(axis.line.x = element_line(color="black", size = 0.5),
        axis.line.y = element_line(color="black", size = 0.5),
        axis.text.x = element_text(angle = 60, vjust = 0.8, hjust = 1),
        axis.text.y = element_text(vjust = 0.2, hjust = 0.2),
        axis.title.y = element_text(margin=margin(r=0.3, unit="cm")),
        axis.title.x = element_text(margin=margin(t=0.5, unit="cm")),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.border = element_blank(),
        panel.background = element_blank(),
        strip.background = element_blank(),
        strip.text.x = element_blank())+ 
        stat_summary(fun.y=mean, colour="darkred", geom="point", shape=18, size=3,show_guide = FALSE) + 
  geom_text(data = df.pm.gathered, aes(feature, percentage_po_ki, label=paste((percentage_po_ki %>% round(2)),(percentage_no_ki%>% round(2)), sep='\n')), angle = 45, size=2, vjust="inward",hjust="inward")
ggsave (file = paste(wd,"/",name_diversity[1],".diversity_feature_pairwise.eps", sep = ""), device = "eps")
# dev.off()



ggplot(data = dataframe, aes(x=feature, y = watterson_ave, fill =pop)) + 
  geom_boxplot(outlier.shape=NA, position=position_dodge(width=1.5))+
  geom_point(data=df.wm,aes(x=feature, y = weightedmean, fill=pop),shape = 23, position=position_dodge(width=1.5), size = 3, inherit.aes=FALSE) +
  facet_grid(.~feature, scales="free") +
  scale_y_continuous(trans = 'log10') +
  scale_x_discrete("Genomic feature") +
  xlab(label = "Genomic feature") + #x title
  ylab(label = "Watterson average") + # y title
  theme_bw() +  #theme selection for background and lines
  # scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  ggtitle(paste ("Diversity by feature",name_diversity[1], sep = " " )) +
  theme(axis.line.x = element_line(color="black", size = 0.5),
        axis.line.y = element_line(color="black", size = 0.5),
        axis.text.x = element_text(angle = 60, vjust = 0.8, hjust = 1),
        axis.text.y = element_text(vjust = 0.2, hjust = 0.2),
        axis.title.y = element_text(margin=margin(r=0.3, unit="cm")),
        axis.title.x = element_text(margin=margin(t=0.5, unit="cm")),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.border = element_blank(),
        panel.background = element_blank(),
        strip.background = element_blank(),
        strip.text.x = element_blank()) + 
        stat_summary(fun.y=mean, colour="darkred", geom="point", shape=18, size=3,show_guide = FALSE) + 
        geom_text(data = df.wm.gathered, aes(feature, percentage_po_ki, label=paste((percentage_po_ki %>% round(2)),(percentage_no_ki%>% round(2)), sep='\n')), angle = 45, size=2, vjust="inward",hjust="inward")

ggsave(filename = paste(wd,"/",name_diversity[1],".diversity_feature_watterson.eps", sep =""),device="eps")


# Cosas útiles 

# geom_box(outliers.shape=NA)

# Si quiero plotear todos los indices a la vez:
#index = c("watterson_ave", "pairwise_ave")
#for (i in 1:length(index))
#{
#  ggplot(data = dataframe, aes_string(x='feature', y = index[1], fill='pop'))+



```

 Pairwise Pi diversity

```{r}


library ("ggplot2")
library ("plyr")
library ("dplyr")
library ("RColorBrewer")
library ("scales")
library ("magrittr")
library("tidyr")
library(viridis)


# wd <- "/Users/marialucenaperez/Owncloud/publico/WG_diversity/ANGSD/sfs/diversity_per_unit"
# wd <- "/home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/diversity_per_unit"
wd <- "/Users/marialucenaperez/Desktop/"


# finsdiversity = list.files(path = wd,pattern="*per.unit.averages.per_specie.tsv$") %>% grep("informative_sites", ., invert = T, value=T)

infile <- "c_ll_ki_n013-c_ll_no_n008-c_ll_po_n008.per.unit.averages.per_specie.tsv"

INFORMATIVE_SITES=1
PERCENTAGE_COVERED=0




# for (i in 1:length(finsdiversity))
# {
#  infile <- finsdiversity[i]
  
  dataframe <- read.csv (paste (wd,infile, sep="/"), header = T, sep = '\t', stringsAsFactors = FALSE,  row.names=NULL, na.strings = "", dec=".") 
  dataframe$watterson_ave <- as.numeric(gsub("\\*10\\^","e",dataframe$watterson_ave))
  dataframe$pairwise_ave <- as.numeric(gsub("\\*10\\^","e",dataframe$pairwise_ave))
  dataframe$tajimaD <- as.numeric(gsub("\\*10\\^","e",dataframe$tajimaD))
# Filtering
  dataframe <- dataframe %>% 
  mutate(percentage_covered=ifelse(NAs>0, (as.numeric(length)-as.numeric(NAs))/as.numeric(length), 1)) %>% dplyr::filter(percentage_covered>PERCENTAGE_COVERED) %>% dplyr::filter(informative_sites>INFORMATIVE_SITES)
  

  
# scaffold_start_end_feature_id, length NAs feature strandness frame id_gene watterson_ave watterson_sd pairwise_ave pairwise_sdtajimaD informative_sites pop specie length NAs
distinct_pop <- dataframe %>% distinct(pop)
distinct_vector_pop = distinct_pop$pop

# distinct_vector_pop=c("don","smo","po","cat","bia")

combinations <- t(combn(distinct_vector_pop, 2)) # %>% mutate (combination=paste(V1,V2,sep="/")) %>% select(combination)

# for(a in 1:nrow(combinations)) {
#    row <- NULL
    row <- combinations[a,]
    # do stuff with row
    POP1 <- NULL
    POP2 <- NULL
    POP1 <- row[1]
    POP2 <- row[2]
    NAME <- paste(POP1,"_",POP2,".percentagecovered",PERCENTAGE_COVERED,"_mininformativesites",INFORMATIVE_SITES, sep="")

    
    df_POP1 <- filter (dataframe, pop == POP1) %>% mutate(., feature2=feature) %>% unite (., scaffold, start, end, feature2, id, col="scaffold_start_end_feature_id")
    
    df_POP2 <- filter (dataframe, pop == POP2) %>% mutate(., feature2=feature) %>% unite (., scaffold, start, end, feature2, id, col="scaffold_start_end_feature_id") 
    

    vector_positions_POP1 <- unique(df_POP1$scaffold_start_end_feature_id) %>% sort(.)
    vector_positions_POP2 <- unique(df_POP2$scaffold_start_end_feature_id) %>% sort(.)
    
    vector_common_positions <- as.vector(intersect(vector_positions_POP1, vector_positions_POP2))
    
    df_POP1_filtered <- df_POP1 %>% filter(scaffold_start_end_feature_id  %in% vector_common_positions) %>% arrange(., scaffold_start_end_feature_id) %>% select (., scaffold_start_end_feature_id, feature, id_gene, pairwise_ave, informative_sites, length, NAs, pop )
    
    df_POP2_filtered <- df_POP2 %>% filter(scaffold_start_end_feature_id  %in% vector_common_positions) %>% arrange(., scaffold_start_end_feature_id) %>% select (., scaffold_start_end_feature_id, feature, id_gene, pairwise_ave, informative_sites, length, NAs, pop )

if (identical(df_POP1_filtered$scaffold_start_end_feature_id, df_POP2_filtered$scaffold_start_end_feature_id))
{ df_pairwise <- cbind (
  setNames(data.frame(df_POP1_filtered), c("scaffold_start_end_feature_id", "feature", "id_gene", "pairwise_ave1","informative_sites1", "length1", "NAs1", "pop1")),
  setNames(data.frame(df_POP2_filtered), c("scaffold_start_end_feature_id2", "feature2", "id_gene2", "pairwise_ave2","informative_sites2", "length2", "NAs2", "pop2"))) %>% select_ (., "scaffold_start_end_feature_id", "feature", "id_gene", "pairwise_ave1","informative_sites1", "length1", "NAs1", "pop1","pairwise_ave2","informative_sites2", "length2", "NAs2", "pop2")
           } else {print("First column not matching") }

    
ggplot(df_pairwise, aes(informative_sites1, informative_sites2)) +
  geom_point() +
  labs(x=paste("Informative sites", POP1, sep =" "),y=paste("Informative sites", POP2, sep =" ")) +
  theme_classic()

ggsave(filename = paste(wd,"/",NAME,"_informativesites.eps", sep = ""), device="eps")


ggplot(df_pairwise, aes(pairwise_ave1, pairwise_ave2,  color = informative_sites1)) +
  geom_point() +
  facet_wrap(~feature) +
  # scale_color_gradient(low="red", high="blue") +
  #scale_color_gradientn(colours = rainbow(5)) +
  # scale_colour_gradient(limits = c(0, quantile(df_pairwise$informative_sites1, 0.5))) +
  # scale_colour_gradientn(colours = myPalette(100), values=nrm.vals) +
  labs(colour = paste("Informative sites", POP1, sep =" ")) +
  labs(x=paste("Pi diversity", POP1, sep =" "),y=paste("Pi diversity", POP2, sep =" ")) +
  scale_color_viridis(option="magma",limits = c(0, quantile(df_pairwise$informative_sites1, 0.5))) +
  theme_classic()

ggsave(filename = paste(wd,"/",NAME,"_informative_sites1_Pi_pairwise.eps", sep = ""), device="eps")

}

}

# https://cran.r-project.org/web/packages/viridis/vignettes/intro-to-viridis.html

```


Representation per pop

Este script nos devuelve una gráfica por cada población. Hay un segundo script que devuelve una gráfica para lynx lynx y otra para lynx pardinus. 



```{r}

library ("ggplot2")
library ("plyr")
library ("RColorBrewer")
library ("scales")
library ("magrittr")

# wd <- "/Users/marialucenaperez/Owncloud/publico/WG_diversity/ANGSD/sfs/diversity_per_unit"
wd <- "/home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/diversity_per_region"
# wd <- "/Users/marialucenaperez/Desktop/Diversity_per_region"

finsdiversity = list.files(path = wd,pattern="*.per.region.averages.tsv$")

for (i in 1:length(finsdiversity))
{
  infile <- finsdiversity[i]
  name_diversity <- unlist(strsplit(finsdiversity[i], "[.]"))

#### Para que almacene varias dataframe con el nombre correcto
#  dataframename <- paste("data_diversity", name_diversity[1], sep ='_')
#  assign(dataframename, read.csv (paste(wd,infile, sep="/"), header = T, sep = '\t',stringsAsFactors = FALSE,  row.names=NULL, na.strings = "", dec=".", colClasses=c("watterson_ave"="character")))

dataframe <- read.csv (paste(wd,infile, sep="/"), header = T, sep = '\t',stringsAsFactors = FALSE,  row.names=NULL, na.strings = "", dec=".")

dataframe = dataframe[-1,]
dataframe$watterson_ave <- as.numeric(gsub("\\*10\\^","e",dataframe$watterson_ave))
dataframe$pairwise_ave <- as.numeric(gsub("\\*10\\^","e",dataframe$pairwise_ave))
dataframe$tajimaD <- as.numeric(gsub("\\*10\\^","e",dataframe$tajimaD))
#dataframe  <- dataframe[complete.cases(dataframe),] ## --> No se que pasa que la columna de specie sale NA

########################################################
# Filtering
dataframe <- dataframe %>% 
  mutate(percentage_covered=ifelse(NAs>0, (as.numeric(length)-as.numeric(NAs))/as.numeric(length), 1)) %>% dplyr::filter(percentage_covered>0.5) %>% dplyr::filter(informative_sites>10)
name_diversity[1] <- paste(name_diversity[1],".percentagecovered05_mininformativesites10", sep="")
########################################################

ddply(dataframe, "feature", summarise, weightedmean=mean(pairwise_ave))
df.pm <- ddply(dataframe, "feature", summarise, weightedmean=weighted.mean(pairwise_ave, informative_sites))


ddply(dataframe, "feature", summarise, weightedmean=mean(watterson_ave))
df.wm <- ddply(dataframe, "feature", summarise, weightedmean=weighted.mean(watterson_ave, informative_sites))


# pdf(file = paste(wd,"/Diversity_by_region_pairwise",name_diversity[1],".pdf", sep = ""))
ggplot(data = dataframe, aes(x=feature, y = pairwise_ave)) + 
  geom_boxplot()+
  geom_point(data=df.pm,aes(x=feature, y = weightedmean),shape = 23, 
             size = 3, inherit.aes=FALSE) +
  # scale_y_continuous(limits = c(-20,-3), expand =c(0,0), labels = comma) +
  facet_grid(.~feature, scales="free") +
  scale_y_continuous(trans = 'log10') +
  scale_x_discrete("Genomic region") +
  xlab(label = "Genomic region") + #x title
  ylab(label = "Pairwise average") + # y title
  theme_bw() +  #theme selection for background and lines
  # scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  ggtitle(paste ("Diversity by region",name_diversity[1], sep = " " )) +
  theme(axis.line.x = element_line(color="black", size = 0.5),
        axis.line.y = element_line(color="black", size = 0.5),
        axis.text.x = element_text(angle = 60, vjust = 0.8, hjust = 1),
        axis.text.y = element_text(vjust = 0.2, hjust = 0.2),
        axis.title.y = element_text(margin=margin(r=0.3, unit="cm")),
        axis.title.x = element_text(margin=margin(t=0.5, unit="cm")),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.border = element_blank(),
        panel.background = element_blank(),
        strip.background = element_blank(),
        strip.text.x = element_blank())
ggsave (file = paste(wd,"/",name_diversity[1],".diversity_region_pairwise.pdf", sep = ""), device = "pdf")
# dev.off()



# pdf(file = paste(wd,"/Diversity_by_region_watterson",name_diversity[1],".pdf", sep = ""))

ggplot(data = dataframe, aes(x=feature, y = watterson_ave)) + 
  geom_boxplot()+
  geom_point(data=df.wm,aes(x=feature, y = weightedmean),shape = 23, 
             size = 3, inherit.aes=FALSE) +
  # scale_y_continuous(limits = c(-20,-3), expand =c(0,0), labels = comma) +
  facet_grid(.~feature, scales="free") +
  scale_y_continuous(trans = 'log10') +
  scale_x_discrete("Genomic region") +
  xlab(label = "Genomic region") + #x title
  ylab(label = "Watterson average") + # y title
  theme_bw() +  #theme selection for background and lines
  # scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  ggtitle(paste ("Diversity by region",name_diversity[1], sep = " " )) +
  theme(axis.line.x = element_line(color="black", size = 0.5),
        axis.line.y = element_line(color="black", size = 0.5),
        axis.text.x = element_text(angle = 60, vjust = 0.8, hjust = 1),
        axis.text.y = element_text(vjust = 0.2, hjust = 0.2),
        axis.title.y = element_text(margin=margin(r=0.3, unit="cm")),
        axis.title.x = element_text(margin=margin(t=0.5, unit="cm")),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.border = element_blank(),
        panel.background = element_blank(),
        strip.background = element_blank(),
        strip.text.x = element_blank())

ggsave(filename = paste(wd,"/", name_diversity[1], ".diversity_region_watterson.pdf", sep = ""), device = "pdf")


ggplot(data = dataframe, aes(x=feature, y = tajimaD)) + 
  geom_boxplot()+
  # scale_y_continuous(limits = c(-20,-3), expand =c(0,0), labels = comma) +
  facet_grid(.~feature, scales="free") +
  #  scale_y_continuous(trans = 'log10') +
  scale_x_discrete("Genomic region") +
  xlab(label = "Genomic region") + #x title
  ylab(label = "Watterson average") + # y title
  theme_bw() +  #theme selection for background and lines
  # scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  ggtitle(paste ("Diversity by region",name_diversity[1], sep = " " )) +
  theme(axis.line.x = element_line(color="black", size = 0.5),
        axis.line.y = element_line(color="black", size = 0.5),
        axis.text.x = element_text(angle = 60, vjust = 0.8, hjust = 1),
        axis.text.y = element_text(vjust = 0.2, hjust = 0.2),
        axis.title.y = element_text(margin=margin(r=0.3, unit="cm")),
        axis.title.x = element_text(margin=margin(t=0.5, unit="cm")),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.border = element_blank(),
        panel.background = element_blank(),
        strip.background = element_blank(),
        strip.text.x = element_blank())
ggsave(filename = paste(wd,"/",name_diversity[1],".diversity_region_tajimaD.pdf", sep = ""), device = "pdf")


# Exploratory plots

jpeg(paste(wd,"/",name_diversity[1],".plot1.jpg", sep=""))
plot (dataframe$informative_sites, dataframe$tajimaD)
dev.off()

jpeg(paste(wd,"/",name_diversity[1],".plot2.jpg", sep=""))
plot (dataframe$pairwise_ave, dataframe$watterson_ave)
dev.off()

jpeg(paste(wd,"/",name_diversity[1],".plot3.jpg", sep=""))
plot (dataframe$pairwise_ave, dataframe$tajimaD)
dev.off()

jpeg(paste(wd,"/",name_diversity[1],"plot4.jpg", sep=""))
plot (dataframe$watterson_ave, dataframe$tajimaD)
dev.off()

jpeg(paste(wd,"/",name_diversity[1],"plot5.jpg", sep=""))
plot (dataframe$informative_sites, dataframe$pairwise_ave)
dev.off()

jpeg(paste(wd,"/",name_diversity[1],"plot6.jpg", sep=""))
plot (dataframe$informative_sites, dataframe$watterson_ave)
dev.off()


}


# Cosas útiles outliers.shape=NA.

```

Representation per specie

Esta gráfica representa por especie.

```{r}

library ("ggplot2")
library ("plyr")
library ("RColorBrewer")
library ("scales")
library ("magrittr")
library("tidyr")

# wd <- "/Users/marialucenaperez/Owncloud/publico/WG_diversity/ANGSD/sfs/diversity_per_unit"
wd <- "/home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/diversity_per_region"
# wd <- "/Users/marialucenaperez/Desktop/Diversity_per_unit"

finsdiversity = list.files(path = wd,pattern="*per_specie.tsv$")


for (i in 1:length(finsdiversity))
{
  infile <- finsdiversity[i]
  name_diversity <- unlist(strsplit(finsdiversity[i], "[.]"))
  dataframe <- read.csv (paste(wd,infile, sep="/"), header = T, sep = '\t',stringsAsFactors = FALSE,  row.names=NULL, na.strings = "", dec=".")
  dataframe = dataframe[-1,]
  dataframe$watterson_ave <- as.numeric(gsub("\\*10\\^","e",dataframe$watterson_ave))
  dataframe$pairwise_ave <- as.numeric(gsub("\\*10\\^","e",dataframe$pairwise_ave))
  dataframe$tajimaD <- as.numeric(gsub("\\*10\\^","e",dataframe$tajimaD))
  dataframe  <- dataframe[complete.cases(dataframe),]
  dataframe$pop <- as.factor(dataframe$pop)
  dataframe$pop <- factor(dataframe$pop, levels=c("c_ll_ki_n013", "c_ll_po_n008", "c_ll_no_n008", "c_lp_sm_n019", "c_lp_do_n012"))
  #### Para que almacene varias dataframe con el nombre correcto
  dataframename <- paste("data_diversity", name_diversity[1], sep ='_')
  assign(dataframename, dataframe)
}


##### Lynx pardinus


dataframe <- `data_diversity_c_lp_do_n012-c_lp_sm_n019`
name_diversity[1] <- "c_lp_do_n012-c_lp_sm_n019"


# Calculo weighted mean y lo guardo en un df.

df.pm <- ddply(dataframe, c("feature","specie","pop"), summarise, weightedmean=weighted.mean(pairwise_ave, informative_sites), mean=mean(pairwise_ave))
df.wm <- ddply(dataframe, c("feature","specie","pop"), summarise, weightedmean=weighted.mean(watterson_ave, informative_sites), mean=mean(watterson_ave))



# LYNX PARDINUS
# Hago spread para calcular el porcentage que representa do respecto a sm
df.pm_lost <- df.pm %>%  select (., -mean) %>% tidyr::spread(., pop, weightedmean) %>%
 mutate(., percentage_do_sm=(c_lp_do_n012 *100)/c_lp_sm_n019)
# Hago gathered para poder plotearlo.
df.pm.gathered <- tidyr::gather(df.pm_lost, "pop", "weightedmean", -specie, -feature, -percentage_do_sm)

# Hago spread para calcular el porcentage de perdida
df.wm_lost <- df.pm %>%  select (., -mean) %>% tidyr::spread(., pop, weightedmean) %>% mutate(., percentage_do_sm=(c_lp_do_n012 *100)/c_lp_sm_n019)
# Hago gathered para poder plotearlo.
df.wm.gathered <- tidyr::gather(df.wm_lost, "pop", "weightedmean", -specie, -feature, -percentage_do_sm)


# Guardo el porcentage de perdida. 
write.csv(df.pm, paste(wd,"/", name_diversity[1],".pairwise_mean.csv", sep = ""), row.names = F)
write.csv(df.wm, paste(wd,"/", name_diversity[1],".watterson_mean.csv", sep = ""), row.names = F)


ggplot(data = dataframe, aes(x=feature, y = pairwise_ave, fill=pop)) +
  geom_boxplot(outlier.shape=NA)+
  geom_point(data=df.pm,aes(x=feature, y = weightedmean, fill=pop),shape = 23, position=position_dodge(width=0.75), size = 3, inherit.aes=FALSE) +
  facet_grid(.~feature, scales="free") +
  scale_y_continuous(trans = 'log10') +
  scale_x_discrete("Genomic region") +
  xlab(label = "Genomic region") + #x title
  ylab(label = "Pairwise average") + # y title
  theme_bw() +  #theme selection for background and lines
 # scale_x_continuous(expand = c(0, 1)) +# + scale_y_continuous(expand = c(0, 0)) +
  ggtitle(paste ("Diversity region",name_diversity[1], sep = " " )) +
  theme(axis.line.x = element_line(color="black", size = 0.5),
        axis.line.y = element_line(color="black", size = 0.5),
        axis.text.x = element_text(angle = 60, vjust = 0.8, hjust = 1),
        axis.text.y = element_text(vjust = 0.2, hjust = 0.2),
        axis.title.y = element_text(margin=margin(r=0.3, unit="cm")),
        axis.title.x = element_text(margin=margin(t=0.5, unit="cm")),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.border = element_blank(),
        panel.background = element_blank(),
        strip.background = element_blank(),
        strip.text.x = element_blank())+ 
        stat_summary(fun.y=mean, colour="darkred", geom="point", shape=18, size=3,show_guide = FALSE) + 
  geom_text(data = df.pm.gathered, aes(feature, percentage_do_sm, label=(percentage_do_sm %>% round(2))),angle = 45, size=2)
ggsave (file = paste(wd,"/",name_diversity[1],".diversity_region_pairwise.eps", sep = ""), device = "eps")
# dev.off()


ggplot(data = dataframe, aes(x=feature, y = watterson_ave, fill =pop)) + 
  geom_boxplot(outlier.shape=NA)+
  geom_point(data=df.wm,aes(x=feature, y = weightedmean, fill=pop),shape = 23, position=position_dodge(width=0.75), size = 3, inherit.aes=FALSE) +
  # scale_y_continuous(limits = c(-20,-3), expand =c(0,0), labels = comma) +
  facet_grid(.~feature, scales="free") +
  scale_y_continuous(trans = 'log10') +
  scale_x_discrete("Genomic region") +
  xlab(label = "Genomic region") + #x title
  ylab(label = "Watterson average") + # y title
  theme_bw() +  #theme selection for background and lines
  # scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  ggtitle(paste ("Diversity by region",name_diversity[1], sep = " " )) +
  theme(axis.line.x = element_line(color="black", size = 0.5),
        axis.line.y = element_line(color="black", size = 0.5),
        axis.text.x = element_text(angle = 60, vjust = 0.8, hjust = 1),
        axis.text.y = element_text(vjust = 0.2, hjust = 0.2),
        axis.title.y = element_text(margin=margin(r=0.3, unit="cm")),
        axis.title.x = element_text(margin=margin(t=0.5, unit="cm")),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.border = element_blank(),
        panel.background = element_blank(),
        strip.background = element_blank(),
        strip.text.x = element_blank()) + 
        stat_summary(fun.y=mean, colour="darkred", geom="point", shape=18, size=3,show_guide = FALSE) + 
  geom_text(data = df.wm.gathered, aes(feature, percentage_do_sm, label=(percentage_do_sm %>% round(2))),angle = 45, size=2)
ggsave(filename = paste(wd,"/",name_diversity[1],".diversity_region_watterson.eps", sep = ""), device="eps")

#### Lynx lynx

dataframe <- `data_diversity_c_ll_ki_n013-c_ll_no_n008-c_ll_po_n008`
name_diversity[1] <- "c_ll_ki_n013-c_ll_no_n008-c_ll_po_n008"

# Calculo weighted mean y lo guardo en un df.

df.pm <- ddply(dataframe, c("feature","specie","pop"), summarise, weightedmean=weighted.mean(pairwise_ave, informative_sites), mean=mean(pairwise_ave))
df.wm <- ddply(dataframe, c("feature","specie","pop"), summarise, weightedmean=weighted.mean(watterson_ave, informative_sites), mean=mean(watterson_ave))


df.pm_lost <- df.pm %>%  select (., -mean) %>% tidyr::spread(., pop, weightedmean) %>% 
  mutate(., percentage_po_ki=(c_ll_po_n008 *100)/c_ll_ki_n013) %>% 
  mutate(., percentage_no_ki=(c_ll_no_n008*100)/c_ll_ki_n013)
df.pm.gathered <- tidyr::gather(df.pm_lost, "pop", "weightedmean", -specie, -feature,  -percentage_po_ki, -percentage_no_ki)

df.wm_lost <- df.wm %>%  select (., -mean) %>% tidyr::spread(., pop, weightedmean) %>% 
  mutate(., percentage_po_ki=(c_ll_po_n008 *100)/c_ll_ki_n013) %>% 
  mutate(., percentage_no_ki=(c_ll_no_n008*100)/c_ll_ki_n013)
df.wm.gathered <- tidyr::gather(df.wm_lost, "pop", "weightedmean", -specie, -feature,  -percentage_po_ki, -percentage_no_ki) 



# Guardo el porcentage de perdida. 
write.csv(df.wm_lost, paste(wd,"/", name_diversity[1],".pairwise_mean.csv", sep = ""))
write.csv(df.wm_lost, paste(wd,"/", name_diversity[1],".watterson_mean.csv", sep = ""))

ggplot(data = dataframe, aes(x=feature, y = pairwise_ave, fill=pop)) +
  geom_boxplot(outlier.shape=NA)+
  geom_point(data=df.pm,aes(x=feature, y = weightedmean, fill=pop),shape = 23, position=position_dodge(width=0.75), size = 3, inherit.aes=FALSE) +
  # scale_y_continuous(limits = c(-20,-3), expand =c(0,0), labels = comma) +
  facet_grid(.~feature, scales="free") +
  scale_y_continuous(trans = 'log10') +
  scale_x_discrete("Genomic region") +
  xlab(label = "Genomic region") + #x title
  ylab(label = "Pairwise average") + # y title
  theme_bw() +  #theme selection for background and lines
  #scale_x_continuous(expand = c(0, 1)) +# + scale_y_continuous(expand = c(0, 0)) +
  ggtitle(paste ("Diversity region",name_diversity[1], sep = " " )) +
  theme(axis.line.x = element_line(color="black", size = 0.5),
        axis.line.y = element_line(color="black", size = 0.5),
        axis.text.x = element_text(angle = 60, vjust = 0.8, hjust = 1),
        axis.text.y = element_text(vjust = 0.2, hjust = 0.2),
        axis.title.y = element_text(margin=margin(r=0.3, unit="cm")),
        axis.title.x = element_text(margin=margin(t=0.5, unit="cm")),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.border = element_blank(),
        panel.background = element_blank(),
        strip.background = element_blank(),
        strip.text.x = element_blank())+
  geom_text(data = df.pm.gathered, aes(feature, percentage_po_ki, label=paste((percentage_po_ki %>% round(2)),(percentage_no_ki%>% round(2)), sep='\n')), angle = 45, size=2, vjust="inward",hjust="inward")

ggsave (file = paste(wd,"/",name_diversity[1],".diversity_region_pairwise.eps", sep = ""), device = "eps")
# dev.off()



ggplot(data = dataframe, aes(x=feature, y = watterson_ave, fill =pop)) + 
  geom_boxplot(outlier.shape=NA, position=position_dodge(width=1.5))+
  geom_point(data=df.wm,aes(x=feature, y = weightedmean, fill=pop),shape = 23, position=position_dodge(width=1.5), size = 3, inherit.aes=FALSE) +
  facet_grid(.~feature, scales="free") +
  scale_y_continuous(trans = 'log10') +
  scale_x_discrete("Genomic region") +
  xlab(label = "Genomic region") + #x title
  ylab(label = "Watterson average") + # y title
  theme_bw() +  #theme selection for background and lines
  # scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  ggtitle(paste ("Diversity by region",name_diversity[1], sep = " " )) +
  theme(axis.line.x = element_line(color="black", size = 0.5),
        axis.line.y = element_line(color="black", size = 0.5),
        axis.text.x = element_text(angle = 60, vjust = 0.8, hjust = 1),
        axis.text.y = element_text(vjust = 0.2, hjust = 0.2),
        axis.title.y = element_text(margin=margin(r=0.3, unit="cm")),
        axis.title.x = element_text(margin=margin(t=0.5, unit="cm")),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.border = element_blank(),
        panel.background = element_blank(),
        strip.background = element_blank(),
        strip.text.x = element_blank()) +
        geom_text(data = df.wm.gathered, aes(feature, percentage_po_ki, label=paste((percentage_po_ki %>% round(2)),(percentage_no_ki%>% round(2)), sep='\n')), angle = 45, size=2, vjust="inward",hjust="inward")


ggsave(filename = paste(wd,"/",name_diversity[1],".diversity_region_watterson.eps", sep = ""), device="eps")


# Cosas útiles outliers.shape=NA.



```

Copy

```{bash}

scp mlucena@genomics-b.ebd.csic.es://home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/diversity_per_unit/*eps /Users/marialucenaperez/ownCloud/publico/WG_diversity/diversity_plots/diversity_per_unit 

scp mlucena@genomics-b.ebd.csic.es://home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/diversity_per_unit/*csv /Users/marialucenaperez/ownCloud/publico/WG_diversity/diversity_plots/diversity_per_unit 


scp mlucena@genomics-b.ebd.csic.es:/home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/diversity_per_region/*eps /Users/marialucenaperez/ownCloud/publico/WG_diversity/diversity_plots/diversity_per_region/



```

Diversity per region: Cat Coordinates

He probado con el script de Fede de perl:

```{bash}

# Primero le quito el cabecero porque si no, no puede usarse bedintersect:
# sed '1d' c_ll_ki_n013.per.region.averages.tsv  > c_ll_ki_n013.per.region.averages.NOHEADER.tsv 
# Ahora lo corro tal y como indica el propio script:
# perl lynx2cat.pl c_ll_ki_n013.per.region.averages.NOHEADER.tsv > c_ll_ki_n013.per.region.averages.CatCoordinates.tsv
# Este script me ha dado el siguiente error:
#Use of uninitialized value $s in concatenation (.) or string at lynx2cat.pl line 14, <I> line 156394492.
#Use of uninitialized value $e in concatenation (.) or string at lynx2cat.pl line 14, <I> line 156394492.
# Además me he dado cuenta que solo reporta las coordenadas de uno y otro, y yo quiero toda la información. 

```

Por tanto pruebo con el básico:

```{bash}

intersectBed -sorted -wo -a c_ll_ki_n013.per.region.averages.NOHEADER.tsv -b lynx2cat_wTiger.bed > c_ll_ki_n013.per.region.averages.CatCoordinates.tsv

# Me da el error:

# ERROR: chromomsome sort ordering for file c_ll_ki_n013.per.region.averages.NOHEADER.tsv is inconsistent with other files. Record was:
# lp23.s00015     2903992 3006783 102791  101509  telomers0-10m   na      na      na      na      6.5495647224*10^-06     1.4900294206*10^-05        2.5287837844*10^-06     5.8944110986*10^-06     -4.4603274016*10^-01    1282    c_ll_ki_n013    c_ll

```


Así que ordeno antes:

```{bash}

 bedtools sort c_ll_ki_n013.per.region.averages.NOHEADER.tsv > c_ll_ki_n013.per.region.averages.NOHEADER.SORTED.tsv

```


--------------------------------
Window analysis


```{bash}

RUTA=/home/mlucena/ANGSD_analysis 
cd $RUTA/whole_genome_analysis/sfs/window_analysis

#To launch one by one
POP="c_lp_sm_n019"  # <--CHANGE POP HERE
screen -S "$POP"_sfs_window
POP="c_lp_sm_n019"  # <--CHANGE POP HERE
script "$POP"_sfs_window.log
POP="c_lp_sm_n019"  # <--CHANGE POP HERE

THREADS=10                     # no. of computer cores used by bwa and samtools. 20 = OK, >20 = ask people first!

NGSTOOLS="/opt/angsd/angsd/misc"

WINDOWSIZE=50000
WINDOWSTEP=10000 

RUTA=/home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs

echo "-------$POP---------- Window analysis -----------------------------------------"

$NGSTOOLS/thetaStat do_stat $RUTA/$POP.unfolded-lr.postprob.thetas.idx -win $WINDOWSIZE -step $WINDOWSTEP  -outnames $POP.unfolded-lr.postprob.thetasWindow_$WINDOWSIZE.$WINDOWSTEP.gz

```
Values in this output file are the sum of the per-site estimates for the whole window.


Window R representation


Download data:

```{bash}

scp mlucena@genomics-b.ebd.csic.es:/home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/window_analysis/*PG /Users/marialucenaperez/Owncloud/publico/WG_diversity/ANGSD/sfs/window_analysis/

```


```{r}

# Representación de las ventanas en manhattan plots. 

library(dplyr)
library(qqman)
library(ggplot2)
library ("GGally")

# wd <- "/Users/marialucenaperez/Owncloud/publico/WG_diversity/ANGSD/sfs/window_analysis/" 
wd <- "/home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/window_analysis/"

finsThetas = list.files(path = wd, pattern="*.pestPG$")

for (i in 1:length(finsThetas))
{
  datThetas <- read.csv (paste(wd,finsThetas[i],sep=""), header = T, sep = '\t')  
  name_thetas <- unlist(strsplit(finsThetas[i], "[.]"))
  datThetas$CHR <- as.numeric(gsub('^.{6}', '', datThetas$Chr))
  datThetas$Nsites <- as.numeric(datThetas$nSites)
  datThetas$SNPS_window = paste(datThetas$Chr, datThetas$WinCenter, sep='_')
  datThetas[,-c(1,2)][datThetas[, -c(1,2)] < 0] <- 0
  # Manhattan plot Theta Waterson
  max_thetas <- max (datThetas$tW)
  min_thetas <- min(datThetas$tW)
  setEPS()
  postscript(file=paste(wd,name_thetas[1], '_watterson_manhattan.eps', sep=''))
  manhattan(datThetas, chr = "CHR", bp = "WinCenter", p = "tW", snp = "SNPS_window", ylim = c(0 , max_thetas+10), ylab = "tW", xlab = "Scaffold", logp=F, genomewideline = F, suggestiveline = F, main = paste ("Manhattan plot of tW ",name_thetas[1], sep =''))
  dev.off() 
  # Correlation between diversity indixes
  setEPS()
  postscript(file=paste(wd,name_thetas[1], '_diversity_correlation.eps', sep=''), horizontal = FALSE, onefile = FALSE, paper = "special")
  print(ggpairs(datThetas, columns=4:8, title=paste("Theta correlations ", name_thetas[1], sep='')))
  dev.off()
  # Correlation between neutrality indixes
  setEPS()
  postscript(file=paste(wd,name_thetas[1], '_neutrality_correlation.eps', sep=''), horizontal = FALSE, onefile = FALSE, paper = "special")
  print(ggpairs(datThetas, columns=9:13, title=paste("Neutrality indexes correlations ", name_thetas[1], sep='')))
  dev.off()
}


```


```{bash}
scp mlucena@genomics-b.ebd.csic.es:/home/mlucena/ANGSD_analysis/whole_genome_analysis/sfs/window_analysis/*eps /Users/marialucenaperez/Desktop

```

# ----------------------------------

# ATLAS on contemporary data



```{bash}

mkdir mkdir /home/mlucena/ATLAS_contemporary_data
cd /home/mlucena/ATLAS_contemporary_data


# Cuando he probado a Validate samtools me dice que Value was put into PairInfoMap more than once.

java -jar /opt/picard-tools/picard.jar FixMateInformation I=c_lp_sm_0325_recal_round-1.bam O=/home/mlucena/ATLAS_contemporary_data/test_sorted_rg_solved.bam VALIDATION_STRINGENCY=LENIENT


java -jar /opt/picard-tools/picard.jar ValidateSamFile I=test_sorted_rg_solved.bam MAX_OPEN_TEMP_FILES=1000  IGNORE_WARNINGS=true MODE=VERBOSE

samtools index test_sorted_rg_solved.bam

REF="/home/GRUPOS/grupolince/reference_genomes/lynx_pardinus_genome/lp23_without_repetitive_transposable_low_complexity.fa"
bam_to_test=test_sorted_rg_solved.bam


/home/mlucena/atlas/atlas task=mergeReads bam=$bam_to_test fasta=$REF chr=lp23.s00001 verbose
#WARNING: read HWI-ST858:204:C5RR4ACXX:5:1311:13192:36371 was filtered out because it was longer than the insert size
#ERROR: One read of 'HWI-ST0857:233:C5T3DACXX:2:2201:5181:65649' is reverse mate, but forward one has not been read!


# Vuelvo a correr validate samtools

java -jar /opt/picard-tools/picard.jar ValidateSamFile I=test_sorted_rg_solved.bam MAX_OPEN_TEMP_FILES=1000  IGNORE_WARNINGS=true MODE=VERBOSE
# No errors found

# Preubo a correr estimateTheta:
/home/mlucena/atlas/atlas task=estimateTheta bam=$bam_to_test fasta=$REF chr=lp23.s00001 verbose
# WARNING: The following alignment is longer than its insert size: HWI-ST858:204:C5RR4ACXX:5:1311:15377:69596
# WARNING: The following alignment is longer than its insert size: HWI-ST858:204:C5RR4ACXX:5:1311:15377:69596

# aun así me devuelve output.
# Ya tengo el archivo con los valores theta.


# ¿qué pasaría si estimo el PMD?
REF="/home/GRUPOS/grupolince/reference_genomes/lynx_pardinus_genome/lp23_without_repetitive_transposable_low_complexity.fa"
/home/mlucena/atlas/atlas task=estimatePMD bam=test_sorted_rg_solved.bam fasta=$REF  verbose 
# ERROR: position in TPMDTable add function is < 0!
# El mismo error que ccon las antiguas mapeadas con BWA-mem






```


## R representation

Lo represento

```{r}

wd <- "/Users/marialucenaperez/Dropbox/PhD/ancient_historical/ATLAS/contemporary_data/"

thetas_file <- read.table(paste (wd, "test_sorted_rg_solved_theta_estimates.txt", sep =""), header = T)


c="lp23.s00001"
colors=c("#000000",rainbow(tot-2))
    plot('', type="n", xlim=c(0, 2.5e+8), ylim=c(1e-5,0.025),log='y',xlab="Chromosome position",main=paste("Chr=",c,sep=''), yaxt='n') #ylab=expression("Estimated "*theta)
    labelsY1=parse(text=paste(c(1,1,1,1),"%*%","10^",c(-5,-4,-3,-2), sep=""))
    axis(2, at=c(10^-5, 10^-4, 10^-3, 10^-2), labels=labelsY1, las=2)

    for (i in (2:tot)){
            a0<-read.delim(paste(args[i], "_theta_estimates.txt", sep=''))
            a0<-na.omit(a0)
            a0.1<-subset(a0,Chr==c|Chr==paste("chr",c,sep=''))
            lines(a0.1$start,a0.1$theta_MLE,type='l',col=colors[i-1])
            abline(h=median(subset(a0.1,theta_MLE>0)$theta_MLE),lty=2,col=colors[i-1])
            base=basename(args[i])
            legendvec[i-1]=strsplit(base,split="[_.]+")[[1]][1]
    }







eval(parse(text=args[thetas_file]))





# El script que suben ellos:
args <- commandArgs(TRUE)
tot=length(args)
legendvec=vector(length=tot-1)
chr=eval(parse(text=args[1]))

pdf(paste(args[2], "_theta_plot.pdf", sep=""),height=50,width=15)
par(mfrow=c(22,1))
for(c in chr){
    colors=c("#000000",rainbow(tot-2))
    plot('', type="n", xlim=c(0, 2.5e+8), ylim=c(1e-5,0.025),log='y',xlab="Chromosome position",main=paste("Chr=",c,sep=''), yaxt='n') #ylab=expression("Estimated "*theta)
    labelsY1=parse(text=paste(c(1,1,1,1),"%*%","10^",c(-5,-4,-3,-2), sep=""))
    axis(2, at=c(10^-5, 10^-4, 10^-3, 10^-2), labels=labelsY1, las=2)

    for (i in (2:tot)){
            a0<-read.delim(paste(args[i], "_theta_estimates.txt", sep=''))
            a0<-na.omit(a0)
            a0.1<-subset(a0,Chr==c|Chr==paste("chr",c,sep=''))
            lines(a0.1$start,a0.1$theta_MLE,type='l',col=colors[i-1])
            abline(h=median(subset(a0.1,theta_MLE>0)$theta_MLE),lty=2,col=colors[i-1])
            base=basename(args[i])
            legendvec[i-1]=strsplit(base,split="[_.]+")[[1]][1]
    }

    legend("bottomleft",legend=legendvec,col=colors,lty=rep(1,tot-1),lwd=rep(1.5,tot-1),horiz=T)
}
dev.off()



```


# ----------------------------------

# Otra info util


Coincide para sierra morena

3UTR
5UTR
CDS
CDS,lncRNA
feature
intergenic
intron
intron,intron_lncRNA
intron_lncRNA
intron,ncRNA
lncRNA
lncRNA,ncRNA
ncRNA
promoter_gene_1000
promoter_gene_1000,promoter_gene_250,promoter_gene_500
promoter_gene_250
promoter_gene_500
promoter_lncRNA_1000
promoter_lncRNA_1000,promoter_lncRNA_250,promoter_lncRNA_500
promoter_lncRNA_250
promoter_lncRNA_500
UCNE



____


Omnogovi (in Mongolian) is equivalent South Gobi (or S.Gobi)

Khentii−Aimag is better as Khentiy_Mong

Tov (also Aimag/region) is equivalent Central region (Centr_Mong)



_____


Si queremos descargar tablas en base a un nombre y si no está que nos de error: tryCatch

Procesamiento de la tabla con region annotation. Esto sólo se hace una vez!
Comprobamos que el archivo tiene información y añadimos un porcentaje con las bases que tiene que son teloméricas / centroméricas.

```{r}

library(dplyr)

wd <- "/Users/marialucenaperez/Documents/WG_lynx_diversity_per_unit/"

# Tel2m
files_tel2m <- list.files(path = wd, pattern = "region.tel2m")

# Esto saca el porcentaje de bases anotadas como de una región determinada para cada población
for (file_tel2m in files_tel2m){
dataframe <- tryCatch({
        if (file.size(paste(wd, file_tel2m, sep="")) > 0)
          {
    read.table(file=paste(wd, file_tel2m, sep=""), header=T) %>% 
            mutate (tel2m_percentage=tel2m_bases/length)
          }
}, error =function(err) {
            # error handler picks up where error was generated
            print(paste("Read.table didn't work!:  ",err))
        })
# Hasta aquí bien, ahora guardo estos archivos en la carpeta de interes.
write.table(dataframe, paste(wd, file_tel2m, sep=""), row.names = F, quote = F, sep = '\t')
}


# Tel10m
files_tel10m <- list.files(path = wd, pattern = "region.tel10m")

# Esto saca el porcentaje de bases anotadas como de una región determinada para cada población
for (file_tel10m in files_tel10m){
dataframe <- tryCatch({
        if (file.size(paste(wd, file_tel10m, sep="")) > 0)
          {
    read.table(file=paste(wd, file_tel10m, sep=""), header=T) %>% 
            mutate (tel10m_percentage=tel10m_bases/length)
          }
}, error =function(err) {
            # error handler picks up where error was generated
            print(paste("Read.table didn't work!:  ",err))
        })
# Hasta aquí bien, ahora guardo estos archivos en la carpeta de interes.
write.table(dataframe, paste(wd, file_tel10m, sep=""), row.names = F, quote = F, sep = '\t')
}

# Centr

files_centr <- list.files(path = wd, pattern = "region.centr")

# Esto saca el porcentaje de bases anotadas como de una región determinada para cada población
for (file_centr in files_centr){
dataframe <- tryCatch({
        if (file.size(paste(wd, file_centr, sep="")) > 0)
          {
    read.table(file=paste(wd, file_centr, sep=""), header=T) %>% 
            mutate (centr_percentage=centr_bases/length)
          }
}, error =function(err) {
            # error handler picks up where error was generated
            print(paste("Read.table didn't work!:  ",err))
        })
# Hasta aquí bien, ahora guardo estos archivos en la carpeta de interes.
write.table(dataframe, paste(wd, file_centr, sep=""), row.names = F, quote = F, sep = '\t')
}

```




